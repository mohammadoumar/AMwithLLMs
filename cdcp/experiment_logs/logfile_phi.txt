Running CDCP_finetune.py with arguments: unsloth/Phi-3-mini-4k-instruct-bnb-4bit acc
09/11/2024 22:56:58 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:25405
09/11/2024 22:57:08 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/11/2024 22:57:08 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/11/2024 22:57:08 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/11/2024 22:57:08 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/11/2024 22:57:08 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/11/2024 22:57:08 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/11/2024 22:57:08 - INFO - llamafactory.data.template - Replace eos token: <|end|>
09/11/2024 22:57:08 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.
09/11/2024 22:57:08 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_acc_train.json...
09/11/2024 22:57:08 - INFO - llamafactory.data.template - Replace eos token: <|end|>
09/11/2024 22:57:08 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.
09/11/2024 22:57:09 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_acc_train.json...
training example:
input_ids:
[1, 32010, 835, 887, 526, 385, 17924, 297, 23125, 341, 2827, 29889, 887, 526, 2183, 263, 1426, 607, 3743, 1353, 287, 2980, 7117, 427, 15603, 491, 529, 2477, 2565, 2477, 29958, 8282, 29889, 3575, 3414, 338, 304, 770, 1598, 1269, 2980, 4163, 297, 278, 1426, 408, 2845, 376, 17028, 613, 376, 22197, 613, 376, 5679, 613, 376, 1688, 326, 2592, 29908, 470, 376, 1767, 1642, 887, 1818, 736, 263, 1051, 310, 2980, 4163, 4072, 29892, 18719, 310, 3309, 29871, 29941, 29892, 297, 1494, 4663, 3402, 29901, 8853, 9700, 29918, 8768, 1115, 6796, 9700, 29918, 1853, 313, 710, 19123, 376, 9700, 29918, 1853, 313, 710, 19123, 376, 9700, 29918, 1853, 313, 710, 29897, 3108, 29913, 988, 1269, 1543, 376, 9700, 29918, 1853, 313, 710, 5513, 338, 8611, 491, 2845, 376, 17028, 613, 376, 22197, 613, 376, 5679, 613, 376, 1688, 326, 2592, 29908, 470, 376, 1767, 1642, 29871, 13, 13, 2277, 29937, 2266, 338, 278, 1426, 29901, 529, 2477, 29896, 29958, 2792, 322, 1887, 8973, 6865, 6041, 1207, 2322, 6577, 29887, 1860, 1568, 901, 5517, 21106, 2477, 29896, 5299, 2477, 29906, 29958, 1152, 1342, 29892, 746, 263, 2022, 1058, 16831, 23244, 8152, 267, 263, 2553, 29873, 338, 5429, 304, 2041, 304, 8973, 373, 263, 664, 2462, 29892, 896, 1122, 367, 11826, 304, 6755, 1546, 263, 2322, 24284, 322, 1009, 4982, 21106, 2477, 29906, 5299, 2477, 29941, 29958, 306, 5065, 479, 278, 17861, 29925, 29933, 304, 1284, 23274, 393, 25135, 28598, 19478, 8293, 886, 472, 22629, 854, 993, 3064, 29395, 1466, 29892, 316, 1547, 573, 29892, 322, 633, 375, 573, 29892, 470, 22435, 9696, 411, 29871, 29896, 29953, 29929, 29906, 29875, 21106, 2477, 29941, 29958, 32007, 29871, 13, 32001, 8853, 9700, 29918, 8768, 1115, 6796, 1767, 613, 376, 1767, 613, 376, 22197, 3108, 29913, 32007]
inputs:
<s><|user|> ### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to classify each argument component in the text as either "fact", "policy", "reference", "testimony" or "value". You must return a list of argument component types, strictly of length 3, in following JSON format: {"component_types": ["component_type (str)", "component_type (str)", "component_type (str)"]} where each element "component_type (str)" is replaced by either "fact", "policy", "reference", "testimony" or "value". 

### Here is the text: <AC1>State and local court rules sometimes make default judgments much more likely.</AC1><AC2> For example, when a person who allegedly owes a debt is told to come to court on a work day, they may be forced to choose between a default judgment and their job.</AC2><AC3> I urge the CFPB to find practices that involve scheduling hearings at inconvenient times unfair, deceptive, and abusive, or inconsistent with 1692i.</AC3><|end|> 
<|assistant|> {"component_types": ["value", "value", "policy"]}<|end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 8853, 9700, 29918, 8768, 1115, 6796, 1767, 613, 376, 1767, 613, 376, 22197, 3108, 29913, 32007]
labels:
{"component_types": ["value", "value", "policy"]}<|end|>
09/11/2024 22:57:09 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/11/2024 22:57:09 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/11/2024 22:57:09 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/11/2024 22:57:09 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/11/2024 22:57:13 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/11/2024 22:57:13 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/11/2024 22:57:13 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/11/2024 22:57:13 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/11/2024 22:57:13 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,k_proj,gate_proj,o_proj,up_proj,down_proj,q_proj
09/11/2024 22:57:13 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/11/2024 22:57:13 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/11/2024 22:57:13 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/11/2024 22:57:13 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/11/2024 22:57:13 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,v_proj,down_proj,k_proj,up_proj,q_proj,o_proj
09/11/2024 22:57:13 - INFO - llamafactory.model.loader - trainable params: 14,942,208 || all params: 3,836,021,760 || trainable%: 0.3895
09/11/2024 22:57:13 - INFO - llamafactory.model.loader - trainable params: 14,942,208 || all params: 3,836,021,760 || trainable%: 0.3895
09/11/2024 22:57:13 - WARNING - llamafactory.train.callbacks - Previous trainer log in this folder will be deleted.
09/11/2024 22:57:14 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/11/2024 22:57:14 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.3271, 'grad_norm': 0.27278876304626465, 'learning_rate': 2.777777777777778e-05, 'epoch': 0.28}
{'loss': 0.1945, 'grad_norm': 0.2326863557100296, 'learning_rate': 4.998119881260576e-05, 'epoch': 0.55}
{'loss': 0.1401, 'grad_norm': 0.3139885365962982, 'learning_rate': 4.9326121764495596e-05, 'epoch': 0.83}
{'loss': 0.1219, 'grad_norm': 0.3055376410484314, 'learning_rate': 4.775907352415367e-05, 'epoch': 1.1}
{'loss': 0.1045, 'grad_norm': 0.33344200253486633, 'learning_rate': 4.533880175657419e-05, 'epoch': 1.38}
{'loss': 0.0898, 'grad_norm': 0.16993571817874908, 'learning_rate': 4.215604094671835e-05, 'epoch': 1.66}
{'loss': 0.0905, 'grad_norm': 0.5624772310256958, 'learning_rate': 3.8330110820042285e-05, 'epoch': 1.93}
{'loss': 0.0863, 'grad_norm': 0.4729595482349396, 'learning_rate': 3.400444312011776e-05, 'epoch': 2.21}
{'loss': 0.0531, 'grad_norm': 0.31299924850463867, 'learning_rate': 2.9341204441673266e-05, 'epoch': 2.48}
{'loss': 0.0585, 'grad_norm': 0.2800579369068146, 'learning_rate': 2.4515216705704395e-05, 'epoch': 2.76}
{'loss': 0.0622, 'grad_norm': 0.4470510482788086, 'learning_rate': 1.970740319426474e-05, 'epoch': 3.03}
{'loss': 0.0362, 'grad_norm': 0.13627906143665314, 'learning_rate': 1.509800584902108e-05, 'epoch': 3.31}
{'loss': 0.0383, 'grad_norm': 0.2685307562351227, 'learning_rate': 1.085982811283654e-05, 'epoch': 3.59}
{'loss': 0.0411, 'grad_norm': 0.16509276628494263, 'learning_rate': 7.1517566360525284e-06, 'epoch': 3.86}
{'loss': 0.0327, 'grad_norm': 0.21310988068580627, 'learning_rate': 4.112804714676594e-06, 'epoch': 4.14}
{'loss': 0.027, 'grad_norm': 0.2289748191833496, 'learning_rate': 1.8569007682777417e-06, 'epoch': 4.41}
{'loss': 0.0267, 'grad_norm': 0.13440890610218048, 'learning_rate': 4.6861723431538276e-07, 'epoch': 4.69}
{'loss': 0.0232, 'grad_norm': 0.19036178290843964, 'learning_rate': 0.0, 'epoch': 4.97}
{'train_runtime': 587.2406, 'train_samples_per_second': 4.938, 'train_steps_per_second': 0.307, 'train_loss': 0.08630719681580862, 'epoch': 4.97}
***** train metrics *****
  epoch                    =     4.9655
  total_flos               = 30326061GF
  train_loss               =     0.0863
  train_runtime            = 0:09:47.24
  train_samples_per_second =      4.938
  train_steps_per_second   =      0.307
09/11/2024 23:07:10 - INFO - llamafactory.data.template - Replace eos token: <|end|>
09/11/2024 23:07:10 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.
09/11/2024 23:07:10 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/11/2024 23:07:10 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/11/2024 23:07:10 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/11/2024 23:07:12 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/11/2024 23:07:13 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/finetuned_models_run3/CDCP_acc_Phi-3-mini-4k-instruct-bnb-4bit
09/11/2024 23:07:13 - INFO - llamafactory.model.loader - all params: 3,836,021,760
              precision    recall  f1-score   support

        fact      0.602     0.606     0.604       132
      policy      0.844     0.850     0.847       153
   reference      1.000     1.000     1.000         1
   testimony      0.874     0.881     0.878       244
       value      0.829     0.823     0.826       496

    accuracy                          0.813      1026
   macro avg      0.830     0.832     0.831      1026
weighted avg      0.813     0.813     0.813      1026

Successfully ran CDCP_finetune.py with arguments: unsloth/Phi-3-mini-4k-instruct-bnb-4bit acc 
 
  *************** 

Running CDCP_finetune.py with arguments: unsloth/Phi-3-mini-4k-instruct-bnb-4bit ari
09/11/2024 23:14:00 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:22076
09/11/2024 23:14:10 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/11/2024 23:14:10 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/11/2024 23:14:10 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/11/2024 23:14:10 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/11/2024 23:14:10 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/11/2024 23:14:10 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/11/2024 23:14:10 - INFO - llamafactory.data.template - Replace eos token: <|end|>
09/11/2024 23:14:10 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.
09/11/2024 23:14:10 - INFO - llamafactory.data.template - Replace eos token: <|end|>
09/11/2024 23:14:10 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.
09/11/2024 23:14:10 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_ari_train.json...
09/11/2024 23:14:11 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_ari_train.json...
training example:
input_ids:
[1, 32010, 835, 887, 526, 385, 17924, 297, 23125, 341, 2827, 29889, 887, 526, 2183, 263, 1426, 607, 3743, 1353, 287, 2980, 7117, 427, 15603, 491, 529, 2477, 2565, 2477, 29958, 8282, 29889, 3575, 3414, 338, 304, 12439, 2980, 5302, 1546, 2980, 7117, 297, 278, 1426, 29889, 887, 1818, 736, 263, 1051, 310, 11000, 297, 278, 1494, 4663, 3402, 29901, 8853, 1761, 29918, 23516, 29918, 2674, 800, 1115, 5519, 4993, 14614, 313, 524, 511, 3646, 14614, 313, 524, 29897, 1402, 2023, 29892, 518, 4993, 14614, 313, 524, 511, 3646, 14614, 313, 524, 4638, 12258, 13, 13, 2277, 29937, 2266, 338, 278, 1426, 29901, 529, 2477, 29900, 29958, 2792, 322, 1887, 8973, 6865, 6041, 1207, 2322, 6577, 29887, 1860, 1568, 901, 5517, 21106, 2477, 29900, 5299, 2477, 29896, 29958, 1152, 1342, 29892, 746, 263, 2022, 1058, 16831, 23244, 8152, 267, 263, 2553, 29873, 338, 5429, 304, 2041, 304, 8973, 373, 263, 664, 2462, 29892, 896, 1122, 367, 11826, 304, 6755, 1546, 263, 2322, 24284, 322, 1009, 4982, 21106, 2477, 29896, 5299, 2477, 29906, 29958, 306, 5065, 479, 278, 17861, 29925, 29933, 304, 1284, 23274, 393, 25135, 28598, 19478, 8293, 886, 472, 22629, 854, 993, 3064, 29395, 1466, 29892, 316, 1547, 573, 29892, 322, 633, 375, 573, 29892, 470, 22435, 9696, 411, 29871, 29896, 29953, 29929, 29906, 29875, 21106, 2477, 29906, 29958, 32007, 29871, 13, 32001, 8853, 1761, 29918, 23516, 29918, 2674, 800, 1115, 5519, 29900, 29892, 29871, 29896, 1402, 518, 29906, 29892, 29871, 29900, 5262, 29913, 32007]
inputs:
<s><|user|> ### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to identify argument relations between argument components in the text. You must return a list of pairs in the following JSON format: {"list_argument_relations": [[source AC (int), target AC (int)], ..., [source AC (int), target AC (int)]]}

### Here is the text: <AC0>State and local court rules sometimes make default judgments much more likely.</AC0><AC1> For example, when a person who allegedly owes a debt is told to come to court on a work day, they may be forced to choose between a default judgment and their job.</AC1><AC2> I urge the CFPB to find practices that involve scheduling hearings at inconvenient times unfair, deceptive, and abusive, or inconsistent with 1692i.</AC2><|end|> 
<|assistant|> {"list_argument_relations": [[0, 1], [2, 0]]}<|end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 8853, 1761, 29918, 23516, 29918, 2674, 800, 1115, 5519, 29900, 29892, 29871, 29896, 1402, 518, 29906, 29892, 29871, 29900, 5262, 29913, 32007]
labels:
{"list_argument_relations": [[0, 1], [2, 0]]}<|end|>
09/11/2024 23:14:12 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/11/2024 23:14:12 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/11/2024 23:14:12 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/11/2024 23:14:12 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/11/2024 23:14:15 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/11/2024 23:14:15 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/11/2024 23:14:15 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/11/2024 23:14:15 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/11/2024 23:14:15 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,q_proj,v_proj,down_proj,k_proj,o_proj,up_proj
09/11/2024 23:14:15 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/11/2024 23:14:15 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/11/2024 23:14:15 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/11/2024 23:14:15 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/11/2024 23:14:15 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,gate_proj,k_proj,up_proj,o_proj,v_proj,down_proj
09/11/2024 23:14:16 - INFO - llamafactory.model.loader - trainable params: 14,942,208 || all params: 3,836,021,760 || trainable%: 0.3895
09/11/2024 23:14:16 - INFO - llamafactory.model.loader - trainable params: 14,942,208 || all params: 3,836,021,760 || trainable%: 0.3895
09/11/2024 23:14:16 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/11/2024 23:14:16 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.5634, 'grad_norm': 0.5301043391227722, 'learning_rate': 2.777777777777778e-05, 'epoch': 0.28}
{'loss': 0.3068, 'grad_norm': 0.26768776774406433, 'learning_rate': 4.998119881260576e-05, 'epoch': 0.55}
{'loss': 0.267, 'grad_norm': 0.21170541644096375, 'learning_rate': 4.9326121764495596e-05, 'epoch': 0.83}
{'loss': 0.2656, 'grad_norm': 0.15550820529460907, 'learning_rate': 4.775907352415367e-05, 'epoch': 1.1}
{'loss': 0.2365, 'grad_norm': 0.20329059660434723, 'learning_rate': 4.533880175657419e-05, 'epoch': 1.38}
{'loss': 0.2282, 'grad_norm': 0.20590457320213318, 'learning_rate': 4.215604094671835e-05, 'epoch': 1.66}
{'loss': 0.2263, 'grad_norm': 0.22335529327392578, 'learning_rate': 3.8330110820042285e-05, 'epoch': 1.93}
{'loss': 0.2158, 'grad_norm': 0.2786203920841217, 'learning_rate': 3.400444312011776e-05, 'epoch': 2.21}
{'loss': 0.1716, 'grad_norm': 0.31963050365448, 'learning_rate': 2.9341204441673266e-05, 'epoch': 2.48}
{'loss': 0.1909, 'grad_norm': 0.2603835463523865, 'learning_rate': 2.4515216705704395e-05, 'epoch': 2.76}
{'loss': 0.1876, 'grad_norm': 0.2854428291320801, 'learning_rate': 2.0182193486297755e-05, 'epoch': 3.03}
{'loss': 0.1441, 'grad_norm': 0.3287264406681061, 'learning_rate': 1.554500354570894e-05, 'epoch': 3.31}
{'loss': 0.135, 'grad_norm': 0.38280051946640015, 'learning_rate': 1.126227554822985e-05, 'epoch': 3.59}
{'loss': 0.1287, 'grad_norm': 0.5198612809181213, 'learning_rate': 7.494566305820788e-06, 'epoch': 3.86}
{'loss': 0.1162, 'grad_norm': 0.3080197870731354, 'learning_rate': 4.383124897272331e-06, 'epoch': 4.14}
{'loss': 0.0989, 'grad_norm': 0.34679093956947327, 'learning_rate': 2.044597327993153e-06, 'epoch': 4.41}
{'loss': 0.0952, 'grad_norm': 0.3113698959350586, 'learning_rate': 5.666535437341108e-07, 'epoch': 4.69}
{'loss': 0.0996, 'grad_norm': 0.25640735030174255, 'learning_rate': 4.700738787466463e-09, 'epoch': 4.97}
{'train_runtime': 568.8513, 'train_samples_per_second': 5.098, 'train_steps_per_second': 0.316, 'train_loss': 0.2043048600355784, 'epoch': 4.97}
***** train metrics *****
  epoch                    =     4.9655
  total_flos               = 24612586GF
  train_loss               =     0.2043
  train_runtime            = 0:09:28.85
  train_samples_per_second =      5.098
  train_steps_per_second   =      0.316
09/11/2024 23:23:52 - INFO - llamafactory.data.template - Replace eos token: <|end|>
09/11/2024 23:23:52 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.
09/11/2024 23:23:52 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/11/2024 23:23:52 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/11/2024 23:23:52 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/11/2024 23:23:54 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/11/2024 23:23:55 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/finetuned_models_run3/CDCP_ari_Phi-3-mini-4k-instruct-bnb-4bit
09/11/2024 23:23:55 - INFO - llamafactory.model.loader - all params: 3,836,021,760
              precision    recall  f1-score   support

       N-Rel      0.976     0.974     0.975     10004
         Rel      0.240     0.250     0.245       324

    accuracy                          0.952     10328
   macro avg      0.608     0.612     0.610     10328
weighted avg      0.953     0.952     0.952     10328

Successfully ran CDCP_finetune.py with arguments: unsloth/Phi-3-mini-4k-instruct-bnb-4bit ari 
 
  *************** 

Running CDCP_finetune.py with arguments: unsloth/Phi-3-mini-4k-instruct-bnb-4bit arc
09/11/2024 23:29:21 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:23140
09/11/2024 23:29:31 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/11/2024 23:29:31 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/11/2024 23:29:31 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/11/2024 23:29:31 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/11/2024 23:29:31 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/11/2024 23:29:31 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/11/2024 23:29:31 - INFO - llamafactory.data.template - Replace eos token: <|end|>
09/11/2024 23:29:31 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.
09/11/2024 23:29:31 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_arc_train.json...
09/11/2024 23:29:31 - INFO - llamafactory.data.template - Replace eos token: <|end|>
09/11/2024 23:29:31 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.
09/11/2024 23:29:32 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_arc_train.json...
training example:
input_ids:
[1, 32010, 835, 887, 526, 385, 17924, 297, 23125, 341, 2827, 29889, 887, 526, 2183, 263, 1426, 607, 3743, 1353, 287, 2980, 7117, 427, 15603, 491, 529, 2477, 2565, 2477, 29958, 8282, 29889, 887, 526, 884, 2183, 263, 1051, 310, 11000, 310, 4475, 2980, 7117, 297, 278, 883, 29901, 17288, 5182, 14614, 313, 524, 511, 2752, 14614, 313, 524, 8243, 313, 5182, 14614, 313, 524, 511, 2752, 14614, 313, 524, 8243, 2023, 29892, 313, 5182, 14614, 313, 524, 511, 2752, 14614, 313, 524, 876, 1822, 3575, 3414, 338, 304, 770, 1598, 1269, 5101, 310, 4475, 2980, 7117, 297, 278, 1051, 408, 2845, 376, 23147, 29908, 470, 376, 5750, 5084, 1642, 887, 1818, 736, 263, 1051, 310, 2980, 8220, 4072, 29892, 18719, 310, 3309, 29871, 29906, 29892, 297, 1494, 4663, 3402, 29901, 8853, 23445, 29918, 8768, 1115, 6796, 9700, 29918, 1853, 313, 710, 19123, 376, 9700, 29918, 1853, 313, 710, 29897, 3108, 29913, 988, 1269, 1543, 376, 23445, 29918, 1853, 313, 710, 5513, 338, 8611, 491, 2845, 376, 23147, 29908, 470, 376, 5750, 5084, 1642, 29871, 13, 13, 2277, 29937, 2266, 338, 278, 1426, 29901, 529, 2477, 29896, 29958, 2792, 322, 1887, 8973, 6865, 6041, 1207, 2322, 6577, 29887, 1860, 1568, 901, 5517, 21106, 2477, 29896, 5299, 2477, 29906, 29958, 1152, 1342, 29892, 746, 263, 2022, 1058, 16831, 23244, 8152, 267, 263, 2553, 29873, 338, 5429, 304, 2041, 304, 8973, 373, 263, 664, 2462, 29892, 896, 1122, 367, 11826, 304, 6755, 1546, 263, 2322, 24284, 322, 1009, 4982, 21106, 2477, 29906, 5299, 2477, 29941, 29958, 306, 5065, 479, 278, 17861, 29925, 29933, 304, 1284, 23274, 393, 25135, 28598, 19478, 8293, 886, 472, 22629, 854, 993, 3064, 29395, 1466, 29892, 316, 1547, 573, 29892, 322, 633, 375, 573, 29892, 470, 22435, 9696, 411, 29871, 29896, 29953, 29929, 29906, 29875, 21106, 2477, 29941, 29958, 13, 2277, 29937, 2266, 338, 278, 1051, 310, 11000, 310, 4475, 2980, 7117, 297, 445, 14880, 29901, 5519, 29900, 29892, 29871, 29896, 1402, 518, 29906, 29892, 29871, 29900, 5262, 32007, 29871, 13, 32001, 8853, 23445, 29918, 8768, 1115, 6796, 23147, 613, 376, 23147, 3108, 29913, 32007]
inputs:
<s><|user|> ### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. You are also given a list of pairs of related argument components in the form: [(target AC (int), source AC (int)), (target AC (int), source AC (int)), ..., (target AC (int), source AC (int))]. Your task is to classify each pair of related argument components in the list as either "reason" or "evidence". You must return a list of argument relation types, strictly of length 2, in following JSON format: {"relation_types": ["component_type (str)", "component_type (str)"]} where each element "relation_type (str)" is replaced by either "reason" or "evidence". 

### Here is the text: <AC1>State and local court rules sometimes make default judgments much more likely.</AC1><AC2> For example, when a person who allegedly owes a debt is told to come to court on a work day, they may be forced to choose between a default judgment and their job.</AC2><AC3> I urge the CFPB to find practices that involve scheduling hearings at inconvenient times unfair, deceptive, and abusive, or inconsistent with 1692i.</AC3>
### Here is the list of pairs of related argument components in this paragraph: [[0, 1], [2, 0]]<|end|> 
<|assistant|> {"relation_types": ["reason", "reason"]}<|end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 8853, 23445, 29918, 8768, 1115, 6796, 23147, 613, 376, 23147, 3108, 29913, 32007]
labels:
{"relation_types": ["reason", "reason"]}<|end|>
09/11/2024 23:29:33 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/11/2024 23:29:33 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/11/2024 23:29:33 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/11/2024 23:29:33 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/11/2024 23:29:36 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/11/2024 23:29:36 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/11/2024 23:29:36 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/11/2024 23:29:36 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/11/2024 23:29:36 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,k_proj,down_proj,q_proj,gate_proj,o_proj,v_proj
09/11/2024 23:29:36 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/11/2024 23:29:36 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/11/2024 23:29:36 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/11/2024 23:29:36 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/11/2024 23:29:36 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,v_proj,gate_proj,down_proj,up_proj,k_proj,q_proj
09/11/2024 23:29:37 - INFO - llamafactory.model.loader - trainable params: 14,942,208 || all params: 3,836,021,760 || trainable%: 0.3895
09/11/2024 23:29:37 - INFO - llamafactory.model.loader - trainable params: 14,942,208 || all params: 3,836,021,760 || trainable%: 0.3895
09/11/2024 23:29:37 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/11/2024 23:29:37 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.2657, 'grad_norm': 0.1712826043367386, 'learning_rate': 2.777777777777778e-05, 'epoch': 0.28}
{'loss': 0.0394, 'grad_norm': 0.006647017318755388, 'learning_rate': 4.998119881260576e-05, 'epoch': 0.55}
{'loss': 0.0186, 'grad_norm': 0.2871270179748535, 'learning_rate': 4.9326121764495596e-05, 'epoch': 0.83}
{'loss': 0.0228, 'grad_norm': 0.13644328713417053, 'learning_rate': 4.775907352415367e-05, 'epoch': 1.1}
{'loss': 0.0226, 'grad_norm': 0.3350832760334015, 'learning_rate': 4.533880175657419e-05, 'epoch': 1.38}
{'loss': 0.027, 'grad_norm': 0.25136303901672363, 'learning_rate': 4.215604094671835e-05, 'epoch': 1.66}
{'loss': 0.0144, 'grad_norm': 0.08241989463567734, 'learning_rate': 3.8330110820042285e-05, 'epoch': 1.93}
{'loss': 0.0139, 'grad_norm': 0.318805068731308, 'learning_rate': 3.400444312011776e-05, 'epoch': 2.21}
{'loss': 0.0096, 'grad_norm': 0.10347987711429596, 'learning_rate': 2.9341204441673266e-05, 'epoch': 2.48}
{'loss': 0.0154, 'grad_norm': 0.17433464527130127, 'learning_rate': 2.4515216705704395e-05, 'epoch': 2.76}
{'loss': 0.0118, 'grad_norm': 0.13285475969314575, 'learning_rate': 1.970740319426474e-05, 'epoch': 3.03}
{'loss': 0.0092, 'grad_norm': 0.06688065826892853, 'learning_rate': 1.509800584902108e-05, 'epoch': 3.31}
{'loss': 0.0052, 'grad_norm': 0.05672159790992737, 'learning_rate': 1.085982811283654e-05, 'epoch': 3.59}
{'loss': 0.0043, 'grad_norm': 0.024436965584754944, 'learning_rate': 7.1517566360525284e-06, 'epoch': 3.86}
{'loss': 0.0054, 'grad_norm': 0.21060222387313843, 'learning_rate': 4.112804714676594e-06, 'epoch': 4.14}
{'loss': 0.002, 'grad_norm': 0.005940602160990238, 'learning_rate': 1.8569007682777417e-06, 'epoch': 4.41}
{'loss': 0.0029, 'grad_norm': 0.02537493035197258, 'learning_rate': 4.6861723431538276e-07, 'epoch': 4.69}
{'loss': 0.0051, 'grad_norm': 0.14218607544898987, 'learning_rate': 0.0, 'epoch': 4.97}
{'train_runtime': 577.6873, 'train_samples_per_second': 5.02, 'train_steps_per_second': 0.312, 'train_loss': 0.027519369456503125, 'epoch': 4.97}
***** train metrics *****
  epoch                    =     4.9655
  total_flos               = 30485790GF
  train_loss               =     0.0275
  train_runtime            = 0:09:37.68
  train_samples_per_second =       5.02
  train_steps_per_second   =      0.312
09/11/2024 23:39:23 - INFO - llamafactory.data.template - Replace eos token: <|end|>
09/11/2024 23:39:23 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.
09/11/2024 23:39:24 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/11/2024 23:39:24 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/11/2024 23:39:24 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/11/2024 23:39:26 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/11/2024 23:39:26 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/finetuned_models_run3/CDCP_arc_Phi-3-mini-4k-instruct-bnb-4bit
09/11/2024 23:39:26 - INFO - llamafactory.model.loader - all params: 3,836,021,760
              precision    recall  f1-score   support

    evidence      1.000     0.385     0.556        26
      reason      0.949     1.000     0.974       298

    accuracy                          0.951       324
   macro avg      0.975     0.692     0.765       324
weighted avg      0.953     0.951     0.940       324

Successfully ran CDCP_finetune.py with arguments: unsloth/Phi-3-mini-4k-instruct-bnb-4bit arc 
 
  *************** 

