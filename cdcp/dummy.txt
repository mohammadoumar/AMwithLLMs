Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit acc
Error encountered with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit acc. Skipping to the next pair. 
 
  ************* 

Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit ari
Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit acc
Error encountered with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit acc. Skipping to the next pair. 
 
  ************* 

Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit ari
Error encountered with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit ari. Skipping to the next pair. 
 
  ************* 

Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit arc
Error encountered with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit arc. Skipping to the next pair. 
 
  ************* 

Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit acc
/nfs/scratch/umushtaq/coling_2025
Error encountered with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit acc. Skipping to the next pair. 
 
  ************* 

Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit ari
/nfs/scratch/umushtaq/coling_2025
Error encountered with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit ari. Skipping to the next pair. 
 
  ************* 

Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit arc
/nfs/scratch/umushtaq/coling_2025
Error encountered with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit arc. Skipping to the next pair. 
 
  ************* 

Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit acc
/nfs/scratch/umushtaq/coling_2025
Error encountered with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit acc. Skipping to the next pair. 
 
  ************* 

Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit ari
/nfs/scratch/umushtaq/coling_2025
Error encountered with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit ari. Skipping to the next pair. 
 
  ************* 

Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit arc
/nfs/scratch/umushtaq/coling_2025
Error encountered with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit arc. Skipping to the next pair. 
 
  ************* 

Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit acc
Error encountered with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit acc. Skipping to the next pair. 
 
  ************* 

Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit ari
Error encountered with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit ari. Skipping to the next pair. 
 
  ************* 

Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit arc
Error encountered with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit arc. Skipping to the next pair. 
 
  ************* 

Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit acc
/nfs/scratch/umushtaq/coling_2025/cdcp
Error encountered with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit acc. Skipping to the next pair. 
 
  ************* 

Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit ari
/nfs/scratch/umushtaq/coling_2025/cdcp
Error encountered with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit ari. Skipping to the next pair. 
 
  ************* 

Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit arc
/nfs/scratch/umushtaq/coling_2025/cdcp
Error encountered with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit arc. Skipping to the next pair. 
 
  ************* 

Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit acc
09/09/2024 16:20:14 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/09/2024 16:20:14 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16
09/09/2024 16:20:16 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 16:20:16 - INFO - llamafactory.data.loader - Loading dataset /nfs/scratch/umushtaq/coling_2025/cdcp/datasets/CDCP_acc_train.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 49229, 1855, 5811, 3777, 304, 279, 1495, 439, 3060, 330, 34210, 498, 330, 35890, 498, 330, 16690, 498, 330, 1985, 65556, 1, 477, 330, 970, 3343, 1472, 2011, 471, 264, 1160, 315, 5811, 3777, 4595, 11, 26549, 315, 3160, 220, 18, 11, 304, 2768, 4823, 3645, 25, 5324, 8739, 9962, 794, 4482, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 8, 93546, 1405, 1855, 2449, 330, 8739, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 34210, 498, 330, 35890, 498, 330, 16690, 498, 330, 1985, 65556, 1, 477, 330, 970, 3343, 4815, 14711, 5810, 374, 279, 1495, 25, 366, 1741, 16, 29, 1423, 323, 2254, 5590, 5718, 7170, 1304, 1670, 59358, 1790, 810, 4461, 4005, 1741, 16, 1822, 1741, 17, 29, 1789, 3187, 11, 994, 264, 1732, 889, 19755, 69944, 264, 11897, 374, 3309, 311, 2586, 311, 5590, 389, 264, 990, 1938, 11, 814, 1253, 387, 9770, 311, 5268, 1990, 264, 1670, 19971, 323, 872, 2683, 4005, 1741, 17, 1822, 1741, 18, 29, 358, 33147, 279, 356, 11960, 33, 311, 1505, 12659, 430, 21736, 38952, 45348, 520, 84783, 3115, 28743, 11, 81374, 11, 323, 43658, 11, 477, 40240, 449, 220, 11739, 17, 72, 4005, 1741, 18, 29, 128009, 128006, 78191, 128007, 271, 5018, 8739, 9962, 794, 4482, 970, 498, 330, 970, 498, 330, 35890, 93546, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to classify each argument component in the text as either "fact", "policy", "reference", "testimony" or "value". You must return a list of argument component types, strictly of length 3, in following JSON format: {"component_types": ["component_type (str)", "component_type (str)", "component_type (str)"]} where each element "component_type (str)" is replaced by either "fact", "policy", "reference", "testimony" or "value". 

### Here is the text: <AC1>State and local court rules sometimes make default judgments much more likely.</AC1><AC2> For example, when a person who allegedly owes a debt is told to come to court on a work day, they may be forced to choose between a default judgment and their job.</AC2><AC3> I urge the CFPB to find practices that involve scheduling hearings at inconvenient times unfair, deceptive, and abusive, or inconsistent with 1692i.</AC3><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"component_types": ["value", "value", "policy"]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 8739, 9962, 794, 4482, 970, 498, 330, 970, 498, 330, 35890, 93546, 128009]
labels:
{"component_types": ["value", "value", "policy"]}<|eot_id|>
09/09/2024 16:20:17 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 16:20:17 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
