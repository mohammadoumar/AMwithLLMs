{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.988864142538976,
  "eval_steps": 500,
  "global_step": 1120,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.044543429844097995,
      "grad_norm": 1.486271858215332,
      "learning_rate": 4.017857142857143e-06,
      "loss": 0.6797,
      "step": 10
    },
    {
      "epoch": 0.08908685968819599,
      "grad_norm": 0.8280135989189148,
      "learning_rate": 8.482142857142858e-06,
      "loss": 0.0953,
      "step": 20
    },
    {
      "epoch": 0.133630289532294,
      "grad_norm": 0.6106705069541931,
      "learning_rate": 1.2946428571428574e-05,
      "loss": 0.0793,
      "step": 30
    },
    {
      "epoch": 0.17817371937639198,
      "grad_norm": 0.6755025386810303,
      "learning_rate": 1.7410714285714287e-05,
      "loss": 0.0553,
      "step": 40
    },
    {
      "epoch": 0.22271714922049,
      "grad_norm": 0.7484505772590637,
      "learning_rate": 2.1875e-05,
      "loss": 0.0745,
      "step": 50
    },
    {
      "epoch": 0.267260579064588,
      "grad_norm": 0.8131399750709534,
      "learning_rate": 2.6339285714285716e-05,
      "loss": 0.052,
      "step": 60
    },
    {
      "epoch": 0.311804008908686,
      "grad_norm": 0.32850387692451477,
      "learning_rate": 3.080357142857143e-05,
      "loss": 0.0326,
      "step": 70
    },
    {
      "epoch": 0.35634743875278396,
      "grad_norm": 0.405264675617218,
      "learning_rate": 3.5267857142857145e-05,
      "loss": 0.0457,
      "step": 80
    },
    {
      "epoch": 0.40089086859688194,
      "grad_norm": 0.29954004287719727,
      "learning_rate": 3.9732142857142855e-05,
      "loss": 0.0367,
      "step": 90
    },
    {
      "epoch": 0.44543429844098,
      "grad_norm": 0.34600141644477844,
      "learning_rate": 4.419642857142857e-05,
      "loss": 0.0469,
      "step": 100
    },
    {
      "epoch": 0.48997772828507796,
      "grad_norm": 0.24222348630428314,
      "learning_rate": 4.866071428571429e-05,
      "loss": 0.0456,
      "step": 110
    },
    {
      "epoch": 0.534521158129176,
      "grad_norm": 0.20530857145786285,
      "learning_rate": 4.999405067699773e-05,
      "loss": 0.0443,
      "step": 120
    },
    {
      "epoch": 0.579064587973274,
      "grad_norm": 0.036490436643362045,
      "learning_rate": 4.996491795204623e-05,
      "loss": 0.0343,
      "step": 130
    },
    {
      "epoch": 0.623608017817372,
      "grad_norm": 0.008619527332484722,
      "learning_rate": 4.991153735294049e-05,
      "loss": 0.0258,
      "step": 140
    },
    {
      "epoch": 0.6681514476614699,
      "grad_norm": 0.2528669536113739,
      "learning_rate": 4.9833960727078975e-05,
      "loss": 0.0377,
      "step": 150
    },
    {
      "epoch": 0.7126948775055679,
      "grad_norm": 0.8612486720085144,
      "learning_rate": 4.9732263422927315e-05,
      "loss": 0.0535,
      "step": 160
    },
    {
      "epoch": 0.7572383073496659,
      "grad_norm": 0.3990365266799927,
      "learning_rate": 4.960654421683386e-05,
      "loss": 0.0491,
      "step": 170
    },
    {
      "epoch": 0.8017817371937639,
      "grad_norm": 1.2097632884979248,
      "learning_rate": 4.94569252170905e-05,
      "loss": 0.0535,
      "step": 180
    },
    {
      "epoch": 0.8463251670378619,
      "grad_norm": 0.6878500580787659,
      "learning_rate": 4.9283551745331534e-05,
      "loss": 0.0337,
      "step": 190
    },
    {
      "epoch": 0.89086859688196,
      "grad_norm": 0.6218805909156799,
      "learning_rate": 4.9086592195385974e-05,
      "loss": 0.0262,
      "step": 200
    },
    {
      "epoch": 0.9354120267260579,
      "grad_norm": 0.5058399438858032,
      "learning_rate": 4.886623786972033e-05,
      "loss": 0.0355,
      "step": 210
    },
    {
      "epoch": 0.9799554565701559,
      "grad_norm": 0.5425015091896057,
      "learning_rate": 4.8622702793630756e-05,
      "loss": 0.041,
      "step": 220
    },
    {
      "epoch": 1.024498886414254,
      "grad_norm": 0.1844608634710312,
      "learning_rate": 4.838389672247585e-05,
      "loss": 0.0481,
      "step": 230
    },
    {
      "epoch": 1.069042316258352,
      "grad_norm": 0.10589539259672165,
      "learning_rate": 4.8096988312782174e-05,
      "loss": 0.0218,
      "step": 240
    },
    {
      "epoch": 1.1135857461024499,
      "grad_norm": 0.004892429802566767,
      "learning_rate": 4.778764630779183e-05,
      "loss": 0.0206,
      "step": 250
    },
    {
      "epoch": 1.158129175946548,
      "grad_norm": 0.2599962651729584,
      "learning_rate": 4.7456171164571104e-05,
      "loss": 0.0342,
      "step": 260
    },
    {
      "epoch": 1.2026726057906458,
      "grad_norm": 0.08900672197341919,
      "learning_rate": 4.7102884837615244e-05,
      "loss": 0.0277,
      "step": 270
    },
    {
      "epoch": 1.247216035634744,
      "grad_norm": 0.2475135624408722,
      "learning_rate": 4.672813046614116e-05,
      "loss": 0.0372,
      "step": 280
    },
    {
      "epoch": 1.2917594654788418,
      "grad_norm": 0.03842359036207199,
      "learning_rate": 4.6332272040803895e-05,
      "loss": 0.0227,
      "step": 290
    },
    {
      "epoch": 1.3363028953229399,
      "grad_norm": 0.10391160100698471,
      "learning_rate": 4.591569405016049e-05,
      "loss": 0.018,
      "step": 300
    },
    {
      "epoch": 1.3808463251670378,
      "grad_norm": 0.06600431352853775,
      "learning_rate": 4.54788011072248e-05,
      "loss": 0.0207,
      "step": 310
    },
    {
      "epoch": 1.4253897550111359,
      "grad_norm": 0.6530949473381042,
      "learning_rate": 4.5022017556475706e-05,
      "loss": 0.0252,
      "step": 320
    },
    {
      "epoch": 1.469933184855234,
      "grad_norm": 0.08790720999240875,
      "learning_rate": 4.454578706170075e-05,
      "loss": 0.0212,
      "step": 330
    },
    {
      "epoch": 1.5144766146993318,
      "grad_norm": 0.3344818353652954,
      "learning_rate": 4.405057217507527e-05,
      "loss": 0.0309,
      "step": 340
    },
    {
      "epoch": 1.5590200445434297,
      "grad_norm": 0.3681241571903229,
      "learning_rate": 4.3536853887895664e-05,
      "loss": 0.0363,
      "step": 350
    },
    {
      "epoch": 1.6035634743875278,
      "grad_norm": 0.8715216517448425,
      "learning_rate": 4.300513116340317e-05,
      "loss": 0.0297,
      "step": 360
    },
    {
      "epoch": 1.6481069042316259,
      "grad_norm": 0.15996551513671875,
      "learning_rate": 4.245592045215182e-05,
      "loss": 0.0225,
      "step": 370
    },
    {
      "epoch": 1.692650334075724,
      "grad_norm": 0.1942717730998993,
      "learning_rate": 4.188975519039151e-05,
      "loss": 0.0233,
      "step": 380
    },
    {
      "epoch": 1.7371937639198218,
      "grad_norm": 0.2367590367794037,
      "learning_rate": 4.130718528195303e-05,
      "loss": 0.0211,
      "step": 390
    },
    {
      "epoch": 1.7817371937639197,
      "grad_norm": 0.48004427552223206,
      "learning_rate": 4.070877656413868e-05,
      "loss": 0.0297,
      "step": 400
    },
    {
      "epoch": 1.8262806236080178,
      "grad_norm": 0.21925225853919983,
      "learning_rate": 4.009511025813694e-05,
      "loss": 0.0207,
      "step": 410
    },
    {
      "epoch": 1.8708240534521159,
      "grad_norm": 0.3649859130382538,
      "learning_rate": 3.946678240449515e-05,
      "loss": 0.0239,
      "step": 420
    },
    {
      "epoch": 1.9153674832962138,
      "grad_norm": 0.1882893294095993,
      "learning_rate": 3.882440328419848e-05,
      "loss": 0.0202,
      "step": 430
    },
    {
      "epoch": 1.9599109131403119,
      "grad_norm": 0.2199024111032486,
      "learning_rate": 3.816859682591752e-05,
      "loss": 0.0316,
      "step": 440
    },
    {
      "epoch": 2.0044543429844097,
      "grad_norm": 0.13334921002388,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.024,
      "step": 450
    },
    {
      "epoch": 2.048997772828508,
      "grad_norm": 0.22999776899814606,
      "learning_rate": 3.6819262199795676e-05,
      "loss": 0.0116,
      "step": 460
    },
    {
      "epoch": 2.093541202672606,
      "grad_norm": 0.6037943363189697,
      "learning_rate": 3.6127044610914804e-05,
      "loss": 0.015,
      "step": 470
    },
    {
      "epoch": 2.138084632516704,
      "grad_norm": 0.25847315788269043,
      "learning_rate": 3.542401956903321e-05,
      "loss": 0.0164,
      "step": 480
    },
    {
      "epoch": 2.1826280623608016,
      "grad_norm": 0.11351197957992554,
      "learning_rate": 3.471086990686737e-05,
      "loss": 0.0181,
      "step": 490
    },
    {
      "epoch": 2.2271714922048997,
      "grad_norm": 0.051167991012334824,
      "learning_rate": 3.398828829095419e-05,
      "loss": 0.011,
      "step": 500
    },
    {
      "epoch": 2.271714922048998,
      "grad_norm": 0.15739144384860992,
      "learning_rate": 3.3256976548879184e-05,
      "loss": 0.0174,
      "step": 510
    },
    {
      "epoch": 2.316258351893096,
      "grad_norm": 0.3987313508987427,
      "learning_rate": 3.251764498760683e-05,
      "loss": 0.016,
      "step": 520
    },
    {
      "epoch": 2.3608017817371936,
      "grad_norm": 0.18237198889255524,
      "learning_rate": 3.177101170357513e-05,
      "loss": 0.0089,
      "step": 530
    },
    {
      "epoch": 2.4053452115812917,
      "grad_norm": 0.3583185076713562,
      "learning_rate": 3.101780188522433e-05,
      "loss": 0.0135,
      "step": 540
    },
    {
      "epoch": 2.4498886414253898,
      "grad_norm": 0.0012351319892331958,
      "learning_rate": 3.0258747108637397e-05,
      "loss": 0.0116,
      "step": 550
    },
    {
      "epoch": 2.494432071269488,
      "grad_norm": 0.20208239555358887,
      "learning_rate": 2.949458462697632e-05,
      "loss": 0.0271,
      "step": 560
    },
    {
      "epoch": 2.538975501113586,
      "grad_norm": 0.1261892467737198,
      "learning_rate": 2.872605665440436e-05,
      "loss": 0.0124,
      "step": 570
    },
    {
      "epoch": 2.5835189309576836,
      "grad_norm": 0.0032058479264378548,
      "learning_rate": 2.7953909645189825e-05,
      "loss": 0.0141,
      "step": 580
    },
    {
      "epoch": 2.6280623608017817,
      "grad_norm": 0.11899396777153015,
      "learning_rate": 2.717889356869146e-05,
      "loss": 0.0177,
      "step": 590
    },
    {
      "epoch": 2.6726057906458798,
      "grad_norm": 0.2974337339401245,
      "learning_rate": 2.6401761180929797e-05,
      "loss": 0.0227,
      "step": 600
    },
    {
      "epoch": 2.717149220489978,
      "grad_norm": 0.13843266665935516,
      "learning_rate": 2.5623267293451826e-05,
      "loss": 0.0155,
      "step": 610
    },
    {
      "epoch": 2.7616926503340755,
      "grad_norm": 0.4871245324611664,
      "learning_rate": 2.484416804019916e-05,
      "loss": 0.027,
      "step": 620
    },
    {
      "epoch": 2.8062360801781736,
      "grad_norm": 0.12185034155845642,
      "learning_rate": 2.4065220143091864e-05,
      "loss": 0.0207,
      "step": 630
    },
    {
      "epoch": 2.8507795100222717,
      "grad_norm": 0.2371405065059662,
      "learning_rate": 2.3287180177041085e-05,
      "loss": 0.0123,
      "step": 640
    },
    {
      "epoch": 2.89532293986637,
      "grad_norm": 0.16666646301746368,
      "learning_rate": 2.251080383510459e-05,
      "loss": 0.0161,
      "step": 650
    },
    {
      "epoch": 2.939866369710468,
      "grad_norm": 0.1387941539287567,
      "learning_rate": 2.173684519449872e-05,
      "loss": 0.0218,
      "step": 660
    },
    {
      "epoch": 2.984409799554566,
      "grad_norm": 0.07015864551067352,
      "learning_rate": 2.0966055984179832e-05,
      "loss": 0.0148,
      "step": 670
    },
    {
      "epoch": 3.0289532293986636,
      "grad_norm": 0.16571444272994995,
      "learning_rate": 2.01991848547066e-05,
      "loss": 0.0105,
      "step": 680
    },
    {
      "epoch": 3.0734966592427617,
      "grad_norm": 0.017984887585043907,
      "learning_rate": 1.9436976651092144e-05,
      "loss": 0.0045,
      "step": 690
    },
    {
      "epoch": 3.11804008908686,
      "grad_norm": 0.034024905413389206,
      "learning_rate": 1.8680171689352562e-05,
      "loss": 0.0049,
      "step": 700
    },
    {
      "epoch": 3.1625835189309575,
      "grad_norm": 0.18465672433376312,
      "learning_rate": 1.7929505037454315e-05,
      "loss": 0.0075,
      "step": 710
    },
    {
      "epoch": 3.2071269487750556,
      "grad_norm": 0.060605160892009735,
      "learning_rate": 1.7185705801358892e-05,
      "loss": 0.0072,
      "step": 720
    },
    {
      "epoch": 3.2516703786191536,
      "grad_norm": 0.07747386395931244,
      "learning_rate": 1.6449496416858284e-05,
      "loss": 0.0151,
      "step": 730
    },
    {
      "epoch": 3.2962138084632517,
      "grad_norm": 0.14488068222999573,
      "learning_rate": 1.5721591947889053e-05,
      "loss": 0.0094,
      "step": 740
    },
    {
      "epoch": 3.34075723830735,
      "grad_norm": 0.1641593724489212,
      "learning_rate": 1.500269939200648e-05,
      "loss": 0.0071,
      "step": 750
    },
    {
      "epoch": 3.3853006681514475,
      "grad_norm": 0.04164399579167366,
      "learning_rate": 1.429351699369343e-05,
      "loss": 0.0178,
      "step": 760
    },
    {
      "epoch": 3.4298440979955456,
      "grad_norm": 0.17366081476211548,
      "learning_rate": 1.3594733566170926e-05,
      "loss": 0.0065,
      "step": 770
    },
    {
      "epoch": 3.4743875278396437,
      "grad_norm": 0.03960200399160385,
      "learning_rate": 1.2907027822369005e-05,
      "loss": 0.004,
      "step": 780
    },
    {
      "epoch": 3.5189309576837418,
      "grad_norm": 0.5757846832275391,
      "learning_rate": 1.2231067715707867e-05,
      "loss": 0.0055,
      "step": 790
    },
    {
      "epoch": 3.5634743875278394,
      "grad_norm": 0.024691417813301086,
      "learning_rate": 1.1567509791329401e-05,
      "loss": 0.008,
      "step": 800
    },
    {
      "epoch": 3.6080178173719375,
      "grad_norm": 0.07375093549489975,
      "learning_rate": 1.0916998548409449e-05,
      "loss": 0.0068,
      "step": 810
    },
    {
      "epoch": 3.6525612472160356,
      "grad_norm": 0.043500859290361404,
      "learning_rate": 1.0280165814169884e-05,
      "loss": 0.005,
      "step": 820
    },
    {
      "epoch": 3.6971046770601337,
      "grad_norm": 0.01026364415884018,
      "learning_rate": 9.65763013019882e-06,
      "loss": 0.0043,
      "step": 830
    },
    {
      "epoch": 3.7416481069042318,
      "grad_norm": 0.064612977206707,
      "learning_rate": 9.049996151674789e-06,
      "loss": 0.0073,
      "step": 840
    },
    {
      "epoch": 3.78619153674833,
      "grad_norm": 0.19554008543491364,
      "learning_rate": 8.45785406007852e-06,
      "loss": 0.0114,
      "step": 850
    },
    {
      "epoch": 3.8307349665924275,
      "grad_norm": 0.3605819344520569,
      "learning_rate": 7.881778989962662e-06,
      "loss": 0.0045,
      "step": 860
    },
    {
      "epoch": 3.8752783964365256,
      "grad_norm": 0.27159982919692993,
      "learning_rate": 7.3223304703363135e-06,
      "loss": 0.0048,
      "step": 870
    },
    {
      "epoch": 3.9198218262806237,
      "grad_norm": 0.0017510289326310158,
      "learning_rate": 6.780051881206792e-06,
      "loss": 0.004,
      "step": 880
    },
    {
      "epoch": 3.9643652561247213,
      "grad_norm": 0.2317109853029251,
      "learning_rate": 6.255469925806643e-06,
      "loss": 0.0106,
      "step": 890
    },
    {
      "epoch": 4.008908685968819,
      "grad_norm": 0.07145106047391891,
      "learning_rate": 5.749094119018431e-06,
      "loss": 0.0043,
      "step": 900
    },
    {
      "epoch": 4.0534521158129175,
      "grad_norm": 0.02846544049680233,
      "learning_rate": 5.261416292494117e-06,
      "loss": 0.0032,
      "step": 910
    },
    {
      "epoch": 4.097995545657016,
      "grad_norm": 0.02987564541399479,
      "learning_rate": 4.79291011694987e-06,
      "loss": 0.0037,
      "step": 920
    },
    {
      "epoch": 4.142538975501114,
      "grad_norm": 0.0547742024064064,
      "learning_rate": 4.344030642100133e-06,
      "loss": 0.0025,
      "step": 930
    },
    {
      "epoch": 4.187082405345212,
      "grad_norm": 0.23607884347438812,
      "learning_rate": 3.9152138546778625e-06,
      "loss": 0.003,
      "step": 940
    },
    {
      "epoch": 4.23162583518931,
      "grad_norm": 0.013957308605313301,
      "learning_rate": 3.5068762549702427e-06,
      "loss": 0.0006,
      "step": 950
    },
    {
      "epoch": 4.276169265033408,
      "grad_norm": 0.3023984730243683,
      "learning_rate": 3.119414452281158e-06,
      "loss": 0.0025,
      "step": 960
    },
    {
      "epoch": 4.320712694877505,
      "grad_norm": 0.03299376368522644,
      "learning_rate": 2.7532047797132867e-06,
      "loss": 0.0038,
      "step": 970
    },
    {
      "epoch": 4.365256124721603,
      "grad_norm": 0.09161108732223511,
      "learning_rate": 2.4086029286440882e-06,
      "loss": 0.0014,
      "step": 980
    },
    {
      "epoch": 4.409799554565701,
      "grad_norm": 0.04481474310159683,
      "learning_rate": 2.0859436032505952e-06,
      "loss": 0.0021,
      "step": 990
    },
    {
      "epoch": 4.4543429844097995,
      "grad_norm": 0.013636688701808453,
      "learning_rate": 1.7855401954186612e-06,
      "loss": 0.0016,
      "step": 1000
    },
    {
      "epoch": 4.498886414253898,
      "grad_norm": 0.26149946451187134,
      "learning_rate": 1.5076844803522922e-06,
      "loss": 0.0033,
      "step": 1010
    },
    {
      "epoch": 4.543429844097996,
      "grad_norm": 0.054645173251628876,
      "learning_rate": 1.25264633317885e-06,
      "loss": 0.0014,
      "step": 1020
    },
    {
      "epoch": 4.587973273942094,
      "grad_norm": 0.020780202001333237,
      "learning_rate": 1.020673466825306e-06,
      "loss": 0.0006,
      "step": 1030
    },
    {
      "epoch": 4.632516703786192,
      "grad_norm": 0.009569312445819378,
      "learning_rate": 8.119911914200973e-07,
      "loss": 0.0033,
      "step": 1040
    },
    {
      "epoch": 4.67706013363029,
      "grad_norm": 0.0016834558919072151,
      "learning_rate": 6.268021954544096e-07,
      "loss": 0.0027,
      "step": 1050
    },
    {
      "epoch": 4.721603563474387,
      "grad_norm": 0.36021530628204346,
      "learning_rate": 4.6528634891530867e-07,
      "loss": 0.0034,
      "step": 1060
    },
    {
      "epoch": 4.766146993318485,
      "grad_norm": 0.009538449347019196,
      "learning_rate": 3.276005285819728e-07,
      "loss": 0.006,
      "step": 1070
    },
    {
      "epoch": 4.810690423162583,
      "grad_norm": 0.02907608263194561,
      "learning_rate": 2.1387846565474045e-07,
      "loss": 0.0013,
      "step": 1080
    },
    {
      "epoch": 4.855233853006681,
      "grad_norm": 0.19249562919139862,
      "learning_rate": 1.2423061586496477e-07,
      "loss": 0.009,
      "step": 1090
    },
    {
      "epoch": 4.8997772828507795,
      "grad_norm": 0.08802404999732971,
      "learning_rate": 5.874405219177814e-08,
      "loss": 0.0031,
      "step": 1100
    },
    {
      "epoch": 4.944320712694878,
      "grad_norm": 0.07008452713489532,
      "learning_rate": 1.7482380290034794e-08,
      "loss": 0.0013,
      "step": 1110
    },
    {
      "epoch": 4.988864142538976,
      "grad_norm": 0.082894466817379,
      "learning_rate": 4.856767115452021e-10,
      "loss": 0.0016,
      "step": 1120
    }
  ],
  "logging_steps": 10,
  "max_steps": 1120,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 3000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.10540363268096e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
