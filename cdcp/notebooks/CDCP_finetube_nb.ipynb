{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oHFCsV0z-Jw"
   },
   "source": [
    "# Finetune LLaMA 3.1 on the CDCP dataset on the ATC task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr7rB3szzhtx"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "giM74oK1rRIH",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %cd ..\n",
    "# %rm -rf LLaMA-Factory\n",
    "# !git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "# %cd LLaMA-Factory\n",
    "# %ls\n",
    "# !pip install -e .[torch,bitsandbytes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip uninstall -y pydantic\n",
    "# !pip install pydantic==1.10.9 # \n",
    "\n",
    "# !pip uninstall -y gradio\n",
    "# !pip install gradio==3.48.0\n",
    "\n",
    "# !pip uninstall -y bitsandbytes\n",
    "# !pip install --upgrade bitsandbytes\n",
    "\n",
    "# !pip install tqdm\n",
    "# !pip install ipywidgets\n",
    "# !pip install scikit-learn\n",
    "\n",
    "# Restart kernel afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import inspect\n",
    "import argparse\n",
    "import subprocess\n",
    "\n",
    "# sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from llamafactory.chat import ChatModel\n",
    "from llamafactory.extras.misc import torch_gc\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:    \n",
    "    assert torch.cuda.is_available() is True\n",
    "    \n",
    "except AssertionError:\n",
    "    \n",
    "    print(\"Please set up a GPU before using LLaMA Factory...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep  9 15:50:10 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 PCIe               Off |   00000000:82:00.0 Off |                    0 |\n",
      "| N/A   51C    P0             85W /  350W |       4MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# key = \"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\"\n",
    "# login(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "TASK = \"acc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = Path(os.path.dirname(os.path.abspath(\"__file__\"))).as_posix()\n",
    "cdcp_dir = Path(current_dir).parent.absolute().as_posix()\n",
    "parent_dir = Path(cdcp_dir).parent.absolute().as_posix()\n",
    "sys.path.append(os.path.abspath(cdcp_dir))\n",
    "from utils.post_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = parent_dir\n",
    "DATASET_DIR = os.path.join(cdcp_dir, \"datasets\")\n",
    "LLAMA_FACTORY_DIR = os.path.join(ROOT_DIR, \"LLaMA-Factory\")\n",
    "OUTPUT_DIR = os.path.join(cdcp_dir, \"finetuned_models\", f\"\"\"CDCP_{TASK}_{BASE_MODEL.split(\"/\")[1]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeYs5Lz-QJYk"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# *** TRAIN DATASET *** #\n",
    "\n",
    "train_dataset_name = f\"\"\"CDCP_{TASK}_train.json\"\"\"\n",
    "train_dataset_file = os.path.join(DATASET_DIR, train_dataset_name)\n",
    "\n",
    "# *** TEST DATASET *** #\n",
    "\n",
    "test_dataset_name = f\"\"\"CDCP_{TASK}_test.json\"\"\"\n",
    "test_dataset_file = os.path.join(DATASET_DIR, test_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit /nfs/scratch/umushtaq/coling_2025/cdcp/datasets/CDCP_acc_train.json /nfs/scratch/umushtaq/coling_2025/cdcp/datasets/CDCP_acc_test.json\n"
     ]
    }
   ],
   "source": [
    "print(BASE_MODEL, train_dataset_file, test_dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgR3UFhB0Ifq"
   },
   "source": [
    "## Fine-tune Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update dataset info file in LLaMA Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(cdcp_dir, \"ft_arg_files\")):\n",
    "    os.mkdir(os.path.join(cdcp_dir, \"ft_arg_files\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = os.path.join(cdcp_dir, \"ft_arg_files\", f\"\"\"{train_dataset_name.split(\".\")[0].split(\"train\")[0]}{BASE_MODEL.split(\"/\")[1]}.json\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info_line =  {\n",
    "  \"file_name\": f\"{train_dataset_file}\",\n",
    "  \"columns\": {\n",
    "    \"prompt\": \"instruction\",\n",
    "    \"query\": \"input\",\n",
    "    \"response\": \"output\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(LLAMA_FACTORY_DIR, \"data/dataset_info.json\"), \"r\") as jsonFile:\n",
    "    data = json.load(jsonFile)\n",
    "\n",
    "data[\"cdcp\"] = dataset_info_line\n",
    "\n",
    "with open(os.path.join(LLAMA_FACTORY_DIR, \"data/dataset_info.json\"), \"w\") as jsonFile:\n",
    "    json.dump(data, jsonFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Download and Args File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_downloads = os.path.join(ROOT_DIR, \"model_downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nfs/scratch/umushtaq/coling_2025/model_downloads'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCHS = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "CS0Qk5OR0i4Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = dict(\n",
    "  stage=\"sft\",                           # do supervised fine-tuning\n",
    "  do_train=True,\n",
    "  model_name_or_path=BASE_MODEL,         # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  dataset=\"cdcp\",                     # use alpaca and identity datasets\n",
    "  cache_dir=model_downloads,\n",
    "  template=\"llama3\",                     # use llama3 prompt template\n",
    "  finetuning_type=\"lora\",                # use LoRA adapters to save memory\n",
    "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
    "  output_dir=OUTPUT_DIR,                 # the path to save LoRA adapters\n",
    "  overwrite_output_dir=True,             # overrides existing output contents\n",
    "  per_device_train_batch_size=2,         # the batch size\n",
    "  gradient_accumulation_steps=4,         # the gradient accumulation steps\n",
    "  lr_scheduler_type=\"cosine\",            # use cosine learning rate scheduler\n",
    "  logging_steps=100,                      # log every 10 steps\n",
    "  warmup_ratio=0.1,                      # use warmup scheduler\n",
    "  save_steps=3000,                       # save checkpoint every 1000 steps\n",
    "  learning_rate=5e-5,                    # the learning rate\n",
    "  num_train_epochs=NB_EPOCHS,            # the epochs of training\n",
    "  max_samples=2000,                       # use 500 examples in each dataset\n",
    "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
    "  quantization_bit=4,                    # use 4-bit QLoRA\n",
    "  loraplus_lr_ratio=16.0,                # use LoRA+ algorithm with lambda=16.0\n",
    "  fp16=True,                             # use float16 mixed precision training\n",
    "  report_to=\"none\"                       # discards wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.dump(args, open(train_file, \"w\", encoding=\"utf-8\"), indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Fine-tune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = subprocess.Popen([\"llamafactory-cli\", \"train\", train_file], cwd=LLAMA_FACTORY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/09/2024 15:50:16 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "09/09/2024 15:50:16 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-09-09 15:50:16,225 >> loading configuration file config.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-09-09 15:50:16,226 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-09 15:50:16,350 >> loading file tokenizer.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-09 15:50:16,351 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-09 15:50:16,351 >> loading file special_tokens_map.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-09 15:50:16,351 >> loading file tokenizer_config.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-09-09 15:50:16,515 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:733] 2024-09-09 15:50:17,009 >> loading configuration file config.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-09-09 15:50:17,009 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-09 15:50:17,138 >> loading file tokenizer.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-09 15:50:17,138 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-09 15:50:17,138 >> loading file special_tokens_map.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-09 15:50:17,138 >> loading file tokenizer_config.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-09-09 15:50:17,294 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/09/2024 15:50:17 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "09/09/2024 15:50:17 - INFO - llamafactory.data.loader - Loading dataset /nfs/scratch/umushtaq/coling_2025/cdcp/datasets/CDCP_acc_train.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-09-09 15:50:17,812 >> loading configuration file config.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-09-09 15:50:17,813 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[WARNING|quantization_config.py:398] 2024-09-09 15:50:17,881 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:3678] 2024-09-09 15:50:17,882 >> loading weights file model.safetensors from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/model.safetensors\n",
      "[INFO|modeling_utils.py:1606] 2024-09-09 15:50:17,900 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1038] 2024-09-09 15:50:17,902 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"pad_token_id\": 128004\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training example:\n",
      "input_ids:\n",
      "[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 49229, 1855, 5811, 3777, 304, 279, 1495, 439, 3060, 330, 34210, 498, 330, 35890, 498, 330, 16690, 498, 330, 1985, 65556, 1, 477, 330, 970, 3343, 1472, 2011, 471, 264, 1160, 315, 5811, 3777, 4595, 11, 26549, 315, 3160, 220, 18, 11, 304, 2768, 4823, 3645, 25, 5324, 8739, 9962, 794, 4482, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 8, 93546, 1405, 1855, 2449, 330, 8739, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 34210, 498, 330, 35890, 498, 330, 16690, 498, 330, 1985, 65556, 1, 477, 330, 970, 3343, 4815, 14711, 5810, 374, 279, 1495, 25, 366, 1741, 16, 29, 1423, 323, 2254, 5590, 5718, 7170, 1304, 1670, 59358, 1790, 810, 4461, 4005, 1741, 16, 1822, 1741, 17, 29, 1789, 3187, 11, 994, 264, 1732, 889, 19755, 69944, 264, 11897, 374, 3309, 311, 2586, 311, 5590, 389, 264, 990, 1938, 11, 814, 1253, 387, 9770, 311, 5268, 1990, 264, 1670, 19971, 323, 872, 2683, 4005, 1741, 17, 1822, 1741, 18, 29, 358, 33147, 279, 356, 11960, 33, 311, 1505, 12659, 430, 21736, 38952, 45348, 520, 84783, 3115, 28743, 11, 81374, 11, 323, 43658, 11, 477, 40240, 449, 220, 11739, 17, 72, 4005, 1741, 18, 29, 128009, 128006, 78191, 128007, 271, 5018, 8739, 9962, 794, 4482, 970, 498, 330, 970, 498, 330, 35890, 93546, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to classify each argument component in the text as either \"fact\", \"policy\", \"reference\", \"testimony\" or \"value\". You must return a list of argument component types, strictly of length 3, in following JSON format: {\"component_types\": [\"component_type (str)\", \"component_type (str)\", \"component_type (str)\"]} where each element \"component_type (str)\" is replaced by either \"fact\", \"policy\", \"reference\", \"testimony\" or \"value\". \n",
      "\n",
      "### Here is the text: <AC1>State and local court rules sometimes make default judgments much more likely.</AC1><AC2> For example, when a person who allegedly owes a debt is told to come to court on a work day, they may be forced to choose between a default judgment and their job.</AC2><AC3> I urge the CFPB to find practices that involve scheduling hearings at inconvenient times unfair, deceptive, and abusive, or inconsistent with 1692i.</AC3><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\"component_types\": [\"value\", \"value\", \"policy\"]}<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 8739, 9962, 794, 4482, 970, 498, 330, 970, 498, 330, 35890, 93546, 128009]\n",
      "labels:\n",
      "{\"component_types\": [\"value\", \"value\", \"policy\"]}<|eot_id|>\n",
      "09/09/2024 15:50:17 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
      "09/09/2024 15:50:17 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:4507] 2024-09-09 15:50:19,865 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4515] 2024-09-09 15:50:19,866 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:993] 2024-09-09 15:50:20,054 >> loading configuration file generation_config.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-09-09 15:50:20,054 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 131072,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/09/2024 15:50:20 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "09/09/2024 15:50:20 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "09/09/2024 15:50:20 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "09/09/2024 15:50:20 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "09/09/2024 15:50:20 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,q_proj,o_proj,up_proj,gate_proj,down_proj,v_proj\n",
      "09/09/2024 15:50:20 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
      "09/09/2024 15:50:20 - WARNING - llamafactory.train.callbacks - Previous trainer log in this folder will be deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:648] 2024-09-09 15:50:20,338 >> Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/09/2024 15:50:21 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2134] 2024-09-09 15:50:21,104 >> ***** Running training *****\n",
      "[INFO|trainer.py:2135] 2024-09-09 15:50:21,104 >>   Num examples = 580\n",
      "[INFO|trainer.py:2136] 2024-09-09 15:50:21,104 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2137] 2024-09-09 15:50:21,104 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2140] 2024-09-09 15:50:21,104 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2141] 2024-09-09 15:50:21,104 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2142] 2024-09-09 15:50:21,104 >>   Total optimization steps = 15\n",
      "[INFO|trainer.py:2143] 2024-09-09 15:50:21,107 >>   Number of trainable parameters = 20,971,520\n",
      "100%|██████████| 15/15 [00:26<00:00,  1.73s/it][INFO|trainer.py:3503] 2024-09-09 15:50:47,561 >> Saving model checkpoint to /nfs/scratch/umushtaq/coling_2025/cdcp/finetuned_models/CDCP_acc_Meta-Llama-3.1-8B-Instruct-bnb-4bit/checkpoint-15\n",
      "[INFO|configuration_utils.py:733] 2024-09-09 15:50:47,831 >> loading configuration file config.json from cache at /home/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-09-09 15:50:47,831 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-09-09 15:50:47,962 >> tokenizer config file saved in /nfs/scratch/umushtaq/coling_2025/cdcp/finetuned_models/CDCP_acc_Meta-Llama-3.1-8B-Instruct-bnb-4bit/checkpoint-15/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-09-09 15:50:47,963 >> Special tokens file saved in /nfs/scratch/umushtaq/coling_2025/cdcp/finetuned_models/CDCP_acc_Meta-Llama-3.1-8B-Instruct-bnb-4bit/checkpoint-15/special_tokens_map.json\n",
      "[INFO|trainer.py:2394] 2024-09-09 15:50:48,304 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 15/15 [00:27<00:00,  1.81s/it]\n",
      "[INFO|trainer.py:3503] 2024-09-09 15:50:48,306 >> Saving model checkpoint to /nfs/scratch/umushtaq/coling_2025/cdcp/finetuned_models/CDCP_acc_Meta-Llama-3.1-8B-Instruct-bnb-4bit\n",
      "[INFO|configuration_utils.py:733] 2024-09-09 15:50:48,571 >> loading configuration file config.json from cache at /home/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-09-09 15:50:48,572 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-09-09 15:50:48,702 >> tokenizer config file saved in /nfs/scratch/umushtaq/coling_2025/cdcp/finetuned_models/CDCP_acc_Meta-Llama-3.1-8B-Instruct-bnb-4bit/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-09-09 15:50:48,704 >> Special tokens file saved in /nfs/scratch/umushtaq/coling_2025/cdcp/finetuned_models/CDCP_acc_Meta-Llama-3.1-8B-Instruct-bnb-4bit/special_tokens_map.json\n",
      "[INFO|modelcard.py:449] 2024-09-09 15:50:48,788 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 27.1974, 'train_samples_per_second': 4.265, 'train_steps_per_second': 0.552, 'train_loss': 0.2880144437154134, 'epoch': 0.21}\n",
      "***** train metrics *****\n",
      "  epoch                    =     0.2069\n",
      "  total_flos               =  2251414GF\n",
      "  train_loss               =      0.288\n",
      "  train_runtime            = 0:00:27.19\n",
      "  train_samples_per_second =      4.265\n",
      "  train_steps_per_second   =      0.552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVNaC-xS5N40"
   },
   "source": [
    "## Inference on the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['checkpoint-15',\n",
       " 'README.md',\n",
       " 'adapter_model.safetensors',\n",
       " 'adapter_config.json',\n",
       " 'tokenizer_config.json',\n",
       " 'special_tokens_map.json',\n",
       " 'tokenizer.json',\n",
       " 'training_args.bin',\n",
       " 'train_results.json',\n",
       " 'all_results.json',\n",
       " 'trainer_state.json',\n",
       " 'CDCP_acc_results_0.2.pickle',\n",
       " 'classification_report.pickle',\n",
       " 'trainer_log.jsonl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "oh8H9A_25SF9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = dict(\n",
    "  model_name_or_path=BASE_MODEL, # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "    cache_dir=model_downloads,\n",
    "  adapter_name_or_path=OUTPUT_DIR,            # load the saved LoRA adapters\n",
    "  template=\"llama3\",                     # same to the one in training\n",
    "  finetuning_type=\"lora\",                  # same to the one in training\n",
    "  quantization_bit=4,                    # load 4-bit quantized model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-09-09 15:50:50,201 >> loading configuration file config.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-09-09 15:50:50,202 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-09 15:50:50,389 >> loading file tokenizer.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-09 15:50:50,390 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-09 15:50:50,390 >> loading file special_tokens_map.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-09 15:50:50,391 >> loading file tokenizer_config.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-09-09 15:50:50,560 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:733] 2024-09-09 15:50:51,152 >> loading configuration file config.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-09-09 15:50:51,154 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-09 15:50:51,276 >> loading file tokenizer.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-09 15:50:51,277 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-09 15:50:51,278 >> loading file special_tokens_map.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-09 15:50:51,279 >> loading file tokenizer_config.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-09-09 15:50:51,441 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/09/2024 15:50:51 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-09-09 15:50:51,582 >> loading configuration file config.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-09-09 15:50:51,584 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/09/2024 15:50:51 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
      "09/09/2024 15:50:51 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "09/09/2024 15:50:51 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|quantization_config.py:398] 2024-09-09 15:50:51,619 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:3678] 2024-09-09 15:50:51,623 >> loading weights file model.safetensors from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/model.safetensors\n",
      "[INFO|modeling_utils.py:1606] 2024-09-09 15:50:51,642 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1038] 2024-09-09 15:50:51,645 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"pad_token_id\": 128004\n",
      "}\n",
      "\n",
      "[INFO|quantizer_bnb_4bit.py:106] 2024-09-09 15:50:51,946 >> target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "[INFO|modeling_utils.py:4507] 2024-09-09 15:50:53,047 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4515] 2024-09-09 15:50:53,049 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:993] 2024-09-09 15:50:53,173 >> loading configuration file generation_config.json from cache at /nfs/scratch/umushtaq/coling_2025/model_downloads/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/90ff2083c372e1c422abd1b5596cbba1b994a170/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-09-09 15:50:53,175 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 131072,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/09/2024 15:50:53 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "09/09/2024 15:50:53 - INFO - llamafactory.model.adapter - Loaded adapter(s): /nfs/scratch/umushtaq/coling_2025/cdcp/finetuned_models/CDCP_acc_Meta-Llama-3.1-8B-Instruct-bnb-4bit\n",
      "09/09/2024 15:50:53 - INFO - llamafactory.model.loader - all params: 8,051,232,768\n"
     ]
    }
   ],
   "source": [
    "model = ChatModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_dataset_file, \"r+\") as fh:\n",
    "    test_dataset = json.load(fh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_prompts = []\n",
    "test_grounds = []\n",
    "\n",
    "for sample in test_dataset:\n",
    "    test_prompts.append(\"\\nUser:\" + sample[\"instruction\"] + sample[\"input\"])\n",
    "    test_grounds.append(sample[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_prompts), len(test_grounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:54<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "test_predictions = []\n",
    "\n",
    "for prompt in tqdm(test_prompts):\n",
    "\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = \"\"\n",
    "    \n",
    "    for new_text in model.stream_chat(messages):\n",
    "        #print(new_text, end=\"\", flush=True)\n",
    "        response += new_text\n",
    "        #print()\n",
    "    test_predictions.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_DIR, f\"\"\"CDCP_{TASK}_results_{NB_EPOCHS}.pickle\"\"\"), 'wb') as fh:\n",
    "    results_d = {\"ground_truths\": test_grounds,\n",
    "                 \"predictions\": test_predictions    \n",
    "        \n",
    "    }\n",
    "    pickle.dump(results_d, fh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_DIR, f\"\"\"CDCP_{TASK}_results_{NB_EPOCHS}.pickle\"\"\"), \"rb\") as fh:\n",
    "        \n",
    "        results = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ground_truths': ['{\"component_types\": [\"fact\", \"value\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"value\", \"policy\", \"policy\", \"policy\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\", \"testimony\", \"value\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"value\", \"value\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"value\", \"fact\", \"testimony\", \"testimony\", \"testimony\"]}',\n",
       "  '{\"component_types\": [\"value\", \"testimony\", \"value\", \"value\", \"policy\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"fact\", \"fact\", \"policy\", \"fact\", \"value\", \"fact\", \"fact\", \"fact\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"value\", \"testimony\", \"testimony\", \"testimony\", \"policy\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"value\", \"policy\", \"value\", \"policy\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"fact\", \"value\", \"testimony\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"fact\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"fact\", \"fact\", \"fact\"]}',\n",
       "  '{\"component_types\": [\"value\", \"testimony\", \"testimony\", \"testimony\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\"]}',\n",
       "  '{\"component_types\": [\"value\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\", \"value\", \"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"value\", \"fact\", \"fact\", \"policy\", \"fact\", \"policy\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"testimony\", \"value\", \"value\", \"fact\", \"fact\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\", \"testimony\", \"testimony\", \"value\", \"value\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"fact\", \"fact\", \"fact\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"policy\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"fact\", \"testimony\", \"value\", \"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"fact\", \"policy\", \"policy\", \"policy\", \"testimony\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"fact\", \"value\", \"fact\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"testimony\", \"value\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"fact\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"policy\", \"fact\"]}',\n",
       "  '{\"component_types\": [\"value\", \"testimony\", \"testimony\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"value\", \"value\", \"testimony\", \"value\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"fact\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"value\", \"testimony\", \"testimony\", \"value\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"fact\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"fact\", \"testimony\", \"fact\", \"fact\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"policy\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"fact\", \"fact\", \"value\", \"policy\", \"value\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"fact\"]}',\n",
       "  '{\"component_types\": [\"value\", \"fact\", \"value\", \"value\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"fact\", \"value\", \"policy\", \"value\", \"policy\", \"value\", \"fact\", \"value\", \"value\", \"policy\", \"value\", \"value\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"fact\", \"value\", \"testimony\", \"fact\", \"fact\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"testimony\", \"testimony\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"fact\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"value\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"fact\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"policy\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"value\", \"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"testimony\", \"testimony\", \"testimony\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"testimony\", \"value\", \"value\", \"fact\", \"value\", \"value\", \"value\", \"value\", \"value\", \"policy\", \"policy\", \"value\", \"value\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"value\", \"fact\", \"value\", \"value\", \"testimony\", \"testimony\"]}',\n",
       "  '{\"component_types\": [\"value\", \"testimony\", \"testimony\", \"value\", \"testimony\", \"value\", \"value\", \"value\", \"value\", \"testimony\", \"value\", \"value\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"value\", \"value\", \"value\", \"value\", \"value\", \"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"fact\", \"testimony\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"value\", \"value\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"testimony\", \"testimony\", \"testimony\", \"policy\", \"policy\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"value\", \"policy\", \"policy\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"fact\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"value\", \"testimony\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"fact\", \"fact\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\", \"value\", \"testimony\", \"value\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"fact\", \"fact\", \"value\", \"value\", \"value\", \"policy\", \"value\", \"fact\", \"fact\", \"value\", \"value\", \"policy\", \"policy\", \"value\", \"value\", \"policy\", \"policy\", \"policy\", \"policy\", \"value\", \"value\", \"value\", \"policy\", \"policy\", \"value\", \"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"value\", \"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"value\", \"fact\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\", \"fact\", \"value\", \"value\", \"fact\", \"value\", \"value\", \"value\", \"value\", \"policy\", \"value\", \"policy\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"value\", \"value\", \"value\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"fact\", \"fact\", \"fact\", \"value\", \"value\", \"fact\", \"fact\", \"fact\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"testimony\", \"policy\", \"policy\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"fact\", \"value\", \"value\", \"value\", \"value\", \"testimony\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\", \"value\", \"value\", \"fact\", \"fact\", \"value\", \"policy\", \"value\", \"value\", \"fact\", \"fact\", \"value\", \"value\", \"value\", \"value\", \"policy\", \"policy\", \"value\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"fact\", \"value\", \"value\", \"testimony\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"value\", \"policy\", \"testimony\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\", \"testimony\", \"testimony\", \"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"fact\", \"fact\", \"fact\", \"policy\", \"value\", \"testimony\", \"value\", \"value\", \"testimony\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"fact\", \"value\", \"value\", \"value\", \"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"policy\", \"value\", \"value\", \"value\", \"value\", \"policy\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"value\", \"value\", \"value\", \"value\", \"fact\", \"value\", \"policy\", \"fact\", \"fact\", \"value\", \"fact\", \"fact\", \"policy\", \"policy\", \"fact\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"value\", \"value\", \"value\", \"fact\", \"value\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\", \"fact\", \"fact\", \"value\", \"policy\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"policy\", \"value\", \"testimony\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"policy\", \"value\", \"policy\", \"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"fact\", \"value\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"fact\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"fact\", \"value\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"value\", \"testimony\", \"testimony\", \"value\", \"value\", \"value\", \"value\", \"fact\", \"value\", \"value\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"value\", \"policy\", \"value\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"testimony\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"value\", \"value\", \"value\", \"value\", \"value\", \"policy\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"value\", \"value\", \"testimony\", \"value\", \"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"testimony\", \"testimony\", \"testimony\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"fact\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"value\", \"policy\", \"value\", \"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"value\", \"value\", \"value\", \"testimony\", \"fact\", \"value\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\", \"value\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\", \"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\", \"fact\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"value\", \"value\", \"fact\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"testimony\", \"testimony\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"value\", \"fact\", \"value\", \"value\", \"value\", \"value\", \"value\", \"fact\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"fact\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"testimony\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"value\", \"fact\", \"testimony\", \"policy\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"value\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"value\", \"fact\", \"policy\", \"value\", \"value\", \"value\", \"fact\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"policy\", \"policy\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"fact\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\"]}',\n",
       "  '{\"component_types\": [\"value\", \"fact\", \"fact\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"policy\", \"policy\", \"policy\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"testimony\", \"fact\", \"testimony\", \"fact\", \"testimony\", \"testimony\", \"policy\", \"fact\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"policy\", \"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"fact\", \"value\", \"testimony\", \"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"policy\", \"value\", \"value\", \"value\", \"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"policy\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"fact\", \"value\", \"value\", \"fact\", \"fact\", \"value\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"fact\", \"fact\", \"value\", \"value\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"value\", \"fact\", \"testimony\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"policy\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"fact\", \"value\", \"policy\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\"]}',\n",
       "  '{\"component_types\": [\"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"testimony\", \"policy\"]}',\n",
       "  '{\"component_types\": [\"reference\", \"value\", \"value\"]}'],\n",
       " 'predictions': [{'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"policy\", \"fact\", \"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"policy\", \"value\", \"testimony\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"value\", \"testimony\", \"value\", \"policy\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"reference\", \"testimony\", \"testimony\", \"testimony\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"value\", \"value\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"fact\", \"policy\", \"fact\", \"value\", \"fact\", \"fact\", \"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"fact\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"policy\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"value\", \"value\", \"policy\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"value\", \"fact\", \"value\", \"testimony\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"policy\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"value\", \"policy\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"fact\", \"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"testimony\", \"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant', 'content': '{\"component_types\": [\"fact\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"value\", \"value\", \"fact\", \"policy\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"value\", \"fact\", \"fact\", \"policy\", \"value\", \"policy\", \"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"testimony\", \"policy\", \"policy\", \"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant', 'content': '{\"component_types\": [\"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant', 'content': '{\"component_types\": [\"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"policy\", \"fact\", \"testimony\", \"testimony\", \"fact\", \"fact\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"fact\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"value\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"fact\", \"fact\", \"fact\", \"fact\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"value\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\", \"policy\", \"policy\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"testimony\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"policy\", \"policy\", \"policy\", \"testimony\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"fact\", \"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"fact\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"testimony\", \"testimony\", \"fact\", \"testimony\", \"testimony\", \"value\", \"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant', 'content': '{\"component_types\": [\"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"policy\"]}'},\n",
       "  {'role': 'assistant', 'content': '{\"component_types\": [\"policy\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"policy\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"testimony\", \"testimony\", \"value\", \"value\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"value\", \"value\", \"fact\", \"value\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"value\", \"testimony\", \"testimony\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"fact\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"testimony\", \"fact\", \"fact\", \"testimony\", \"testimony\", \"testimony\", \"testimony\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"value\", \"fact\", \"fact\", \"fact\", \"policy\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"value\", \"policy\", \"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"testimony\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"testimony\", \"fact\", \"value\", \"testimony\", \"fact\", \"inference\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"fact\", \"value\", \"policy\", \"value\", \"policy\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"fact\", \"fact\", \"value\", \"fact\", \"value\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"value\", \"fact\", \"policy\", \"fact\", \"policy\", \"value\", \"value\", \"value\", \"value\", \"policy\", \"fact\", \"value\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"value\", \"value\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"value\", \"testimony\", \"testimony\", \"fact\", \"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"value\", \"testimony\", \"testimony\", \"fact\", \"value\", \"testimony\", \"value\", \"value\", \"value\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant', 'content': '{\"component_types\": [\"fact\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"policy\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"policy\", \"policy\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"testimony\", \"testimony\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"value\", \"value\", \"value\", \"fact\", \"value\", \"value\", \"fact\", \"policy\", \"policy\", \"policy\", \"policy\", \"value\", \"value\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"testimony\", \"value\", \"fact\", \"fact\", \"value\", \"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"value\", \"testimony\", \"testimony\", \"value\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"fact\", \"fact\", \"fact\", \"fact\", \"value\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"testimony\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"fact\", \"fact\", \"policy\", \"fact\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"policy\", \"policy\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"policy\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"fact\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"value\", \"value\", \"value\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"value\", \"value\", \"policy\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"value\", \"value\", \"fact\", \"fact\", \"value\", \"fact\", \"fact\", \"value\", \"value\", \"fact\", \"fact\", \"value\", \"fact\", \"fact\", \"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"value\", \"value\", \"policy\", \"value\", \"value\", \"fact\", \"fact\", \"fact\", \"fact\", \"value\", \"value\", \"policy\", \"value\", \"fact\", \"fact\", \"value\", \"value\", \"value\", \"fact\", \"value\", \"value\", \"fact\", \"value\", \"value\", \"fact\", \"value\", \"value\", \"value\", \"value\", \"fact\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"value\", \"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"policy\", \"fact\", \"policy\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"value\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"testimony\", \"value\", \"value\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"fact\", \"fact\", \"value\", \"fact\", \"fact\", \"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant', 'content': '{\"component_types\": [\"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"fact\", \"policy\", \"policy\", \"testimony\", \"testimony\", \"value\", \"testimony\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"policy\", \"value\", \"fact\", \"fact\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"value\", \"policy\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"policy\", \"value\", \"fact\", \"fact\", \"policy\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"testimony\", \"fact\", \"value\", \"testimony\", \"value\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"fact\", \"value\", \"value\", \"policy\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"fact\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"policy\", \"fact\", \"value\", \"testimony\", \"value\", \"policy\", \"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"testimony\", \"value\", \"fact\", \"value\", \"value\", \"fact\", \"fact\", \"policy\", \"fact\", \"fact\", \"value\", \"fact\", \"fact\", \"policy\", \"policy\", \"fact\", \"value\", \"policy\", \"policy\", \"value\", \"policy\", \"fact\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"fact\", \"value\", \"fact\", \"fact\", \"fact\", \"policy\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"policy\", \"fact\", \"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"policy\", \"fact\", \"policy\", \"policy\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"fact\", \"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant', 'content': '{\"component_types\": [\"value\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"value\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"value\", \"value\", \"policy\", \"value\", \"value\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"policy\", \"fact\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"fact\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"value\", \"testimony\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"fact\", \"fact\", \"policy\", \"fact\", \"fact\", \"policy\", \"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"value\", \"value\", \"value\", \"value\", \"policy\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"value\", \"fact\", \"testimony\", \"policy\", \"policy\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"testimony\", \"fact\", \"fact\", \"policy\"]}'},\n",
       "  {'role': 'assistant', 'content': '{\"component_types\": [\"policy\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"policy\", \"policy\", \"value\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"value\", \"value\", \"fact\", \"testimony\", \"fact\", \"value\", \"policy\", \"policy\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"value\", \"value\", \"value\", \"fact\", \"fact\", \"fact\", \"value\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"value\", \"policy\"]}'},\n",
       "  {'role': 'assistant', 'content': '{\"component_types\": [\"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"value\", \"value\", \"policy\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"fact\", \"policy\", \"testimony\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"value\", \"value\", \"value\", \"testimony\", \"testimony\", \"testimony\", \"testimony\", \"fact\", \"fact\", \"value\", \"fact\", \"value\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"fact\", \"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"value\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"fact\", \"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"fact\", \"fact\", \"fact\", \"value\", \"fact\", \"fact\", \"fact\", \"fact\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"testimony\", \"fact\"]}'},\n",
       "  {'role': 'assistant', 'content': '{\"component_types\": [\"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"testimony\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"fact\", \"policy\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant', 'content': '{\"component_types\": [\"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"value\", \"fact\", \"value\", \"value\", \"value\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"testimony\", \"testimony\", \"testimony\", \"fact\", \"policy\", \"fact\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"fact\", \"fact\", \"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"testimony\", \"testimony\", \"testimony\", \"testimony\", \"testimony\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"fact\", \"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"policy\", \"fact\", \"policy\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant', 'content': '{\"component_types\": [\"policy\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"value\", \"value\", \"testimony\", \"testimony\", \"policy\", \"value\", \"fact\", \"fact\", \"fact\", \"value\", \"value\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"policy\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"fact\", \"value\", \"fact\", \"policy\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"policy\", \"value\", \"value\", \"fact\", \"policy\", \"fact\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"value\", \"policy\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"value\", \"fact\", \"fact\", \"fact\", \"value\", \"testimony\", \"testimony\", \"value\", \"fact\", \"value\", \"value\", \"fact\", \"value\", \"value\", \"value\", \"policy\", \"value\", \"policy\", \"policy\", \"value\", \"value\", \"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"value\", \"policy\", \"testimony\", \"value\", \"value\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"policy\", \"value\"]}'},\n",
       "  {'role': 'assistant', 'content': '{\"component_types\": [\"value\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"policy\", \"policy\", \"policy\", \"value\", \"fact\", \"value\", \"policy\", \"policy\", \"value\", \"value\", \"value\", \"policy\", \"fact\", \"fact\", \"value\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"component_types\": [\"reference\", \"fact\", \"policy\"]}'}]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == 'acc':\n",
    "    task_grounds, task_preds = post_process_acc(results)\n",
    "\n",
    "elif TASK == 'ari':\n",
    "    task_grounds, task_preds = post_process_ari(results)\n",
    "\n",
    "elif TASK == 'arc':\n",
    "    task_grounds, task_preds = post_process_arc(results)\n",
    "\n",
    "elif TASK == 'joint':\n",
    "    task_grounds, task_preds = post_process_joint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fact      0.305     0.765     0.436       132\n",
      "   inference      0.000     0.000     0.000         0\n",
      "      policy      0.650     0.752     0.697       153\n",
      "   reference      0.500     1.000     0.667         1\n",
      "   testimony      0.878     0.652     0.748       244\n",
      "       value      0.820     0.552     0.660       496\n",
      "\n",
      "    accuracy                          0.634      1026\n",
      "   macro avg      0.526     0.620     0.535      1026\n",
      "weighted avg      0.742     0.634     0.658      1026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umushtaq/.conda/envs/notebook/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/umushtaq/.conda/envs/notebook/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/umushtaq/.conda/envs/notebook/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(task_grounds, task_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umushtaq/.conda/envs/notebook/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/umushtaq/.conda/envs/notebook/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/umushtaq/.conda/envs/notebook/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "with open(f\"\"\"{OUTPUT_DIR}/classification_report.pickle\"\"\", 'wb') as fh:\n",
    "    \n",
    "    pickle.dump(classification_report(task_grounds, task_preds, output_dict=True), fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
