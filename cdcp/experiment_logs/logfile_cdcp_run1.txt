Running CDCP_finetune.py with arguments: unsloth/llama-3-8b-Instruct-bnb-4bit acc
09/06/2024 23:01:55 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:24683
09/06/2024 23:02:05 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/06/2024 23:02:05 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/06/2024 23:02:05 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/06/2024 23:02:05 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/06/2024 23:02:05 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/06/2024 23:02:05 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/06/2024 23:02:05 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/06/2024 23:02:05 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_acc_train.json...
09/06/2024 23:02:05 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/06/2024 23:02:06 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_acc_train.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 49229, 1855, 5811, 3777, 304, 279, 1495, 439, 3060, 330, 34210, 498, 330, 35890, 498, 330, 16690, 498, 330, 1985, 65556, 1, 477, 330, 970, 3343, 1472, 2011, 471, 264, 1160, 315, 5811, 3777, 4595, 11, 26549, 315, 3160, 220, 18, 11, 304, 2768, 4823, 3645, 25, 5324, 8739, 9962, 794, 4482, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 8, 93546, 1405, 1855, 2449, 330, 8739, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 34210, 498, 330, 35890, 498, 330, 16690, 498, 330, 1985, 65556, 1, 477, 330, 970, 3343, 4815, 14711, 5810, 374, 279, 1495, 25, 366, 1741, 16, 29, 1423, 323, 2254, 5590, 5718, 7170, 1304, 1670, 59358, 1790, 810, 4461, 4005, 1741, 16, 1822, 1741, 17, 29, 1789, 3187, 11, 994, 264, 1732, 889, 19755, 69944, 264, 11897, 374, 3309, 311, 2586, 311, 5590, 389, 264, 990, 1938, 11, 814, 1253, 387, 9770, 311, 5268, 1990, 264, 1670, 19971, 323, 872, 2683, 4005, 1741, 17, 1822, 1741, 18, 29, 358, 33147, 279, 356, 11960, 33, 311, 1505, 12659, 430, 21736, 38952, 45348, 520, 84783, 3115, 28743, 11, 81374, 11, 323, 43658, 11, 477, 40240, 449, 220, 11739, 17, 72, 4005, 1741, 18, 29, 128009, 128006, 78191, 128007, 271, 5018, 8739, 9962, 794, 4482, 970, 498, 330, 970, 498, 330, 35890, 93546, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to classify each argument component in the text as either "fact", "policy", "reference", "testimony" or "value". You must return a list of argument component types, strictly of length 3, in following JSON format: {"component_types": ["component_type (str)", "component_type (str)", "component_type (str)"]} where each element "component_type (str)" is replaced by either "fact", "policy", "reference", "testimony" or "value". 

### Here is the text: <AC1>State and local court rules sometimes make default judgments much more likely.</AC1><AC2> For example, when a person who allegedly owes a debt is told to come to court on a work day, they may be forced to choose between a default judgment and their job.</AC2><AC3> I urge the CFPB to find practices that involve scheduling hearings at inconvenient times unfair, deceptive, and abusive, or inconsistent with 1692i.</AC3><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"component_types": ["value", "value", "policy"]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 8739, 9962, 794, 4482, 970, 498, 330, 970, 498, 330, 35890, 93546, 128009]
labels:
{"component_types": ["value", "value", "policy"]}<|eot_id|>
09/06/2024 23:02:07 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/06/2024 23:02:07 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/06/2024 23:02:07 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/06/2024 23:02:07 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/06/2024 23:02:13 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/06/2024 23:02:13 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/06/2024 23:02:13 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/06/2024 23:02:13 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/06/2024 23:02:13 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,up_proj,k_proj,q_proj,v_proj,o_proj,down_proj
09/06/2024 23:02:13 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/06/2024 23:02:13 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/06/2024 23:02:13 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/06/2024 23:02:13 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/06/2024 23:02:13 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,k_proj,v_proj,gate_proj,up_proj,o_proj,down_proj
09/06/2024 23:02:13 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/06/2024 23:02:14 - WARNING - llamafactory.train.callbacks - Previous trainer log in this folder will be deleted.
09/06/2024 23:02:14 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/06/2024 23:02:14 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/06/2024 23:02:14 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.4347, 'grad_norm': 0.6436071991920471, 'learning_rate': 2.777777777777778e-05, 'epoch': 0.28}
{'loss': 0.1605, 'grad_norm': 1.5594871044158936, 'learning_rate': 4.998119881260576e-05, 'epoch': 0.55}
{'loss': 0.1481, 'grad_norm': 0.7317149043083191, 'learning_rate': 4.9326121764495596e-05, 'epoch': 0.83}
{'loss': 0.1153, 'grad_norm': 0.5661004185676575, 'learning_rate': 4.775907352415367e-05, 'epoch': 1.1}
{'loss': 0.0954, 'grad_norm': 0.9295840263366699, 'learning_rate': 4.533880175657419e-05, 'epoch': 1.38}
{'loss': 0.0798, 'grad_norm': 0.4406852424144745, 'learning_rate': 4.215604094671835e-05, 'epoch': 1.66}
{'loss': 0.0846, 'grad_norm': 0.5403568744659424, 'learning_rate': 3.8330110820042285e-05, 'epoch': 1.93}
{'loss': 0.054, 'grad_norm': 0.5997395515441895, 'learning_rate': 3.400444312011776e-05, 'epoch': 2.21}
{'loss': 0.0326, 'grad_norm': 0.5146560668945312, 'learning_rate': 2.9341204441673266e-05, 'epoch': 2.48}
{'loss': 0.0416, 'grad_norm': 0.6121879816055298, 'learning_rate': 2.4515216705704395e-05, 'epoch': 2.76}
{'loss': 0.0368, 'grad_norm': 0.43110179901123047, 'learning_rate': 1.970740319426474e-05, 'epoch': 3.03}
{'loss': 0.0157, 'grad_norm': 0.08964654058218002, 'learning_rate': 1.509800584902108e-05, 'epoch': 3.31}
{'loss': 0.0134, 'grad_norm': 0.3344857394695282, 'learning_rate': 1.085982811283654e-05, 'epoch': 3.59}
{'loss': 0.0123, 'grad_norm': 0.298018217086792, 'learning_rate': 7.1517566360525284e-06, 'epoch': 3.86}
{'loss': 0.0097, 'grad_norm': 0.46667784452438354, 'learning_rate': 4.112804714676594e-06, 'epoch': 4.14}
{'loss': 0.0088, 'grad_norm': 0.2725563049316406, 'learning_rate': 1.8569007682777417e-06, 'epoch': 4.41}
{'loss': 0.0032, 'grad_norm': 0.05229378119111061, 'learning_rate': 4.6861723431538276e-07, 'epoch': 4.69}
{'loss': 0.0032, 'grad_norm': 0.12351378053426743, 'learning_rate': 0.0, 'epoch': 4.97}
{'train_runtime': 820.7179, 'train_samples_per_second': 3.533, 'train_steps_per_second': 0.219, 'train_loss': 0.07499576980868976, 'epoch': 4.97}
***** train metrics *****
  epoch                    =     4.9655
  total_flos               = 55401222GF
  train_loss               =      0.075
  train_runtime            = 0:13:40.71
  train_samples_per_second =      3.533
  train_steps_per_second   =      0.219
09/06/2024 23:16:02 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/06/2024 23:16:03 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/06/2024 23:16:03 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/06/2024 23:16:03 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/06/2024 23:16:06 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/06/2024 23:16:06 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/finetuned_models/CDCP_acc_llama-3-8b-Instruct-bnb-4bit
09/06/2024 23:16:06 - INFO - llamafactory.model.loader - all params: 8,051,232,768
              precision    recall  f1-score   support

        fact      0.592     0.705     0.644       132
      policy      0.884     0.895     0.890       153
   reference      1.000     1.000     1.000         1
   testimony      0.942     0.861     0.899       244
       value      0.855     0.845     0.850       496

    accuracy                          0.838      1026
   macro avg      0.855     0.861     0.856      1026
weighted avg      0.846     0.838     0.841      1026

Successfully ran CDCP_finetune.py with arguments: unsloth/llama-3-8b-Instruct-bnb-4bit acc 
 
  *************** 

Running CDCP_finetune.py with arguments: unsloth/llama-3-8b-Instruct-bnb-4bit ari
09/06/2024 23:22:23 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:23801
09/06/2024 23:22:33 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/06/2024 23:22:33 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/06/2024 23:22:33 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/06/2024 23:22:33 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/06/2024 23:22:33 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/06/2024 23:22:33 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/06/2024 23:22:34 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/06/2024 23:22:34 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_ari_train.json...
09/06/2024 23:22:34 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/06/2024 23:22:35 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_ari_train.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 10765, 5811, 4398, 1990, 5811, 6956, 304, 279, 1495, 13, 1472, 2011, 471, 264, 1160, 315, 13840, 304, 279, 2768, 4823, 3645, 25, 5324, 1638, 9202, 95321, 794, 4416, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 26090, 61453, 510, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 8, 5163, 633, 14711, 5810, 374, 279, 1495, 25, 366, 1741, 15, 29, 1423, 323, 2254, 5590, 5718, 7170, 1304, 1670, 59358, 1790, 810, 4461, 4005, 1741, 15, 1822, 1741, 16, 29, 1789, 3187, 11, 994, 264, 1732, 889, 19755, 69944, 264, 11897, 374, 3309, 311, 2586, 311, 5590, 389, 264, 990, 1938, 11, 814, 1253, 387, 9770, 311, 5268, 1990, 264, 1670, 19971, 323, 872, 2683, 4005, 1741, 16, 1822, 1741, 17, 29, 358, 33147, 279, 356, 11960, 33, 311, 1505, 12659, 430, 21736, 38952, 45348, 520, 84783, 3115, 28743, 11, 81374, 11, 323, 43658, 11, 477, 40240, 449, 220, 11739, 17, 72, 4005, 1741, 17, 29, 128009, 128006, 78191, 128007, 271, 5018, 1638, 9202, 95321, 794, 4416, 15, 11, 220, 16, 1145, 510, 17, 11, 220, 15, 5163, 92, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to identify argument relations between argument components in the text. You must return a list of pairs in the following JSON format: {"list_argument_relations": [[source AC (int), target AC (int)],..., [source AC (int), target AC (int)]]}

### Here is the text: <AC0>State and local court rules sometimes make default judgments much more likely.</AC0><AC1> For example, when a person who allegedly owes a debt is told to come to court on a work day, they may be forced to choose between a default judgment and their job.</AC1><AC2> I urge the CFPB to find practices that involve scheduling hearings at inconvenient times unfair, deceptive, and abusive, or inconsistent with 1692i.</AC2><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"list_argument_relations": [[0, 1], [2, 0]]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 1638, 9202, 95321, 794, 4416, 15, 11, 220, 16, 1145, 510, 17, 11, 220, 15, 5163, 92, 128009]
labels:
{"list_argument_relations": [[0, 1], [2, 0]]}<|eot_id|>
09/06/2024 23:22:35 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/06/2024 23:22:35 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/06/2024 23:22:35 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/06/2024 23:22:35 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/06/2024 23:22:41 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/06/2024 23:22:41 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/06/2024 23:22:41 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/06/2024 23:22:41 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/06/2024 23:22:41 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,q_proj,o_proj,gate_proj,down_proj,k_proj,up_proj
09/06/2024 23:22:41 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/06/2024 23:22:41 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/06/2024 23:22:41 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/06/2024 23:22:41 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/06/2024 23:22:41 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,v_proj,gate_proj,q_proj,o_proj,k_proj,up_proj
09/06/2024 23:22:42 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/06/2024 23:22:42 - WARNING - llamafactory.train.callbacks - Previous trainer log in this folder will be deleted.
09/06/2024 23:22:42 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/06/2024 23:22:42 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/06/2024 23:22:42 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.7798, 'grad_norm': 0.8945476412773132, 'learning_rate': 2.5e-05, 'epoch': 0.28}
{'loss': 0.3372, 'grad_norm': 0.6798083782196045, 'learning_rate': 4.9995299261212536e-05, 'epoch': 0.55}
{'loss': 0.2861, 'grad_norm': 0.7695131897926331, 'learning_rate': 4.94333464562659e-05, 'epoch': 0.83}
{'loss': 0.2611, 'grad_norm': 0.6266862154006958, 'learning_rate': 4.7955402672006854e-05, 'epoch': 1.1}
{'loss': 0.2029, 'grad_norm': 0.7799506783485413, 'learning_rate': 4.561687510272767e-05, 'epoch': 1.38}
{'loss': 0.1999, 'grad_norm': 0.8827537298202515, 'learning_rate': 4.2505433694179216e-05, 'epoch': 1.66}
{'loss': 0.2076, 'grad_norm': 1.056757926940918, 'learning_rate': 3.873772445177015e-05, 'epoch': 1.93}
{'loss': 0.151, 'grad_norm': 0.7041838765144348, 'learning_rate': 3.445499645429107e-05, 'epoch': 2.21}
{'loss': 0.0988, 'grad_norm': 1.0578874349594116, 'learning_rate': 2.9817806513702244e-05, 'epoch': 2.48}
{'loss': 0.1324, 'grad_norm': 0.6463721394538879, 'learning_rate': 2.5e-05, 'epoch': 2.76}
{'loss': 0.1214, 'grad_norm': 0.8309370279312134, 'learning_rate': 2.0182193486297755e-05, 'epoch': 3.03}
{'loss': 0.0694, 'grad_norm': 0.81508868932724, 'learning_rate': 1.554500354570894e-05, 'epoch': 3.31}
{'loss': 0.0555, 'grad_norm': 0.5876707434654236, 'learning_rate': 1.126227554822985e-05, 'epoch': 3.59}
{'loss': 0.0437, 'grad_norm': 0.7282485961914062, 'learning_rate': 7.494566305820788e-06, 'epoch': 3.86}
{'loss': 0.0374, 'grad_norm': 0.8228853344917297, 'learning_rate': 4.383124897272331e-06, 'epoch': 4.14}
{'loss': 0.0179, 'grad_norm': 0.8373499512672424, 'learning_rate': 2.044597327993153e-06, 'epoch': 4.41}
{'loss': 0.0246, 'grad_norm': 1.0618510246276855, 'learning_rate': 5.666535437341108e-07, 'epoch': 4.69}
{'loss': 0.0192, 'grad_norm': 0.3889780640602112, 'learning_rate': 4.700738787466463e-09, 'epoch': 4.97}
{'train_runtime': 721.5559, 'train_samples_per_second': 4.019, 'train_steps_per_second': 0.249, 'train_loss': 0.16922691182957755, 'epoch': 4.97}
***** train metrics *****
  epoch                    =     4.9655
  total_flos               = 44833838GF
  train_loss               =     0.1692
  train_runtime            = 0:12:01.55
  train_samples_per_second =      4.019
  train_steps_per_second   =      0.249
09/06/2024 23:35:01 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/06/2024 23:35:01 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/06/2024 23:35:01 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/06/2024 23:35:01 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/06/2024 23:35:04 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/06/2024 23:35:04 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/finetuned_models/CDCP_ari_llama-3-8b-Instruct-bnb-4bit
09/06/2024 23:35:04 - INFO - llamafactory.model.loader - all params: 8,051,232,768
              precision    recall  f1-score   support

       N-Rel      0.980     0.985     0.983     10004
         Rel      0.460     0.395     0.425       324

    accuracy                          0.966     10328
   macro avg      0.720     0.690     0.704     10328
weighted avg      0.964     0.966     0.965     10328

Successfully ran CDCP_finetune.py with arguments: unsloth/llama-3-8b-Instruct-bnb-4bit ari 
 
  *************** 

Running CDCP_finetune.py with arguments: unsloth/llama-3-8b-Instruct-bnb-4bit arc
09/06/2024 23:39:35 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:26984
09/06/2024 23:39:45 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/06/2024 23:39:45 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/06/2024 23:39:45 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/06/2024 23:39:45 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/06/2024 23:39:45 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/06/2024 23:39:45 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/06/2024 23:39:45 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/06/2024 23:39:45 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_arc_train.json...
09/06/2024 23:39:45 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/06/2024 23:39:46 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_arc_train.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 1472, 527, 1101, 2728, 264, 1160, 315, 13840, 315, 5552, 5811, 6956, 304, 279, 1376, 25, 18305, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 5850, 320, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 5850, 61453, 320, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 595, 948, 4718, 3465, 374, 311, 49229, 1855, 6857, 315, 5552, 5811, 6956, 304, 279, 1160, 439, 3060, 330, 20489, 1, 477, 330, 68, 28580, 3343, 1472, 2011, 471, 264, 1160, 315, 5811, 12976, 4595, 11, 26549, 315, 3160, 220, 17, 11, 304, 2768, 4823, 3645, 25, 5324, 23013, 9962, 794, 4482, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 8, 93546, 1405, 1855, 2449, 330, 23013, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 20489, 1, 477, 330, 68, 28580, 3343, 4815, 14711, 5810, 374, 279, 1495, 25, 366, 1741, 16, 29, 1423, 323, 2254, 5590, 5718, 7170, 1304, 1670, 59358, 1790, 810, 4461, 4005, 1741, 16, 1822, 1741, 17, 29, 1789, 3187, 11, 994, 264, 1732, 889, 19755, 69944, 264, 11897, 374, 3309, 311, 2586, 311, 5590, 389, 264, 990, 1938, 11, 814, 1253, 387, 9770, 311, 5268, 1990, 264, 1670, 19971, 323, 872, 2683, 4005, 1741, 17, 1822, 1741, 18, 29, 358, 33147, 279, 356, 11960, 33, 311, 1505, 12659, 430, 21736, 38952, 45348, 520, 84783, 3115, 28743, 11, 81374, 11, 323, 43658, 11, 477, 40240, 449, 220, 11739, 17, 72, 4005, 1741, 18, 397, 14711, 5810, 374, 279, 1160, 315, 13840, 315, 5552, 5811, 6956, 304, 420, 14646, 25, 4416, 15, 11, 220, 16, 1145, 510, 17, 11, 220, 15, 5163, 128009, 128006, 78191, 128007, 271, 5018, 23013, 9962, 794, 4482, 20489, 498, 330, 20489, 93546, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. You are also given a list of pairs of related argument components in the form: [(target AC (int), source AC (int)), (target AC (int), source AC (int)),..., (target AC (int), source AC (int))]. Your task is to classify each pair of related argument components in the list as either "reason" or "evidence". You must return a list of argument relation types, strictly of length 2, in following JSON format: {"relation_types": ["component_type (str)", "component_type (str)"]} where each element "relation_type (str)" is replaced by either "reason" or "evidence". 

### Here is the text: <AC1>State and local court rules sometimes make default judgments much more likely.</AC1><AC2> For example, when a person who allegedly owes a debt is told to come to court on a work day, they may be forced to choose between a default judgment and their job.</AC2><AC3> I urge the CFPB to find practices that involve scheduling hearings at inconvenient times unfair, deceptive, and abusive, or inconsistent with 1692i.</AC3>
### Here is the list of pairs of related argument components in this paragraph: [[0, 1], [2, 0]]<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"relation_types": ["reason", "reason"]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 23013, 9962, 794, 4482, 20489, 498, 330, 20489, 93546, 128009]
labels:
{"relation_types": ["reason", "reason"]}<|eot_id|>
09/06/2024 23:39:52 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/06/2024 23:39:52 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/06/2024 23:39:52 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/06/2024 23:39:52 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/06/2024 23:39:58 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/06/2024 23:39:58 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/06/2024 23:39:58 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/06/2024 23:39:58 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/06/2024 23:39:58 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,up_proj,gate_proj,k_proj,o_proj,down_proj,v_proj
09/06/2024 23:39:58 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/06/2024 23:39:58 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/06/2024 23:39:58 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/06/2024 23:39:58 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/06/2024 23:39:58 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,q_proj,k_proj,v_proj,gate_proj,up_proj,down_proj
09/06/2024 23:39:58 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/06/2024 23:39:58 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/06/2024 23:39:58 - WARNING - llamafactory.train.callbacks - Previous trainer log in this folder will be deleted.
09/06/2024 23:39:59 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/06/2024 23:39:59 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.3746, 'grad_norm': 0.18523423373699188, 'learning_rate': 2.777777777777778e-05, 'epoch': 0.28}
{'loss': 0.0161, 'grad_norm': 0.02807045914232731, 'learning_rate': 4.998119881260576e-05, 'epoch': 0.55}
{'loss': 0.0195, 'grad_norm': 0.0976901426911354, 'learning_rate': 4.9326121764495596e-05, 'epoch': 0.83}
{'loss': 0.0239, 'grad_norm': 0.3670220375061035, 'learning_rate': 4.775907352415367e-05, 'epoch': 1.1}
{'loss': 0.0115, 'grad_norm': 0.3632515072822571, 'learning_rate': 4.533880175657419e-05, 'epoch': 1.38}
{'loss': 0.0228, 'grad_norm': 0.11105577647686005, 'learning_rate': 4.215604094671835e-05, 'epoch': 1.66}
{'loss': 0.0207, 'grad_norm': 0.43186599016189575, 'learning_rate': 3.8330110820042285e-05, 'epoch': 1.93}
{'loss': 0.0135, 'grad_norm': 0.747195303440094, 'learning_rate': 3.400444312011776e-05, 'epoch': 2.21}
{'loss': 0.0043, 'grad_norm': 0.11757449060678482, 'learning_rate': 2.9341204441673266e-05, 'epoch': 2.48}
{'loss': 0.0104, 'grad_norm': 0.19346074759960175, 'learning_rate': 2.4515216705704395e-05, 'epoch': 2.76}
{'loss': 0.0077, 'grad_norm': 0.2014424204826355, 'learning_rate': 1.970740319426474e-05, 'epoch': 3.03}
{'loss': 0.0032, 'grad_norm': 0.027441395446658134, 'learning_rate': 1.509800584902108e-05, 'epoch': 3.31}
{'loss': 0.0037, 'grad_norm': 0.06620296835899353, 'learning_rate': 1.085982811283654e-05, 'epoch': 3.59}
{'loss': 0.005, 'grad_norm': 0.0065412363037467, 'learning_rate': 7.1517566360525284e-06, 'epoch': 3.86}
{'loss': 0.0062, 'grad_norm': 0.0646057054400444, 'learning_rate': 4.112804714676594e-06, 'epoch': 4.14}
{'loss': 0.0006, 'grad_norm': 0.018032893538475037, 'learning_rate': 1.8569007682777417e-06, 'epoch': 4.41}
{'loss': 0.0009, 'grad_norm': 0.01424803864210844, 'learning_rate': 4.6861723431538276e-07, 'epoch': 4.69}
{'loss': 0.0023, 'grad_norm': 0.1518784612417221, 'learning_rate': 0.0, 'epoch': 4.97}
{'train_runtime': 809.6185, 'train_samples_per_second': 3.582, 'train_steps_per_second': 0.222, 'train_loss': 0.03039397141740968, 'epoch': 4.97}
***** train metrics *****
  epoch                    =     4.9655
  total_flos               = 56202608GF
  train_loss               =     0.0304
  train_runtime            = 0:13:29.61
  train_samples_per_second =      3.582
  train_steps_per_second   =      0.222
09/06/2024 23:53:37 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/06/2024 23:53:37 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/06/2024 23:53:37 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/06/2024 23:53:37 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/06/2024 23:53:40 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/06/2024 23:53:41 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/finetuned_models/CDCP_arc_llama-3-8b-Instruct-bnb-4bit
09/06/2024 23:53:41 - INFO - llamafactory.model.loader - all params: 8,051,232,768
              precision    recall  f1-score   support

    evidence      1.000     0.308     0.471        26
      reason      0.943     1.000     0.971       298

    accuracy                          0.944       324
   macro avg      0.972     0.654     0.721       324
weighted avg      0.948     0.944     0.931       324

Successfully ran CDCP_finetune.py with arguments: unsloth/llama-3-8b-Instruct-bnb-4bit arc 
 
  *************** 

Running CDCP_finetune.py with arguments: unsloth/llama-3-8b-Instruct acc
09/06/2024 23:57:05 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:25230
09/06/2024 23:57:15 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/06/2024 23:57:15 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/06/2024 23:57:15 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/06/2024 23:57:15 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/06/2024 23:57:15 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/06/2024 23:57:15 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/06/2024 23:57:15 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/06/2024 23:57:25 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/06/2024 23:57:25 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_acc_train.json...
09/06/2024 23:57:26 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_acc_train.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 49229, 1855, 5811, 3777, 304, 279, 1495, 439, 3060, 330, 34210, 498, 330, 35890, 498, 330, 16690, 498, 330, 1985, 65556, 1, 477, 330, 970, 3343, 1472, 2011, 471, 264, 1160, 315, 5811, 3777, 4595, 11, 26549, 315, 3160, 220, 18, 11, 304, 2768, 4823, 3645, 25, 5324, 8739, 9962, 794, 4482, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 8, 93546, 1405, 1855, 2449, 330, 8739, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 34210, 498, 330, 35890, 498, 330, 16690, 498, 330, 1985, 65556, 1, 477, 330, 970, 3343, 4815, 14711, 5810, 374, 279, 1495, 25, 366, 1741, 16, 29, 1423, 323, 2254, 5590, 5718, 7170, 1304, 1670, 59358, 1790, 810, 4461, 4005, 1741, 16, 1822, 1741, 17, 29, 1789, 3187, 11, 994, 264, 1732, 889, 19755, 69944, 264, 11897, 374, 3309, 311, 2586, 311, 5590, 389, 264, 990, 1938, 11, 814, 1253, 387, 9770, 311, 5268, 1990, 264, 1670, 19971, 323, 872, 2683, 4005, 1741, 17, 1822, 1741, 18, 29, 358, 33147, 279, 356, 11960, 33, 311, 1505, 12659, 430, 21736, 38952, 45348, 520, 84783, 3115, 28743, 11, 81374, 11, 323, 43658, 11, 477, 40240, 449, 220, 11739, 17, 72, 4005, 1741, 18, 29, 128009, 128006, 78191, 128007, 271, 5018, 8739, 9962, 794, 4482, 970, 498, 330, 970, 498, 330, 35890, 93546, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to classify each argument component in the text as either "fact", "policy", "reference", "testimony" or "value". You must return a list of argument component types, strictly of length 3, in following JSON format: {"component_types": ["component_type (str)", "component_type (str)", "component_type (str)"]} where each element "component_type (str)" is replaced by either "fact", "policy", "reference", "testimony" or "value". 

### Here is the text: <AC1>State and local court rules sometimes make default judgments much more likely.</AC1><AC2> For example, when a person who allegedly owes a debt is told to come to court on a work day, they may be forced to choose between a default judgment and their job.</AC2><AC3> I urge the CFPB to find practices that involve scheduling hearings at inconvenient times unfair, deceptive, and abusive, or inconsistent with 1692i.</AC3><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"component_types": ["value", "value", "policy"]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 8739, 9962, 794, 4482, 970, 498, 330, 970, 498, 330, 35890, 93546, 128009]
labels:
{"component_types": ["value", "value", "policy"]}<|eot_id|>
09/06/2024 23:57:27 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/06/2024 23:57:27 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/06/2024 23:57:58 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/06/2024 23:57:58 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/06/2024 23:57:58 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/06/2024 23:57:58 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/06/2024 23:57:58 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,down_proj,v_proj,o_proj,q_proj,up_proj,gate_proj
09/06/2024 23:57:58 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/06/2024 23:57:58 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/06/2024 23:57:58 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/06/2024 23:57:58 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/06/2024 23:57:58 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/06/2024 23:57:58 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,o_proj,q_proj,v_proj,up_proj,gate_proj,down_proj
09/06/2024 23:57:59 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/06/2024 23:57:59 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/06/2024 23:57:59 - WARNING - llamafactory.train.callbacks - Previous trainer log in this folder will be deleted.
09/06/2024 23:57:59 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.4343, 'grad_norm': 0.6477220058441162, 'learning_rate': 2.777777777777778e-05, 'epoch': 0.28}
{'loss': 0.1596, 'grad_norm': 1.538232684135437, 'learning_rate': 4.998119881260576e-05, 'epoch': 0.55}
{'loss': 0.1462, 'grad_norm': 0.6628393530845642, 'learning_rate': 4.9326121764495596e-05, 'epoch': 0.83}
{'loss': 0.101, 'grad_norm': 0.5195136666297913, 'learning_rate': 4.775907352415367e-05, 'epoch': 1.1}
{'loss': 0.0886, 'grad_norm': 0.8464680314064026, 'learning_rate': 4.533880175657419e-05, 'epoch': 1.38}
{'loss': 0.0775, 'grad_norm': 0.4795377254486084, 'learning_rate': 4.215604094671835e-05, 'epoch': 1.66}
{'loss': 0.0953, 'grad_norm': 0.5207812190055847, 'learning_rate': 3.8330110820042285e-05, 'epoch': 1.93}
{'loss': 0.0561, 'grad_norm': 0.38894766569137573, 'learning_rate': 3.400444312011776e-05, 'epoch': 2.21}
{'loss': 0.0315, 'grad_norm': 0.42391467094421387, 'learning_rate': 2.9341204441673266e-05, 'epoch': 2.48}
{'loss': 0.0421, 'grad_norm': 0.594429612159729, 'learning_rate': 2.4515216705704395e-05, 'epoch': 2.76}
{'loss': 0.0333, 'grad_norm': 0.49721837043762207, 'learning_rate': 1.970740319426474e-05, 'epoch': 3.03}
{'loss': 0.0141, 'grad_norm': 0.11210580915212631, 'learning_rate': 1.509800584902108e-05, 'epoch': 3.31}
{'loss': 0.0129, 'grad_norm': 0.23505020141601562, 'learning_rate': 1.085982811283654e-05, 'epoch': 3.59}
{'loss': 0.0098, 'grad_norm': 0.2223193496465683, 'learning_rate': 7.1517566360525284e-06, 'epoch': 3.86}
{'loss': 0.0094, 'grad_norm': 0.46936461329460144, 'learning_rate': 4.112804714676594e-06, 'epoch': 4.14}
{'loss': 0.0065, 'grad_norm': 0.21486860513687134, 'learning_rate': 1.8569007682777417e-06, 'epoch': 4.41}
{'loss': 0.0038, 'grad_norm': 0.10339617729187012, 'learning_rate': 4.6861723431538276e-07, 'epoch': 4.69}
{'loss': 0.0027, 'grad_norm': 0.11669003218412399, 'learning_rate': 0.0, 'epoch': 4.97}
{'train_runtime': 713.9395, 'train_samples_per_second': 4.062, 'train_steps_per_second': 0.252, 'train_loss': 0.07359159281477332, 'epoch': 4.97}
***** train metrics *****
  epoch                    =     4.9655
  total_flos               = 55401222GF
  train_loss               =     0.0736
  train_runtime            = 0:11:53.93
  train_samples_per_second =      4.062
  train_steps_per_second   =      0.252
09/07/2024 00:10:03 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/07/2024 00:10:03 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/07/2024 00:10:03 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/07/2024 00:10:09 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/07/2024 00:10:09 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/finetuned_models/CDCP_acc_llama-3-8b-Instruct
09/07/2024 00:10:09 - INFO - llamafactory.model.loader - all params: 8,051,232,768
              precision    recall  f1-score   support

        fact      0.642     0.720     0.679       132
      policy      0.856     0.856     0.856       153
   reference      1.000     1.000     1.000         1
   testimony      0.936     0.893     0.914       244
       value      0.855     0.847     0.851       496

    accuracy                          0.843      1026
   macro avg      0.858     0.863     0.860      1026
weighted avg      0.847     0.843     0.845      1026

Successfully ran CDCP_finetune.py with arguments: unsloth/llama-3-8b-Instruct acc 
 
  *************** 

Running CDCP_finetune.py with arguments: unsloth/llama-3-8b-Instruct ari
09/07/2024 00:16:15 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:29619
09/07/2024 00:16:25 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/07/2024 00:16:25 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/07/2024 00:16:25 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/07/2024 00:16:25 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/07/2024 00:16:25 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/07/2024 00:16:25 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/07/2024 00:16:26 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/07/2024 00:16:26 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_ari_train.json...
09/07/2024 00:16:26 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/07/2024 00:16:27 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_ari_train.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 10765, 5811, 4398, 1990, 5811, 6956, 304, 279, 1495, 13, 1472, 2011, 471, 264, 1160, 315, 13840, 304, 279, 2768, 4823, 3645, 25, 5324, 1638, 9202, 95321, 794, 4416, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 26090, 61453, 510, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 8, 5163, 633, 14711, 5810, 374, 279, 1495, 25, 366, 1741, 15, 29, 1423, 323, 2254, 5590, 5718, 7170, 1304, 1670, 59358, 1790, 810, 4461, 4005, 1741, 15, 1822, 1741, 16, 29, 1789, 3187, 11, 994, 264, 1732, 889, 19755, 69944, 264, 11897, 374, 3309, 311, 2586, 311, 5590, 389, 264, 990, 1938, 11, 814, 1253, 387, 9770, 311, 5268, 1990, 264, 1670, 19971, 323, 872, 2683, 4005, 1741, 16, 1822, 1741, 17, 29, 358, 33147, 279, 356, 11960, 33, 311, 1505, 12659, 430, 21736, 38952, 45348, 520, 84783, 3115, 28743, 11, 81374, 11, 323, 43658, 11, 477, 40240, 449, 220, 11739, 17, 72, 4005, 1741, 17, 29, 128009, 128006, 78191, 128007, 271, 5018, 1638, 9202, 95321, 794, 4416, 15, 11, 220, 16, 1145, 510, 17, 11, 220, 15, 5163, 92, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to identify argument relations between argument components in the text. You must return a list of pairs in the following JSON format: {"list_argument_relations": [[source AC (int), target AC (int)],..., [source AC (int), target AC (int)]]}

### Here is the text: <AC0>State and local court rules sometimes make default judgments much more likely.</AC0><AC1> For example, when a person who allegedly owes a debt is told to come to court on a work day, they may be forced to choose between a default judgment and their job.</AC1><AC2> I urge the CFPB to find practices that involve scheduling hearings at inconvenient times unfair, deceptive, and abusive, or inconsistent with 1692i.</AC2><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"list_argument_relations": [[0, 1], [2, 0]]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 1638, 9202, 95321, 794, 4416, 15, 11, 220, 16, 1145, 510, 17, 11, 220, 15, 5163, 92, 128009]
labels:
{"list_argument_relations": [[0, 1], [2, 0]]}<|eot_id|>
09/07/2024 00:16:27 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/07/2024 00:16:28 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/07/2024 00:16:48 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/07/2024 00:16:48 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/07/2024 00:16:48 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/07/2024 00:16:48 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/07/2024 00:16:48 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,v_proj,q_proj,up_proj,o_proj,k_proj,gate_proj
09/07/2024 00:16:48 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/07/2024 00:16:49 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/07/2024 00:16:49 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/07/2024 00:16:49 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/07/2024 00:16:49 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/07/2024 00:16:49 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,q_proj,down_proj,k_proj,o_proj,gate_proj,up_proj
09/07/2024 00:16:49 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/07/2024 00:16:49 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/07/2024 00:16:49 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.7791, 'grad_norm': 0.8967803120613098, 'learning_rate': 2.5e-05, 'epoch': 0.28}
{'loss': 0.3371, 'grad_norm': 0.6453936100006104, 'learning_rate': 4.9995299261212536e-05, 'epoch': 0.55}
{'loss': 0.2856, 'grad_norm': 0.7658582329750061, 'learning_rate': 4.94333464562659e-05, 'epoch': 0.83}
{'loss': 0.2603, 'grad_norm': 0.6466092467308044, 'learning_rate': 4.7955402672006854e-05, 'epoch': 1.1}
{'loss': 0.2034, 'grad_norm': 0.814522385597229, 'learning_rate': 4.561687510272767e-05, 'epoch': 1.38}
{'loss': 0.1977, 'grad_norm': 0.8420964479446411, 'learning_rate': 4.2505433694179216e-05, 'epoch': 1.66}
{'loss': 0.2068, 'grad_norm': 0.9415013194084167, 'learning_rate': 3.873772445177015e-05, 'epoch': 1.93}
{'loss': 0.1521, 'grad_norm': 0.7567236423492432, 'learning_rate': 3.445499645429107e-05, 'epoch': 2.21}
{'loss': 0.1035, 'grad_norm': 0.9673982858657837, 'learning_rate': 2.9817806513702244e-05, 'epoch': 2.48}
{'loss': 0.1313, 'grad_norm': 0.8485075831413269, 'learning_rate': 2.5e-05, 'epoch': 2.76}
{'loss': 0.1226, 'grad_norm': 0.9863039255142212, 'learning_rate': 2.0182193486297755e-05, 'epoch': 3.03}
{'loss': 0.0698, 'grad_norm': 0.681278645992279, 'learning_rate': 1.554500354570894e-05, 'epoch': 3.31}
{'loss': 0.0567, 'grad_norm': 0.5829814076423645, 'learning_rate': 1.126227554822985e-05, 'epoch': 3.59}
{'loss': 0.0439, 'grad_norm': 0.9839174747467041, 'learning_rate': 7.494566305820788e-06, 'epoch': 3.86}
{'loss': 0.0382, 'grad_norm': 0.6213119029998779, 'learning_rate': 4.383124897272331e-06, 'epoch': 4.14}
{'loss': 0.0168, 'grad_norm': 0.5091423392295837, 'learning_rate': 2.044597327993153e-06, 'epoch': 4.41}
{'loss': 0.0216, 'grad_norm': 0.42447495460510254, 'learning_rate': 5.666535437341108e-07, 'epoch': 4.69}
{'loss': 0.0194, 'grad_norm': 0.44596636295318604, 'learning_rate': 4.700738787466463e-09, 'epoch': 4.97}
{'train_runtime': 637.0319, 'train_samples_per_second': 4.552, 'train_steps_per_second': 0.283, 'train_loss': 0.16921568802661366, 'epoch': 4.97}
***** train metrics *****
  epoch                    =     4.9655
  total_flos               = 44833838GF
  train_loss               =     0.1692
  train_runtime            = 0:10:37.03
  train_samples_per_second =      4.552
  train_steps_per_second   =      0.283
09/07/2024 00:27:33 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/07/2024 00:27:33 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/07/2024 00:27:33 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/07/2024 00:27:39 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/07/2024 00:27:39 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/finetuned_models/CDCP_ari_llama-3-8b-Instruct
09/07/2024 00:27:39 - INFO - llamafactory.model.loader - all params: 8,051,232,768
              precision    recall  f1-score   support

       N-Rel      0.981     0.981     0.981     10004
         Rel      0.412     0.404     0.408       324

    accuracy                          0.963     10328
   macro avg      0.696     0.693     0.695     10328
weighted avg      0.963     0.963     0.963     10328

Successfully ran CDCP_finetune.py with arguments: unsloth/llama-3-8b-Instruct ari 
 
  *************** 

Running CDCP_finetune.py with arguments: unsloth/llama-3-8b-Instruct arc
09/07/2024 00:32:25 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:22699
09/07/2024 00:32:35 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/07/2024 00:32:35 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/07/2024 00:32:35 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/07/2024 00:32:35 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/07/2024 00:32:35 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/07/2024 00:32:35 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/07/2024 00:32:35 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/07/2024 00:32:35 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_arc_train.json...
09/07/2024 00:32:35 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/07/2024 00:32:36 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_arc_train.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 1472, 527, 1101, 2728, 264, 1160, 315, 13840, 315, 5552, 5811, 6956, 304, 279, 1376, 25, 18305, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 5850, 320, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 5850, 61453, 320, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 595, 948, 4718, 3465, 374, 311, 49229, 1855, 6857, 315, 5552, 5811, 6956, 304, 279, 1160, 439, 3060, 330, 20489, 1, 477, 330, 68, 28580, 3343, 1472, 2011, 471, 264, 1160, 315, 5811, 12976, 4595, 11, 26549, 315, 3160, 220, 17, 11, 304, 2768, 4823, 3645, 25, 5324, 23013, 9962, 794, 4482, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 8, 93546, 1405, 1855, 2449, 330, 23013, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 20489, 1, 477, 330, 68, 28580, 3343, 4815, 14711, 5810, 374, 279, 1495, 25, 366, 1741, 16, 29, 1423, 323, 2254, 5590, 5718, 7170, 1304, 1670, 59358, 1790, 810, 4461, 4005, 1741, 16, 1822, 1741, 17, 29, 1789, 3187, 11, 994, 264, 1732, 889, 19755, 69944, 264, 11897, 374, 3309, 311, 2586, 311, 5590, 389, 264, 990, 1938, 11, 814, 1253, 387, 9770, 311, 5268, 1990, 264, 1670, 19971, 323, 872, 2683, 4005, 1741, 17, 1822, 1741, 18, 29, 358, 33147, 279, 356, 11960, 33, 311, 1505, 12659, 430, 21736, 38952, 45348, 520, 84783, 3115, 28743, 11, 81374, 11, 323, 43658, 11, 477, 40240, 449, 220, 11739, 17, 72, 4005, 1741, 18, 397, 14711, 5810, 374, 279, 1160, 315, 13840, 315, 5552, 5811, 6956, 304, 420, 14646, 25, 4416, 15, 11, 220, 16, 1145, 510, 17, 11, 220, 15, 5163, 128009, 128006, 78191, 128007, 271, 5018, 23013, 9962, 794, 4482, 20489, 498, 330, 20489, 93546, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. You are also given a list of pairs of related argument components in the form: [(target AC (int), source AC (int)), (target AC (int), source AC (int)),..., (target AC (int), source AC (int))]. Your task is to classify each pair of related argument components in the list as either "reason" or "evidence". You must return a list of argument relation types, strictly of length 2, in following JSON format: {"relation_types": ["component_type (str)", "component_type (str)"]} where each element "relation_type (str)" is replaced by either "reason" or "evidence". 

### Here is the text: <AC1>State and local court rules sometimes make default judgments much more likely.</AC1><AC2> For example, when a person who allegedly owes a debt is told to come to court on a work day, they may be forced to choose between a default judgment and their job.</AC2><AC3> I urge the CFPB to find practices that involve scheduling hearings at inconvenient times unfair, deceptive, and abusive, or inconsistent with 1692i.</AC3>
### Here is the list of pairs of related argument components in this paragraph: [[0, 1], [2, 0]]<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"relation_types": ["reason", "reason"]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 23013, 9962, 794, 4482, 20489, 498, 330, 20489, 93546, 128009]
labels:
{"relation_types": ["reason", "reason"]}<|eot_id|>
09/07/2024 00:32:37 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/07/2024 00:32:37 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/07/2024 00:32:58 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/07/2024 00:32:58 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/07/2024 00:32:58 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/07/2024 00:32:58 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/07/2024 00:32:58 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,k_proj,gate_proj,v_proj,down_proj,o_proj,up_proj
09/07/2024 00:32:58 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/07/2024 00:32:58 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/07/2024 00:32:58 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/07/2024 00:32:58 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/07/2024 00:32:58 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/07/2024 00:32:58 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,gate_proj,v_proj,o_proj,down_proj,up_proj,q_proj
09/07/2024 00:32:59 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/07/2024 00:32:59 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/07/2024 00:32:59 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.4584, 'grad_norm': 0.16350309550762177, 'learning_rate': 2.5e-05, 'epoch': 0.28}
{'loss': 0.0218, 'grad_norm': 0.05522020906209946, 'learning_rate': 4.9995299261212536e-05, 'epoch': 0.55}
{'loss': 0.0209, 'grad_norm': 0.21993887424468994, 'learning_rate': 4.94333464562659e-05, 'epoch': 0.83}
{'loss': 0.0205, 'grad_norm': 0.23761984705924988, 'learning_rate': 4.7955402672006854e-05, 'epoch': 1.1}
{'loss': 0.0095, 'grad_norm': 0.8365825414657593, 'learning_rate': 4.561687510272767e-05, 'epoch': 1.38}
{'loss': 0.0254, 'grad_norm': 0.18319246172904968, 'learning_rate': 4.2505433694179216e-05, 'epoch': 1.66}
{'loss': 0.0201, 'grad_norm': 0.09704572707414627, 'learning_rate': 3.873772445177015e-05, 'epoch': 1.93}
{'loss': 0.0124, 'grad_norm': 0.5191393494606018, 'learning_rate': 3.445499645429107e-05, 'epoch': 2.21}
{'loss': 0.012, 'grad_norm': 2.0467586517333984, 'learning_rate': 2.9817806513702244e-05, 'epoch': 2.48}
{'loss': 0.0122, 'grad_norm': 0.4083324670791626, 'learning_rate': 2.5e-05, 'epoch': 2.76}
{'loss': 0.0092, 'grad_norm': 0.2118920534849167, 'learning_rate': 2.0182193486297755e-05, 'epoch': 3.03}
{'loss': 0.0048, 'grad_norm': 0.053992193192243576, 'learning_rate': 1.554500354570894e-05, 'epoch': 3.31}
{'loss': 0.0046, 'grad_norm': 1.2120022773742676, 'learning_rate': 1.126227554822985e-05, 'epoch': 3.59}
{'loss': 0.0037, 'grad_norm': 0.0032913191244006157, 'learning_rate': 7.494566305820788e-06, 'epoch': 3.86}
{'loss': 0.0037, 'grad_norm': 0.5346170663833618, 'learning_rate': 4.383124897272331e-06, 'epoch': 4.14}
{'loss': 0.0014, 'grad_norm': 0.005685332231223583, 'learning_rate': 2.044597327993153e-06, 'epoch': 4.41}
{'loss': 0.0003, 'grad_norm': 0.004649910144507885, 'learning_rate': 5.666535437341108e-07, 'epoch': 4.69}
{'loss': 0.0042, 'grad_norm': 0.2560233175754547, 'learning_rate': 4.700738787466463e-09, 'epoch': 4.97}
{'train_runtime': 698.9269, 'train_samples_per_second': 4.149, 'train_steps_per_second': 0.258, 'train_loss': 0.03584074329248526, 'epoch': 4.97}
***** train metrics *****
  epoch                    =     4.9655
  total_flos               = 56202608GF
  train_loss               =     0.0358
  train_runtime            = 0:11:38.92
  train_samples_per_second =      4.149
  train_steps_per_second   =      0.258
09/07/2024 00:44:47 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/07/2024 00:44:47 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/07/2024 00:44:47 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/07/2024 00:44:53 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/07/2024 00:44:54 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/finetuned_models/CDCP_arc_llama-3-8b-Instruct
09/07/2024 00:44:54 - INFO - llamafactory.model.loader - all params: 8,051,232,768
              precision    recall  f1-score   support

    evidence      0.857     0.231     0.364        26
      reason      0.937     0.997     0.966       298

    accuracy                          0.935       324
   macro avg      0.897     0.614     0.665       324
weighted avg      0.931     0.935     0.918       324

Successfully ran CDCP_finetune.py with arguments: unsloth/llama-3-8b-Instruct arc 
 
  *************** 

Running CDCP_finetune.py with arguments: unsloth/llama-3-70b-Instruct-bnb-4bit acc
09/07/2024 00:48:01 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:27423
09/07/2024 00:48:11 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/07/2024 00:48:11 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/07/2024 00:48:11 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/07/2024 00:48:11 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/07/2024 00:48:11 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/07/2024 00:48:11 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/07/2024 00:48:12 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/07/2024 00:48:12 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/07/2024 00:48:12 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_acc_train.json...
09/07/2024 00:48:13 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_acc_train.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 49229, 1855, 5811, 3777, 304, 279, 1495, 439, 3060, 330, 34210, 498, 330, 35890, 498, 330, 16690, 498, 330, 1985, 65556, 1, 477, 330, 970, 3343, 1472, 2011, 471, 264, 1160, 315, 5811, 3777, 4595, 11, 26549, 315, 3160, 220, 18, 11, 304, 2768, 4823, 3645, 25, 5324, 8739, 9962, 794, 4482, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 8, 93546, 1405, 1855, 2449, 330, 8739, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 34210, 498, 330, 35890, 498, 330, 16690, 498, 330, 1985, 65556, 1, 477, 330, 970, 3343, 4815, 14711, 5810, 374, 279, 1495, 25, 366, 1741, 16, 29, 1423, 323, 2254, 5590, 5718, 7170, 1304, 1670, 59358, 1790, 810, 4461, 4005, 1741, 16, 1822, 1741, 17, 29, 1789, 3187, 11, 994, 264, 1732, 889, 19755, 69944, 264, 11897, 374, 3309, 311, 2586, 311, 5590, 389, 264, 990, 1938, 11, 814, 1253, 387, 9770, 311, 5268, 1990, 264, 1670, 19971, 323, 872, 2683, 4005, 1741, 17, 1822, 1741, 18, 29, 358, 33147, 279, 356, 11960, 33, 311, 1505, 12659, 430, 21736, 38952, 45348, 520, 84783, 3115, 28743, 11, 81374, 11, 323, 43658, 11, 477, 40240, 449, 220, 11739, 17, 72, 4005, 1741, 18, 29, 128009, 128006, 78191, 128007, 271, 5018, 8739, 9962, 794, 4482, 970, 498, 330, 970, 498, 330, 35890, 93546, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to classify each argument component in the text as either "fact", "policy", "reference", "testimony" or "value". You must return a list of argument component types, strictly of length 3, in following JSON format: {"component_types": ["component_type (str)", "component_type (str)", "component_type (str)"]} where each element "component_type (str)" is replaced by either "fact", "policy", "reference", "testimony" or "value". 

### Here is the text: <AC1>State and local court rules sometimes make default judgments much more likely.</AC1><AC2> For example, when a person who allegedly owes a debt is told to come to court on a work day, they may be forced to choose between a default judgment and their job.</AC2><AC3> I urge the CFPB to find practices that involve scheduling hearings at inconvenient times unfair, deceptive, and abusive, or inconsistent with 1692i.</AC3><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"component_types": ["value", "value", "policy"]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 8739, 9962, 794, 4482, 970, 498, 330, 970, 498, 330, 35890, 93546, 128009]
labels:
{"component_types": ["value", "value", "policy"]}<|eot_id|>
09/07/2024 00:48:14 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/07/2024 00:48:14 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/07/2024 00:48:14 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/07/2024 00:48:14 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/07/2024 00:54:07 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/07/2024 00:54:07 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/07/2024 00:54:07 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/07/2024 00:54:07 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/07/2024 00:54:07 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,k_proj,down_proj,q_proj,v_proj,o_proj,up_proj
09/07/2024 00:54:07 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/07/2024 00:54:07 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/07/2024 00:54:07 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/07/2024 00:54:07 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/07/2024 00:54:07 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,o_proj,k_proj,v_proj,down_proj,gate_proj,q_proj
09/07/2024 00:54:10 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
09/07/2024 00:54:10 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
09/07/2024 00:54:10 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/07/2024 00:54:10 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.6768, 'grad_norm': 0.7845447659492493, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.28}
{'loss': 0.1992, 'grad_norm': 0.410384863615036, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.55}
{'loss': 0.1549, 'grad_norm': 0.5965489745140076, 'learning_rate': 4.969974623692023e-05, 'epoch': 0.83}
{'loss': 0.1091, 'grad_norm': 0.5966088771820068, 'learning_rate': 4.849231551964771e-05, 'epoch': 1.1}
{'loss': 0.1011, 'grad_norm': 0.402173787355423, 'learning_rate': 4.640417248825667e-05, 'epoch': 1.38}
{'loss': 0.0901, 'grad_norm': 0.3124437630176544, 'learning_rate': 4.351360032772512e-05, 'epoch': 1.66}
{'loss': 0.0837, 'grad_norm': 0.26606276631355286, 'learning_rate': 3.9928964792569655e-05, 'epoch': 1.93}
{'loss': 0.0573, 'grad_norm': 0.17295914888381958, 'learning_rate': 3.578465164203134e-05, 'epoch': 2.21}
{'loss': 0.0429, 'grad_norm': 0.3436386287212372, 'learning_rate': 3.1236028601449534e-05, 'epoch': 2.48}
{'loss': 0.0444, 'grad_norm': 0.24037688970565796, 'learning_rate': 2.6453620722761896e-05, 'epoch': 2.76}
{'loss': 0.0435, 'grad_norm': 0.5119331479072571, 'learning_rate': 2.161671750624673e-05, 'epoch': 3.03}
{'loss': 0.0201, 'grad_norm': 0.04500654712319374, 'learning_rate': 1.690665144854198e-05, 'epoch': 3.31}
{'loss': 0.0202, 'grad_norm': 0.22877487540245056, 'learning_rate': 1.2500000000000006e-05, 'epoch': 3.59}
{'loss': 0.0202, 'grad_norm': 0.12413813173770905, 'learning_rate': 8.561965785773413e-06, 'epoch': 3.86}
{'loss': 0.0144, 'grad_norm': 0.25830602645874023, 'learning_rate': 5.240183262031021e-06, 'epoch': 4.14}
{'loss': 0.0116, 'grad_norm': 0.25461021065711975, 'learning_rate': 2.659183991914696e-06, 'epoch': 4.41}
{'loss': 0.0055, 'grad_norm': 0.052333343774080276, 'learning_rate': 9.157280346029918e-07, 'epoch': 4.69}
{'loss': 0.0067, 'grad_norm': 0.094668909907341, 'learning_rate': 7.51764708051994e-08, 'epoch': 4.97}
{'train_runtime': 5930.7125, 'train_samples_per_second': 0.489, 'train_steps_per_second': 0.03, 'train_loss': 0.0945376077045997, 'epoch': 4.97}
***** train metrics *****
  epoch                    =      4.9655
  total_flos               = 512402722GF
  train_loss               =      0.0945
  train_runtime            =  1:38:50.71
  train_samples_per_second =       0.489
  train_steps_per_second   =        0.03
09/07/2024 02:33:14 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/07/2024 02:33:14 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/07/2024 02:33:14 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/07/2024 02:33:14 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/07/2024 02:36:58 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/07/2024 02:37:03 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/finetuned_models/CDCP_acc_llama-3-70b-Instruct-bnb-4bit
09/07/2024 02:37:03 - INFO - llamafactory.model.loader - all params: 70,657,253,376
              precision    recall  f1-score   support

        fact      0.653     0.727     0.688       132
      policy      0.928     0.928     0.928       153
   reference      1.000     1.000     1.000         1
   testimony      0.922     0.877     0.899       244
       value      0.870     0.865     0.868       496

    accuracy                          0.860      1026
   macro avg      0.875     0.879     0.877      1026
weighted avg      0.863     0.860     0.861      1026

Successfully ran CDCP_finetune.py with arguments: unsloth/llama-3-70b-Instruct-bnb-4bit acc 
 
  *************** 

Running CDCP_finetune.py with arguments: unsloth/llama-3-70b-Instruct-bnb-4bit ari
09/07/2024 02:53:06 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:21552
09/07/2024 02:53:16 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/07/2024 02:53:16 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/07/2024 02:53:16 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/07/2024 02:53:16 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/07/2024 02:53:16 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/07/2024 02:53:16 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/07/2024 02:53:17 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/07/2024 02:53:17 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_ari_train.json...
09/07/2024 02:53:17 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/07/2024 02:53:18 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_ari_train.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 10765, 5811, 4398, 1990, 5811, 6956, 304, 279, 1495, 13, 1472, 2011, 471, 264, 1160, 315, 13840, 304, 279, 2768, 4823, 3645, 25, 5324, 1638, 9202, 95321, 794, 4416, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 26090, 61453, 510, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 8, 5163, 633, 14711, 5810, 374, 279, 1495, 25, 366, 1741, 15, 29, 1423, 323, 2254, 5590, 5718, 7170, 1304, 1670, 59358, 1790, 810, 4461, 4005, 1741, 15, 1822, 1741, 16, 29, 1789, 3187, 11, 994, 264, 1732, 889, 19755, 69944, 264, 11897, 374, 3309, 311, 2586, 311, 5590, 389, 264, 990, 1938, 11, 814, 1253, 387, 9770, 311, 5268, 1990, 264, 1670, 19971, 323, 872, 2683, 4005, 1741, 16, 1822, 1741, 17, 29, 358, 33147, 279, 356, 11960, 33, 311, 1505, 12659, 430, 21736, 38952, 45348, 520, 84783, 3115, 28743, 11, 81374, 11, 323, 43658, 11, 477, 40240, 449, 220, 11739, 17, 72, 4005, 1741, 17, 29, 128009, 128006, 78191, 128007, 271, 5018, 1638, 9202, 95321, 794, 4416, 15, 11, 220, 16, 1145, 510, 17, 11, 220, 15, 5163, 92, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to identify argument relations between argument components in the text. You must return a list of pairs in the following JSON format: {"list_argument_relations": [[source AC (int), target AC (int)],..., [source AC (int), target AC (int)]]}

### Here is the text: <AC0>State and local court rules sometimes make default judgments much more likely.</AC0><AC1> For example, when a person who allegedly owes a debt is told to come to court on a work day, they may be forced to choose between a default judgment and their job.</AC1><AC2> I urge the CFPB to find practices that involve scheduling hearings at inconvenient times unfair, deceptive, and abusive, or inconsistent with 1692i.</AC2><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"list_argument_relations": [[0, 1], [2, 0]]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 1638, 9202, 95321, 794, 4416, 15, 11, 220, 16, 1145, 510, 17, 11, 220, 15, 5163, 92, 128009]
labels:
{"list_argument_relations": [[0, 1], [2, 0]]}<|eot_id|>
09/07/2024 02:53:19 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/07/2024 02:53:19 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/07/2024 02:53:19 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/07/2024 02:53:19 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/07/2024 02:59:11 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/07/2024 02:59:11 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/07/2024 02:59:11 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/07/2024 02:59:11 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/07/2024 02:59:11 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,k_proj,up_proj,q_proj,v_proj,down_proj,o_proj
09/07/2024 02:59:12 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/07/2024 02:59:12 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/07/2024 02:59:12 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/07/2024 02:59:12 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/07/2024 02:59:12 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,v_proj,down_proj,up_proj,o_proj,q_proj,k_proj
09/07/2024 02:59:12 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
09/07/2024 02:59:13 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/07/2024 02:59:15 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
09/07/2024 02:59:15 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 1.7132, 'grad_norm': 0.9372544288635254, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.28}
{'loss': 0.369, 'grad_norm': 0.43489930033683777, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.55}
{'loss': 0.2768, 'grad_norm': 0.2788526117801666, 'learning_rate': 4.969974623692023e-05, 'epoch': 0.83}
{'loss': 0.244, 'grad_norm': 0.499546080827713, 'learning_rate': 4.849231551964771e-05, 'epoch': 1.1}
{'loss': 0.1966, 'grad_norm': 0.37520673871040344, 'learning_rate': 4.640417248825667e-05, 'epoch': 1.38}
{'loss': 0.1832, 'grad_norm': 0.45102056860923767, 'learning_rate': 4.351360032772512e-05, 'epoch': 1.66}
{'loss': 0.182, 'grad_norm': 0.4576939642429352, 'learning_rate': 3.9928964792569655e-05, 'epoch': 1.93}
{'loss': 0.1505, 'grad_norm': 0.4661478102207184, 'learning_rate': 3.578465164203134e-05, 'epoch': 2.21}
{'loss': 0.0995, 'grad_norm': 0.522468626499176, 'learning_rate': 3.1236028601449534e-05, 'epoch': 2.48}
{'loss': 0.1346, 'grad_norm': 0.3757391571998596, 'learning_rate': 2.6453620722761896e-05, 'epoch': 2.76}
{'loss': 0.1052, 'grad_norm': 0.40250587463378906, 'learning_rate': 2.161671750624673e-05, 'epoch': 3.03}
{'loss': 0.0641, 'grad_norm': 0.6575137972831726, 'learning_rate': 1.690665144854198e-05, 'epoch': 3.31}
{'loss': 0.0599, 'grad_norm': 0.4635970890522003, 'learning_rate': 1.2500000000000006e-05, 'epoch': 3.59}
{'loss': 0.0466, 'grad_norm': 0.4531158208847046, 'learning_rate': 8.561965785773413e-06, 'epoch': 3.86}
{'loss': 0.0452, 'grad_norm': 0.44167575240135193, 'learning_rate': 5.240183262031021e-06, 'epoch': 4.14}
{'loss': 0.0201, 'grad_norm': 0.1842961609363556, 'learning_rate': 2.659183991914696e-06, 'epoch': 4.41}
{'loss': 0.0251, 'grad_norm': 0.19336602091789246, 'learning_rate': 9.157280346029918e-07, 'epoch': 4.69}
{'loss': 0.0202, 'grad_norm': 0.2597863972187042, 'learning_rate': 7.51764708051994e-08, 'epoch': 4.97}
{'train_runtime': 5206.8757, 'train_samples_per_second': 0.557, 'train_steps_per_second': 0.035, 'train_loss': 0.21864855306016073, 'epoch': 4.97}
***** train metrics *****
  epoch                    =      4.9655
  total_flos               = 414665582GF
  train_loss               =      0.2186
  train_runtime            =  1:26:46.87
  train_samples_per_second =       0.557
  train_steps_per_second   =       0.035
09/07/2024 04:26:14 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/07/2024 04:26:14 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/07/2024 04:26:14 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/07/2024 04:26:14 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/07/2024 04:29:58 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/07/2024 04:30:03 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/finetuned_models/CDCP_ari_llama-3-70b-Instruct-bnb-4bit
09/07/2024 04:30:03 - INFO - llamafactory.model.loader - all params: 70,657,253,376
              precision    recall  f1-score   support

       N-Rel      0.982     0.985     0.984     10004
         Rel      0.500     0.457     0.477       324

    accuracy                          0.969     10328
   macro avg      0.741     0.721     0.731     10328
weighted avg      0.967     0.969     0.968     10328

Successfully ran CDCP_finetune.py with arguments: unsloth/llama-3-70b-Instruct-bnb-4bit ari 
 
  *************** 

Running CDCP_finetune.py with arguments: unsloth/llama-3-70b-Instruct-bnb-4bit arc
09/07/2024 04:42:01 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:20781
09/07/2024 04:42:11 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/07/2024 04:42:11 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/07/2024 04:42:11 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/07/2024 04:42:11 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/07/2024 04:42:11 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/07/2024 04:42:11 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/07/2024 04:42:12 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/07/2024 04:42:12 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_arc_train.json...
09/07/2024 04:42:12 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/07/2024 04:42:13 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/datasets/CDCP_arc_train.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 1472, 527, 1101, 2728, 264, 1160, 315, 13840, 315, 5552, 5811, 6956, 304, 279, 1376, 25, 18305, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 5850, 320, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 5850, 61453, 320, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 595, 948, 4718, 3465, 374, 311, 49229, 1855, 6857, 315, 5552, 5811, 6956, 304, 279, 1160, 439, 3060, 330, 20489, 1, 477, 330, 68, 28580, 3343, 1472, 2011, 471, 264, 1160, 315, 5811, 12976, 4595, 11, 26549, 315, 3160, 220, 17, 11, 304, 2768, 4823, 3645, 25, 5324, 23013, 9962, 794, 4482, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 8, 93546, 1405, 1855, 2449, 330, 23013, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 20489, 1, 477, 330, 68, 28580, 3343, 4815, 14711, 5810, 374, 279, 1495, 25, 366, 1741, 16, 29, 1423, 323, 2254, 5590, 5718, 7170, 1304, 1670, 59358, 1790, 810, 4461, 4005, 1741, 16, 1822, 1741, 17, 29, 1789, 3187, 11, 994, 264, 1732, 889, 19755, 69944, 264, 11897, 374, 3309, 311, 2586, 311, 5590, 389, 264, 990, 1938, 11, 814, 1253, 387, 9770, 311, 5268, 1990, 264, 1670, 19971, 323, 872, 2683, 4005, 1741, 17, 1822, 1741, 18, 29, 358, 33147, 279, 356, 11960, 33, 311, 1505, 12659, 430, 21736, 38952, 45348, 520, 84783, 3115, 28743, 11, 81374, 11, 323, 43658, 11, 477, 40240, 449, 220, 11739, 17, 72, 4005, 1741, 18, 397, 14711, 5810, 374, 279, 1160, 315, 13840, 315, 5552, 5811, 6956, 304, 420, 14646, 25, 4416, 15, 11, 220, 16, 1145, 510, 17, 11, 220, 15, 5163, 128009, 128006, 78191, 128007, 271, 5018, 23013, 9962, 794, 4482, 20489, 498, 330, 20489, 93546, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. You are also given a list of pairs of related argument components in the form: [(target AC (int), source AC (int)), (target AC (int), source AC (int)),..., (target AC (int), source AC (int))]. Your task is to classify each pair of related argument components in the list as either "reason" or "evidence". You must return a list of argument relation types, strictly of length 2, in following JSON format: {"relation_types": ["component_type (str)", "component_type (str)"]} where each element "relation_type (str)" is replaced by either "reason" or "evidence". 

### Here is the text: <AC1>State and local court rules sometimes make default judgments much more likely.</AC1><AC2> For example, when a person who allegedly owes a debt is told to come to court on a work day, they may be forced to choose between a default judgment and their job.</AC2><AC3> I urge the CFPB to find practices that involve scheduling hearings at inconvenient times unfair, deceptive, and abusive, or inconsistent with 1692i.</AC3>
### Here is the list of pairs of related argument components in this paragraph: [[0, 1], [2, 0]]<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"relation_types": ["reason", "reason"]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 23013, 9962, 794, 4482, 20489, 498, 330, 20489, 93546, 128009]
labels:
{"relation_types": ["reason", "reason"]}<|eot_id|>
09/07/2024 04:42:14 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/07/2024 04:42:14 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/07/2024 04:42:14 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/07/2024 04:42:14 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/07/2024 04:48:05 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/07/2024 04:48:05 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/07/2024 04:48:05 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/07/2024 04:48:05 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/07/2024 04:48:05 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,v_proj,down_proj,k_proj,o_proj,q_proj,up_proj
09/07/2024 04:48:05 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/07/2024 04:48:05 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/07/2024 04:48:05 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/07/2024 04:48:05 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/07/2024 04:48:05 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,down_proj,up_proj,q_proj,k_proj,v_proj,o_proj
09/07/2024 04:48:07 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
09/07/2024 04:48:08 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
09/07/2024 04:48:08 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/07/2024 04:48:08 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.7165, 'grad_norm': 0.1635068953037262, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.28}
{'loss': 0.0131, 'grad_norm': 0.18147142231464386, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.55}
{'loss': 0.0183, 'grad_norm': 0.4415587782859802, 'learning_rate': 4.969974623692023e-05, 'epoch': 0.83}
{'loss': 0.0158, 'grad_norm': 0.18420736491680145, 'learning_rate': 4.849231551964771e-05, 'epoch': 1.1}
{'loss': 0.0084, 'grad_norm': 0.3930169343948364, 'learning_rate': 4.640417248825667e-05, 'epoch': 1.38}
{'loss': 0.0181, 'grad_norm': 0.08114416897296906, 'learning_rate': 4.351360032772512e-05, 'epoch': 1.66}
{'loss': 0.0177, 'grad_norm': 0.030219856649637222, 'learning_rate': 3.9928964792569655e-05, 'epoch': 1.93}
{'loss': 0.0119, 'grad_norm': 0.46505504846572876, 'learning_rate': 3.578465164203134e-05, 'epoch': 2.21}
{'loss': 0.0024, 'grad_norm': 0.00887487456202507, 'learning_rate': 3.1236028601449534e-05, 'epoch': 2.48}
{'loss': 0.0084, 'grad_norm': 0.04465922340750694, 'learning_rate': 2.6453620722761896e-05, 'epoch': 2.76}
{'loss': 0.0064, 'grad_norm': 0.11524602770805359, 'learning_rate': 2.161671750624673e-05, 'epoch': 3.03}
{'loss': 0.0013, 'grad_norm': 0.1887320727109909, 'learning_rate': 1.690665144854198e-05, 'epoch': 3.31}
{'loss': 0.003, 'grad_norm': 0.4514204263687134, 'learning_rate': 1.2500000000000006e-05, 'epoch': 3.59}
{'loss': 0.0013, 'grad_norm': 0.0008389215217903256, 'learning_rate': 8.561965785773413e-06, 'epoch': 3.86}
{'loss': 0.0045, 'grad_norm': 0.006538574583828449, 'learning_rate': 5.240183262031021e-06, 'epoch': 4.14}
{'loss': 0.0005, 'grad_norm': 0.00020545943698380142, 'learning_rate': 2.659183991914696e-06, 'epoch': 4.41}
{'loss': 0.0004, 'grad_norm': 0.0018284397665411234, 'learning_rate': 9.157280346029918e-07, 'epoch': 4.69}
{'loss': 0.002, 'grad_norm': 0.5613437294960022, 'learning_rate': 7.51764708051994e-08, 'epoch': 4.97}
{'train_runtime': 5863.0585, 'train_samples_per_second': 0.495, 'train_steps_per_second': 0.031, 'train_loss': 0.047215509399150805, 'epoch': 4.97}
***** train metrics *****
  epoch                    =      4.9655
  total_flos               = 519814686GF
  train_loss               =      0.0472
  train_runtime            =  1:37:43.05
  train_samples_per_second =       0.495
  train_steps_per_second   =       0.031
09/07/2024 06:26:04 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/07/2024 06:26:04 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/07/2024 06:26:04 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/07/2024 06:26:04 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/07/2024 06:29:45 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/07/2024 06:29:50 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/cdcp/finetuned_models/CDCP_arc_llama-3-70b-Instruct-bnb-4bit
09/07/2024 06:29:50 - INFO - llamafactory.model.loader - all params: 70,657,253,376
              precision    recall  f1-score   support

    evidence      1.000     0.308     0.471        26
      reason      0.943     1.000     0.971       298

    accuracy                          0.944       324
   macro avg      0.972     0.654     0.721       324
weighted avg      0.948     0.944     0.931       324

Successfully ran CDCP_finetune.py with arguments: unsloth/llama-3-70b-Instruct-bnb-4bit arc 
 
  *************** 

