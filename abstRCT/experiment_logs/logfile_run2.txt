Running abstRCT_finetune.py with arguments: unsloth/llama-3-8b-Instruct-bnb-4bit acc
09/08/2024 18:31:14 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:27506
09/08/2024 18:31:24 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/08/2024 18:31:24 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/08/2024 18:31:24 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/08/2024 18:31:24 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/08/2024 18:31:24 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/08/2024 18:31:24 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/08/2024 18:31:28 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/08/2024 18:31:30 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/08/2024 18:31:30 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_acc_train_neo.json...
09/08/2024 18:31:31 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_acc_train_neo.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 49229, 1855, 5811, 3777, 304, 279, 1495, 439, 3060, 330, 46644, 1, 477, 330, 42562, 1082, 3343, 1472, 2011, 471, 264, 1160, 315, 5811, 3777, 4595, 11, 26549, 315, 3160, 220, 23, 11, 304, 2768, 4823, 3645, 25, 5324, 8739, 9962, 794, 4482, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 8, 93546, 1405, 1855, 2449, 330, 8739, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 46644, 1, 477, 330, 42562, 1082, 3343, 4815, 14711, 5810, 374, 279, 8278, 1495, 25, 220, 11579, 43035, 15419, 449, 293, 950, 332, 66796, 11, 264, 2536, 3751, 71916, 7294, 438, 26252, 11, 574, 7863, 449, 2211, 55681, 11, 3060, 34933, 477, 6593, 11, 304, 6978, 449, 83920, 22891, 423, 17, 47447, 9572, 13, 763, 459, 1825, 11, 47341, 11, 92520, 1992, 9269, 11, 6978, 1051, 47341, 311, 6514, 449, 220, 1135, 14060, 293, 950, 332, 66796, 320, 77, 284, 220, 14052, 8, 3131, 7446, 477, 311, 2211, 55681, 320, 77, 284, 220, 14052, 705, 3060, 477, 14946, 72783, 477, 45719, 26127, 315, 342, 24332, 33830, 65802, 349, 1475, 220, 1591, 2919, 13, 26150, 41265, 37442, 1051, 3115, 311, 6514, 8060, 323, 16945, 8624, 33824, 323, 20237, 13, 82935, 1392, 5343, 3477, 315, 66303, 68370, 2315, 11, 47447, 15696, 11, 18516, 86805, 77854, 2508, 5856, 5178, 2704, 11, 6784, 11, 3260, 4282, 292, 8670, 11, 323, 4367, 315, 2324, 14847, 13, 578, 23369, 8250, 315, 15419, 574, 220, 2137, 5672, 369, 293, 950, 332, 66796, 88186, 6978, 323, 220, 2983, 5672, 369, 2211, 496, 660, 6978, 26, 6514, 8060, 10222, 304, 220, 4331, 4, 323, 220, 2983, 4, 323, 8624, 33824, 304, 220, 3391, 4, 323, 220, 1644, 13689, 15947, 16134, 1741, 16, 29, 31969, 6372, 46603, 2211, 55681, 369, 2225, 37442, 320, 47, 366, 477, 284, 220, 15, 13, 6726, 705, 449, 31397, 42338, 320, 65, 950, 332, 66796, 25, 936, 55681, 8, 315, 220, 16, 13, 4370, 320, 2721, 4, 12410, 10074, 510, 11487, 1145, 220, 16, 13, 972, 311, 220, 17, 13, 410, 8, 369, 892, 311, 6514, 8060, 323, 220, 16, 13, 21, 320, 2721, 4, 21351, 11, 220, 16, 13, 777, 311, 220, 17, 13, 868, 8, 369, 892, 311, 8624, 33824, 13, 694, 1741, 16, 1822, 1741, 17, 29, 5659, 279, 220, 16, 4771, 20237, 6492, 11, 279, 31397, 11595, 369, 19463, 315, 4648, 574, 220, 16, 13, 1682, 320, 2721, 4, 21351, 11, 220, 15, 13, 4161, 311, 220, 16, 13, 5332, 570, 694, 1741, 17, 1822, 1741, 18, 29, 14636, 3117, 11, 449, 264, 23369, 1833, 5352, 315, 220, 4218, 5672, 11, 23369, 20237, 706, 539, 1027, 8813, 304, 3060, 1912, 13, 694, 1741, 18, 1822, 1741, 19, 29, 29240, 505, 26954, 304, 3892, 4367, 315, 2324, 7482, 1051, 12207, 2204, 320, 47, 366, 477, 284, 220, 15, 13, 1721, 8, 1990, 6514, 5315, 48582, 505, 4038, 220, 16, 311, 220, 21, 11, 323, 682, 46603, 293, 950, 332, 66796, 13, 694, 1741, 19, 1822, 1741, 20, 29, 28993, 11, 279, 7294, 438, 26252, 574, 1664, 66441, 7863, 449, 2211, 55681, 26, 694, 1741, 20, 1822, 1741, 21, 29, 449, 293, 950, 332, 66796, 11, 4106, 18698, 288, 10222, 2753, 3629, 323, 17659, 8541, 29668, 323, 342, 1910, 66274, 561, 689, 810, 3629, 13, 694, 1741, 21, 1822, 1741, 22, 29, 10541, 264, 47040, 315, 220, 1135, 14060, 315, 293, 950, 332, 66796, 3131, 7446, 574, 539, 439, 7524, 439, 2211, 55681, 11, 694, 1741, 22, 1822, 1741, 23, 29, 279, 37849, 4367, 315, 2324, 20124, 323, 279, 3428, 39775, 315, 2536, 71, 494, 25180, 31959, 4455, 3493, 8125, 311, 15806, 293, 950, 332, 66796, 11, 439, 264, 3254, 37471, 8479, 11, 520, 5190, 35130, 13, 694, 1741, 23, 29, 128009, 128006, 78191, 128007, 271, 5018, 8739, 9962, 794, 4482, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 46644, 93546, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to classify each argument component in the text as either "Claim" or "Premise". You must return a list of argument component types, strictly of length 8, in following JSON format: {"component_types": ["component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)"]} where each element "component_type (str)" is replaced by either "Claim" or "Premise". 

### Here is the abstract text:  Single-agent therapy with bicalutamide, a nonsteroidal antiandrogen, was compared with castration, either surgical or medical, in patients with untreated Stage D2 prostate cancer. In an open, randomized, multicenter trial, patients were randomized to treatment with 50 mg bicalutamide (n = 243) once daily or to castration (n = 243), either orchiectomy or depot injection of goserelin acetate every 28 days. Primary efficacy endpoints were times to treatment failure and objective disease progression and survival. Assessments included review of measurable metastases, prostate dimensions, Eastern Cooperative Oncology Group performance status, pain, analgesic requirements, and quality of life responses. The median duration of therapy was 39 weeks for bicalutamide-treated patients and 42 weeks for castrated patients; treatment failure occurred in 53% and 42% and disease progression in 43% and 33%, respectively.<AC1> Treatment effects favored castration for both endpoints (P < or = 0.002), with hazard ratios (bicalutamide:castration) of 1.54 (95% confidence interval [CI], 1.18 to 2.00) for time to treatment failure and 1.6 (95% CI, 1.19 to 2.15) for time to disease progression. </AC1><AC2> From the 1-year survival analysis, the hazard ratio for probability of death was 1.29 (95% CI, 0.96 to 1.72). </AC2><AC3> Thus far, with a median follow-up of 86 weeks, median survival has not been reached in either group. </AC3><AC4> Changes from baseline in several quality of life variables were significantly different (P < or = 0.01) between treatment groups periodically from months 1 to 6, and all favored bicalutamide. </AC4><AC5> Overall, the antiandrogen was well tolerated compared with castration; </AC5><AC6> with bicalutamide, hot flushes occurred less often and breast tenderness and gynecomastia more often. </AC6><AC7> Although a dosage of 50 mg of bicalutamide once daily was not as effective as castration, </AC7><AC8> the favorable quality of life outcomes and the low incidence of nonhormonal adverse events provide reasons to evaluate bicalutamide, as a single therapeutic agent, at higher doses. </AC8><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"component_types": ["Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Claim"]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 8739, 9962, 794, 4482, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 46644, 93546, 128009]
labels:
{"component_types": ["Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Claim"]}<|eot_id|>
09/08/2024 18:31:31 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/08/2024 18:31:31 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/08/2024 18:31:31 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/08/2024 18:31:31 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/08/2024 18:31:38 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/08/2024 18:31:38 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/08/2024 18:31:38 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/08/2024 18:31:38 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/08/2024 18:31:38 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,o_proj,q_proj,v_proj,k_proj,up_proj,down_proj
09/08/2024 18:31:38 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/08/2024 18:31:38 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/08/2024 18:31:38 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/08/2024 18:31:38 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/08/2024 18:31:38 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,gate_proj,up_proj,k_proj,v_proj,down_proj,q_proj
09/08/2024 18:31:38 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/08/2024 18:31:38 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/08/2024 18:31:39 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/08/2024 18:31:39 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.2568, 'grad_norm': 0.5234075784683228, 'learning_rate': 4.0909090909090915e-05, 'epoch': 0.45}
{'loss': 0.0669, 'grad_norm': 0.3882104754447937, 'learning_rate': 4.919871753490891e-05, 'epoch': 0.91}
{'loss': 0.0372, 'grad_norm': 0.8124232292175293, 'learning_rate': 4.6031338320779534e-05, 'epoch': 1.36}
{'loss': 0.0252, 'grad_norm': 0.2743913233280182, 'learning_rate': 4.0763816677113064e-05, 'epoch': 1.82}
{'loss': 0.0243, 'grad_norm': 0.20567980408668518, 'learning_rate': 3.392215553979679e-05, 'epoch': 2.27}
{'loss': 0.0212, 'grad_norm': 0.16937553882598877, 'learning_rate': 2.6189547895593562e-05, 'epoch': 2.73}
{'loss': 0.0172, 'grad_norm': 0.16291341185569763, 'learning_rate': 1.8338154657749128e-05, 'epoch': 3.18}
{'loss': 0.0106, 'grad_norm': 0.047648392617702484, 'learning_rate': 1.1151998403347244e-05, 'epoch': 3.64}
{'loss': 0.0074, 'grad_norm': 0.03273211419582367, 'learning_rate': 5.348672631430318e-06, 'epoch': 4.09}
{'loss': 0.0045, 'grad_norm': 0.11149288713932037, 'learning_rate': 1.5076844803522922e-06, 'epoch': 4.55}
{'loss': 0.0048, 'grad_norm': 0.09703262895345688, 'learning_rate': 1.2586440420372936e-08, 'epoch': 5.0}
{'train_runtime': 685.4269, 'train_samples_per_second': 2.553, 'train_steps_per_second': 0.16, 'train_loss': 0.043264594809575514, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  total_flos               = 53976762GF
  train_loss               =     0.0433
  train_runtime            = 0:11:25.42
  train_samples_per_second =      2.553
  train_steps_per_second   =       0.16
09/08/2024 18:43:12 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/08/2024 18:43:12 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/08/2024 18:43:12 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/08/2024 18:43:12 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/08/2024 18:43:16 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/08/2024 18:43:16 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/finetuned_models_run2/abstRCT_acc_llama-3-8b-Instruct-bnb-4bit
09/08/2024 18:43:16 - INFO - llamafactory.model.loader - all params: 8,051,232,768
              precision    recall  f1-score   support

       Claim      0.938     0.919     0.929       248
     Premise      0.955     0.966     0.961       443

    accuracy                          0.949       691
   macro avg      0.947     0.943     0.945       691
weighted avg      0.949     0.949     0.949       691

              precision    recall  f1-score   support

       Claim      0.903     0.880     0.891       191
     Premise      0.946     0.958     0.952       424

    accuracy                          0.933       615
   macro avg      0.925     0.919     0.922       615
weighted avg      0.933     0.933     0.933       615

              precision    recall  f1-score   support

       Claim      0.935     0.943     0.939       212
     Premise      0.970     0.965     0.967       397

    accuracy                          0.957       609
   macro avg      0.952     0.954     0.953       609
weighted avg      0.957     0.957     0.957       609

Successfully ran abstRCT_finetune.py with arguments: unsloth/llama-3-8b-Instruct-bnb-4bit acc 
 
  *************** 

Running abstRCT_finetune.py with arguments: unsloth/llama-3-8b-Instruct-bnb-4bit aric
09/08/2024 18:56:31 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:24475
09/08/2024 18:56:41 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/08/2024 18:56:41 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/08/2024 18:56:41 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/08/2024 18:56:41 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/08/2024 18:56:41 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/08/2024 18:56:41 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/08/2024 18:56:41 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/08/2024 18:56:41 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_aric_train_neo.json...
09/08/2024 18:56:42 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/08/2024 18:56:42 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_aric_train_neo.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 94612, 8278, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 10765, 5811, 4398, 1990, 5811, 6956, 304, 279, 8278, 1495, 323, 49229, 872, 12976, 955, 439, 3060, 330, 24249, 1, 477, 330, 21208, 3343, 1472, 2011, 471, 264, 1160, 315, 24657, 2641, 304, 279, 2768, 4823, 3645, 25, 5324, 1638, 9202, 48084, 9962, 794, 4416, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 705, 12976, 1857, 320, 496, 26090, 61453, 510, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 705, 12976, 1857, 320, 496, 8, 5163, 92, 1405, 1855, 2449, 330, 23013, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 24249, 1, 477, 330, 21208, 11690, 14711, 5810, 374, 279, 8278, 1495, 25, 220, 11579, 43035, 15419, 449, 293, 950, 332, 66796, 11, 264, 2536, 3751, 71916, 7294, 438, 26252, 11, 574, 7863, 449, 2211, 55681, 11, 3060, 34933, 477, 6593, 11, 304, 6978, 449, 83920, 22891, 423, 17, 47447, 9572, 13, 763, 459, 1825, 11, 47341, 11, 92520, 1992, 9269, 11, 6978, 1051, 47341, 311, 6514, 449, 220, 1135, 14060, 293, 950, 332, 66796, 320, 77, 284, 220, 14052, 8, 3131, 7446, 477, 311, 2211, 55681, 320, 77, 284, 220, 14052, 705, 3060, 477, 14946, 72783, 477, 45719, 26127, 315, 342, 24332, 33830, 65802, 349, 1475, 220, 1591, 2919, 13, 26150, 41265, 37442, 1051, 3115, 311, 6514, 8060, 323, 16945, 8624, 33824, 323, 20237, 13, 82935, 1392, 5343, 3477, 315, 66303, 68370, 2315, 11, 47447, 15696, 11, 18516, 86805, 77854, 2508, 5856, 5178, 2704, 11, 6784, 11, 3260, 4282, 292, 8670, 11, 323, 4367, 315, 2324, 14847, 13, 578, 23369, 8250, 315, 15419, 574, 220, 2137, 5672, 369, 293, 950, 332, 66796, 88186, 6978, 323, 220, 2983, 5672, 369, 2211, 496, 660, 6978, 26, 6514, 8060, 10222, 304, 220, 4331, 4, 323, 220, 2983, 4, 323, 8624, 33824, 304, 220, 3391, 4, 323, 220, 1644, 13689, 15947, 16134, 1741, 16, 29, 31969, 6372, 46603, 2211, 55681, 369, 2225, 37442, 320, 47, 366, 477, 284, 220, 15, 13, 6726, 705, 449, 31397, 42338, 320, 65, 950, 332, 66796, 25, 936, 55681, 8, 315, 220, 16, 13, 4370, 320, 2721, 4, 12410, 10074, 510, 11487, 1145, 220, 16, 13, 972, 311, 220, 17, 13, 410, 8, 369, 892, 311, 6514, 8060, 323, 220, 16, 13, 21, 320, 2721, 4, 21351, 11, 220, 16, 13, 777, 311, 220, 17, 13, 868, 8, 369, 892, 311, 8624, 33824, 13, 694, 1741, 16, 1822, 1741, 17, 29, 5659, 279, 220, 16, 4771, 20237, 6492, 11, 279, 31397, 11595, 369, 19463, 315, 4648, 574, 220, 16, 13, 1682, 320, 2721, 4, 21351, 11, 220, 15, 13, 4161, 311, 220, 16, 13, 5332, 570, 694, 1741, 17, 1822, 1741, 18, 29, 14636, 3117, 11, 449, 264, 23369, 1833, 5352, 315, 220, 4218, 5672, 11, 23369, 20237, 706, 539, 1027, 8813, 304, 3060, 1912, 13, 694, 1741, 18, 1822, 1741, 19, 29, 29240, 505, 26954, 304, 3892, 4367, 315, 2324, 7482, 1051, 12207, 2204, 320, 47, 366, 477, 284, 220, 15, 13, 1721, 8, 1990, 6514, 5315, 48582, 505, 4038, 220, 16, 311, 220, 21, 11, 323, 682, 46603, 293, 950, 332, 66796, 13, 694, 1741, 19, 1822, 1741, 20, 29, 28993, 11, 279, 7294, 438, 26252, 574, 1664, 66441, 7863, 449, 2211, 55681, 26, 694, 1741, 20, 1822, 1741, 21, 29, 449, 293, 950, 332, 66796, 11, 4106, 18698, 288, 10222, 2753, 3629, 323, 17659, 8541, 29668, 323, 342, 1910, 66274, 561, 689, 810, 3629, 13, 694, 1741, 21, 1822, 1741, 22, 29, 10541, 264, 47040, 315, 220, 1135, 14060, 315, 293, 950, 332, 66796, 3131, 7446, 574, 539, 439, 7524, 439, 2211, 55681, 11, 694, 1741, 22, 1822, 1741, 23, 29, 279, 37849, 4367, 315, 2324, 20124, 323, 279, 3428, 39775, 315, 2536, 71, 494, 25180, 31959, 4455, 3493, 8125, 311, 15806, 293, 950, 332, 66796, 11, 439, 264, 3254, 37471, 8479, 11, 520, 5190, 35130, 13, 694, 1741, 23, 29, 128009, 128006, 78191, 128007, 271, 5018, 1638, 9202, 48084, 9962, 794, 4416, 16, 11, 220, 23, 11, 330, 21208, 8073, 510, 17, 11, 220, 23, 11, 330, 24249, 8073, 510, 18, 11, 220, 17, 11, 330, 21208, 8073, 510, 19, 11, 220, 23, 11, 330, 24249, 8073, 510, 20, 11, 220, 23, 11, 330, 24249, 8073, 510, 21, 11, 220, 23, 11, 330, 24249, 8073, 510, 22, 11, 220, 23, 11, 330, 21208, 1365, 14316, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a biomedical abstract text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to identify argument relations between argument components in the abstract text and classify their relation type as either "support" or "attack". You must return a list of triplets in the following JSON format: {"list_argument_relation_types": [[source AC (int), target AC (int), relation_type (str)],..., [source AC (int), target AC (int), relation_type (str)]]} where each element "relation_type (str)" is replaced by either "support" or "attack".

### Here is the abstract text:  Single-agent therapy with bicalutamide, a nonsteroidal antiandrogen, was compared with castration, either surgical or medical, in patients with untreated Stage D2 prostate cancer. In an open, randomized, multicenter trial, patients were randomized to treatment with 50 mg bicalutamide (n = 243) once daily or to castration (n = 243), either orchiectomy or depot injection of goserelin acetate every 28 days. Primary efficacy endpoints were times to treatment failure and objective disease progression and survival. Assessments included review of measurable metastases, prostate dimensions, Eastern Cooperative Oncology Group performance status, pain, analgesic requirements, and quality of life responses. The median duration of therapy was 39 weeks for bicalutamide-treated patients and 42 weeks for castrated patients; treatment failure occurred in 53% and 42% and disease progression in 43% and 33%, respectively.<AC1> Treatment effects favored castration for both endpoints (P < or = 0.002), with hazard ratios (bicalutamide:castration) of 1.54 (95% confidence interval [CI], 1.18 to 2.00) for time to treatment failure and 1.6 (95% CI, 1.19 to 2.15) for time to disease progression. </AC1><AC2> From the 1-year survival analysis, the hazard ratio for probability of death was 1.29 (95% CI, 0.96 to 1.72). </AC2><AC3> Thus far, with a median follow-up of 86 weeks, median survival has not been reached in either group. </AC3><AC4> Changes from baseline in several quality of life variables were significantly different (P < or = 0.01) between treatment groups periodically from months 1 to 6, and all favored bicalutamide. </AC4><AC5> Overall, the antiandrogen was well tolerated compared with castration; </AC5><AC6> with bicalutamide, hot flushes occurred less often and breast tenderness and gynecomastia more often. </AC6><AC7> Although a dosage of 50 mg of bicalutamide once daily was not as effective as castration, </AC7><AC8> the favorable quality of life outcomes and the low incidence of nonhormonal adverse events provide reasons to evaluate bicalutamide, as a single therapeutic agent, at higher doses. </AC8><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"list_argument_relation_types": [[1, 8, "attack"], [2, 8, "support"], [3, 2, "attack"], [4, 8, "support"], [5, 8, "support"], [6, 8, "support"], [7, 8, "attack"]]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 1638, 9202, 48084, 9962, 794, 4416, 16, 11, 220, 23, 11, 330, 21208, 8073, 510, 17, 11, 220, 23, 11, 330, 24249, 8073, 510, 18, 11, 220, 17, 11, 330, 21208, 8073, 510, 19, 11, 220, 23, 11, 330, 24249, 8073, 510, 20, 11, 220, 23, 11, 330, 24249, 8073, 510, 21, 11, 220, 23, 11, 330, 24249, 8073, 510, 22, 11, 220, 23, 11, 330, 21208, 1365, 14316, 128009]
labels:
{"list_argument_relation_types": [[1, 8, "attack"], [2, 8, "support"], [3, 2, "attack"], [4, 8, "support"], [5, 8, "support"], [6, 8, "support"], [7, 8, "attack"]]}<|eot_id|>
09/08/2024 18:56:43 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/08/2024 18:56:43 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/08/2024 18:56:43 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/08/2024 18:56:43 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/08/2024 18:56:49 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/08/2024 18:56:49 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/08/2024 18:56:49 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/08/2024 18:56:49 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/08/2024 18:56:49 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,gate_proj,k_proj,o_proj,q_proj,down_proj,v_proj
09/08/2024 18:56:49 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/08/2024 18:56:49 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/08/2024 18:56:49 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/08/2024 18:56:49 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/08/2024 18:56:49 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,o_proj,k_proj,up_proj,q_proj,down_proj,v_proj
09/08/2024 18:56:50 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/08/2024 18:56:50 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/08/2024 18:56:50 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/08/2024 18:56:50 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.3337, 'grad_norm': 0.36220815777778625, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.45}
{'loss': 0.1684, 'grad_norm': 0.4008246660232544, 'learning_rate': 4.898732434036244e-05, 'epoch': 0.91}
{'loss': 0.1138, 'grad_norm': 0.3240855932235718, 'learning_rate': 4.559191453574582e-05, 'epoch': 1.36}
{'loss': 0.111, 'grad_norm': 0.35580092668533325, 'learning_rate': 4.014024217844167e-05, 'epoch': 1.82}
{'loss': 0.0861, 'grad_norm': 0.3230143189430237, 'learning_rate': 3.3176699082935545e-05, 'epoch': 2.27}
{'loss': 0.0649, 'grad_norm': 0.35956159234046936, 'learning_rate': 2.5396649095870202e-05, 'epoch': 2.73}
{'loss': 0.0517, 'grad_norm': 0.3230198323726654, 'learning_rate': 1.7576990616793137e-05, 'epoch': 3.18}
{'loss': 0.0316, 'grad_norm': 0.25059324502944946, 'learning_rate': 1.049857726072005e-05, 'epoch': 3.64}
{'loss': 0.0294, 'grad_norm': 0.38141313195228577, 'learning_rate': 4.868243561723535e-06, 'epoch': 4.09}
{'loss': 0.0153, 'grad_norm': 0.18046294152736664, 'learning_rate': 1.248222056476367e-06, 'epoch': 4.55}
{'loss': 0.014, 'grad_norm': 0.18980632722377777, 'learning_rate': 0.0, 'epoch': 5.0}
{'train_runtime': 685.9418, 'train_samples_per_second': 2.551, 'train_steps_per_second': 0.16, 'train_loss': 0.09271903430873697, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  total_flos               = 54095185GF
  train_loss               =     0.0927
  train_runtime            = 0:11:25.94
  train_samples_per_second =      2.551
  train_steps_per_second   =       0.16
09/08/2024 19:08:23 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/08/2024 19:08:24 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/08/2024 19:08:24 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/08/2024 19:08:24 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/08/2024 19:08:26 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/08/2024 19:08:27 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/finetuned_models_run2/abstRCT_aric_llama-3-8b-Instruct-bnb-4bit
09/08/2024 19:08:27 - INFO - llamafactory.model.loader - all params: 8,051,232,768
              precision    recall  f1-score   support

        None      0.983     0.979     0.981      6435
      attack      0.673     0.593     0.631        59
     support      0.670     0.733     0.700       360

    accuracy                          0.963      6854
   macro avg      0.775     0.768     0.771      6854
weighted avg      0.964     0.963     0.963      6854

              precision    recall  f1-score   support

        None      0.985     0.970     0.977      5219
      attack      0.517     0.536     0.526        28
     support      0.626     0.785     0.697       317

    accuracy                          0.957      5564
   macro avg      0.709     0.764     0.733      5564
weighted avg      0.962     0.957     0.959      5564

              precision    recall  f1-score   support

        None      0.985     0.968     0.976      5176
      attack      0.571     0.667     0.615        24
     support      0.589     0.760     0.664       296

    accuracy                          0.955      5496
   macro avg      0.715     0.798     0.752      5496
weighted avg      0.962     0.955     0.958      5496

Successfully ran abstRCT_finetune.py with arguments: unsloth/llama-3-8b-Instruct-bnb-4bit aric 
 
  *************** 

Running abstRCT_finetune.py with arguments: unsloth/llama-3-8b-Instruct acc
09/08/2024 19:28:43 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:29605
09/08/2024 19:28:53 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/08/2024 19:28:53 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/08/2024 19:28:53 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/08/2024 19:28:53 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/08/2024 19:28:53 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/08/2024 19:28:53 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/08/2024 19:28:54 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/08/2024 19:29:09 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/08/2024 19:29:09 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_acc_train_neo.json...
09/08/2024 19:29:10 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_acc_train_neo.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 49229, 1855, 5811, 3777, 304, 279, 1495, 439, 3060, 330, 46644, 1, 477, 330, 42562, 1082, 3343, 1472, 2011, 471, 264, 1160, 315, 5811, 3777, 4595, 11, 26549, 315, 3160, 220, 23, 11, 304, 2768, 4823, 3645, 25, 5324, 8739, 9962, 794, 4482, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 8, 93546, 1405, 1855, 2449, 330, 8739, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 46644, 1, 477, 330, 42562, 1082, 3343, 4815, 14711, 5810, 374, 279, 8278, 1495, 25, 220, 11579, 43035, 15419, 449, 293, 950, 332, 66796, 11, 264, 2536, 3751, 71916, 7294, 438, 26252, 11, 574, 7863, 449, 2211, 55681, 11, 3060, 34933, 477, 6593, 11, 304, 6978, 449, 83920, 22891, 423, 17, 47447, 9572, 13, 763, 459, 1825, 11, 47341, 11, 92520, 1992, 9269, 11, 6978, 1051, 47341, 311, 6514, 449, 220, 1135, 14060, 293, 950, 332, 66796, 320, 77, 284, 220, 14052, 8, 3131, 7446, 477, 311, 2211, 55681, 320, 77, 284, 220, 14052, 705, 3060, 477, 14946, 72783, 477, 45719, 26127, 315, 342, 24332, 33830, 65802, 349, 1475, 220, 1591, 2919, 13, 26150, 41265, 37442, 1051, 3115, 311, 6514, 8060, 323, 16945, 8624, 33824, 323, 20237, 13, 82935, 1392, 5343, 3477, 315, 66303, 68370, 2315, 11, 47447, 15696, 11, 18516, 86805, 77854, 2508, 5856, 5178, 2704, 11, 6784, 11, 3260, 4282, 292, 8670, 11, 323, 4367, 315, 2324, 14847, 13, 578, 23369, 8250, 315, 15419, 574, 220, 2137, 5672, 369, 293, 950, 332, 66796, 88186, 6978, 323, 220, 2983, 5672, 369, 2211, 496, 660, 6978, 26, 6514, 8060, 10222, 304, 220, 4331, 4, 323, 220, 2983, 4, 323, 8624, 33824, 304, 220, 3391, 4, 323, 220, 1644, 13689, 15947, 16134, 1741, 16, 29, 31969, 6372, 46603, 2211, 55681, 369, 2225, 37442, 320, 47, 366, 477, 284, 220, 15, 13, 6726, 705, 449, 31397, 42338, 320, 65, 950, 332, 66796, 25, 936, 55681, 8, 315, 220, 16, 13, 4370, 320, 2721, 4, 12410, 10074, 510, 11487, 1145, 220, 16, 13, 972, 311, 220, 17, 13, 410, 8, 369, 892, 311, 6514, 8060, 323, 220, 16, 13, 21, 320, 2721, 4, 21351, 11, 220, 16, 13, 777, 311, 220, 17, 13, 868, 8, 369, 892, 311, 8624, 33824, 13, 694, 1741, 16, 1822, 1741, 17, 29, 5659, 279, 220, 16, 4771, 20237, 6492, 11, 279, 31397, 11595, 369, 19463, 315, 4648, 574, 220, 16, 13, 1682, 320, 2721, 4, 21351, 11, 220, 15, 13, 4161, 311, 220, 16, 13, 5332, 570, 694, 1741, 17, 1822, 1741, 18, 29, 14636, 3117, 11, 449, 264, 23369, 1833, 5352, 315, 220, 4218, 5672, 11, 23369, 20237, 706, 539, 1027, 8813, 304, 3060, 1912, 13, 694, 1741, 18, 1822, 1741, 19, 29, 29240, 505, 26954, 304, 3892, 4367, 315, 2324, 7482, 1051, 12207, 2204, 320, 47, 366, 477, 284, 220, 15, 13, 1721, 8, 1990, 6514, 5315, 48582, 505, 4038, 220, 16, 311, 220, 21, 11, 323, 682, 46603, 293, 950, 332, 66796, 13, 694, 1741, 19, 1822, 1741, 20, 29, 28993, 11, 279, 7294, 438, 26252, 574, 1664, 66441, 7863, 449, 2211, 55681, 26, 694, 1741, 20, 1822, 1741, 21, 29, 449, 293, 950, 332, 66796, 11, 4106, 18698, 288, 10222, 2753, 3629, 323, 17659, 8541, 29668, 323, 342, 1910, 66274, 561, 689, 810, 3629, 13, 694, 1741, 21, 1822, 1741, 22, 29, 10541, 264, 47040, 315, 220, 1135, 14060, 315, 293, 950, 332, 66796, 3131, 7446, 574, 539, 439, 7524, 439, 2211, 55681, 11, 694, 1741, 22, 1822, 1741, 23, 29, 279, 37849, 4367, 315, 2324, 20124, 323, 279, 3428, 39775, 315, 2536, 71, 494, 25180, 31959, 4455, 3493, 8125, 311, 15806, 293, 950, 332, 66796, 11, 439, 264, 3254, 37471, 8479, 11, 520, 5190, 35130, 13, 694, 1741, 23, 29, 128009, 128006, 78191, 128007, 271, 5018, 8739, 9962, 794, 4482, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 46644, 93546, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to classify each argument component in the text as either "Claim" or "Premise". You must return a list of argument component types, strictly of length 8, in following JSON format: {"component_types": ["component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)"]} where each element "component_type (str)" is replaced by either "Claim" or "Premise". 

### Here is the abstract text:  Single-agent therapy with bicalutamide, a nonsteroidal antiandrogen, was compared with castration, either surgical or medical, in patients with untreated Stage D2 prostate cancer. In an open, randomized, multicenter trial, patients were randomized to treatment with 50 mg bicalutamide (n = 243) once daily or to castration (n = 243), either orchiectomy or depot injection of goserelin acetate every 28 days. Primary efficacy endpoints were times to treatment failure and objective disease progression and survival. Assessments included review of measurable metastases, prostate dimensions, Eastern Cooperative Oncology Group performance status, pain, analgesic requirements, and quality of life responses. The median duration of therapy was 39 weeks for bicalutamide-treated patients and 42 weeks for castrated patients; treatment failure occurred in 53% and 42% and disease progression in 43% and 33%, respectively.<AC1> Treatment effects favored castration for both endpoints (P < or = 0.002), with hazard ratios (bicalutamide:castration) of 1.54 (95% confidence interval [CI], 1.18 to 2.00) for time to treatment failure and 1.6 (95% CI, 1.19 to 2.15) for time to disease progression. </AC1><AC2> From the 1-year survival analysis, the hazard ratio for probability of death was 1.29 (95% CI, 0.96 to 1.72). </AC2><AC3> Thus far, with a median follow-up of 86 weeks, median survival has not been reached in either group. </AC3><AC4> Changes from baseline in several quality of life variables were significantly different (P < or = 0.01) between treatment groups periodically from months 1 to 6, and all favored bicalutamide. </AC4><AC5> Overall, the antiandrogen was well tolerated compared with castration; </AC5><AC6> with bicalutamide, hot flushes occurred less often and breast tenderness and gynecomastia more often. </AC6><AC7> Although a dosage of 50 mg of bicalutamide once daily was not as effective as castration, </AC7><AC8> the favorable quality of life outcomes and the low incidence of nonhormonal adverse events provide reasons to evaluate bicalutamide, as a single therapeutic agent, at higher doses. </AC8><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"component_types": ["Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Claim"]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 8739, 9962, 794, 4482, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 46644, 93546, 128009]
labels:
{"component_types": ["Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Claim"]}<|eot_id|>
09/08/2024 19:29:10 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/08/2024 19:29:10 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/08/2024 19:29:42 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/08/2024 19:29:42 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/08/2024 19:29:42 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/08/2024 19:29:42 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/08/2024 19:29:42 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,down_proj,o_proj,q_proj,k_proj,v_proj,gate_proj
09/08/2024 19:29:42 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/08/2024 19:29:42 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/08/2024 19:29:42 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/08/2024 19:29:42 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/08/2024 19:29:42 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,v_proj,k_proj,up_proj,q_proj,down_proj,o_proj
09/08/2024 19:29:43 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/08/2024 19:29:43 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/08/2024 19:29:43 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/08/2024 19:29:43 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.2572, 'grad_norm': 0.5301753878593445, 'learning_rate': 4.0909090909090915e-05, 'epoch': 0.45}
{'loss': 0.066, 'grad_norm': 0.4466138482093811, 'learning_rate': 4.919871753490891e-05, 'epoch': 0.91}
{'loss': 0.0384, 'grad_norm': 0.8971024751663208, 'learning_rate': 4.6031338320779534e-05, 'epoch': 1.36}
{'loss': 0.0267, 'grad_norm': 0.4538302719593048, 'learning_rate': 4.0763816677113064e-05, 'epoch': 1.82}
{'loss': 0.0257, 'grad_norm': 0.29476529359817505, 'learning_rate': 3.392215553979679e-05, 'epoch': 2.27}
{'loss': 0.0172, 'grad_norm': 0.1882167011499405, 'learning_rate': 2.6189547895593562e-05, 'epoch': 2.73}
{'loss': 0.0169, 'grad_norm': 0.18336816132068634, 'learning_rate': 1.8338154657749128e-05, 'epoch': 3.18}
{'loss': 0.0109, 'grad_norm': 0.20080366730690002, 'learning_rate': 1.1151998403347244e-05, 'epoch': 3.64}
{'loss': 0.0078, 'grad_norm': 0.04226601496338844, 'learning_rate': 5.348672631430318e-06, 'epoch': 4.09}
{'loss': 0.0048, 'grad_norm': 0.15850673615932465, 'learning_rate': 1.5076844803522922e-06, 'epoch': 4.55}
{'loss': 0.0057, 'grad_norm': 0.0941445529460907, 'learning_rate': 1.2586440420372936e-08, 'epoch': 5.0}
{'train_runtime': 601.5894, 'train_samples_per_second': 2.909, 'train_steps_per_second': 0.183, 'train_loss': 0.043398498676039954, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  total_flos               = 53976762GF
  train_loss               =     0.0434
  train_runtime            = 0:10:01.58
  train_samples_per_second =      2.909
  train_steps_per_second   =      0.183
09/08/2024 19:39:51 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/08/2024 19:39:51 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/08/2024 19:39:51 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/08/2024 19:39:57 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/08/2024 19:39:57 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/finetuned_models_run2/abstRCT_acc_llama-3-8b-Instruct
09/08/2024 19:39:57 - INFO - llamafactory.model.loader - all params: 8,051,232,768
              precision    recall  f1-score   support

       Claim      0.938     0.919     0.929       248
     Premise      0.955     0.966     0.961       443

    accuracy                          0.949       691
   macro avg      0.947     0.943     0.945       691
weighted avg      0.949     0.949     0.949       691

              precision    recall  f1-score   support

       Claim      0.909     0.890     0.899       191
     Premise      0.951     0.960     0.955       424

    accuracy                          0.938       615
   macro avg      0.930     0.925     0.927       615
weighted avg      0.938     0.938     0.938       615

              precision    recall  f1-score   support

       Claim      0.948     0.948     0.948       212
     Premise      0.972     0.972     0.972       397

    accuracy                          0.964       609
   macro avg      0.960     0.960     0.960       609
weighted avg      0.964     0.964     0.964       609

Successfully ran abstRCT_finetune.py with arguments: unsloth/llama-3-8b-Instruct acc 
 
  *************** 

Running abstRCT_finetune.py with arguments: unsloth/llama-3-8b-Instruct aric
09/08/2024 19:52:40 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:21133
09/08/2024 19:52:50 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/08/2024 19:52:50 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/08/2024 19:52:50 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/08/2024 19:52:50 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/08/2024 19:52:50 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/08/2024 19:52:50 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/08/2024 19:52:51 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/08/2024 19:52:51 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_aric_train_neo.json...
09/08/2024 19:52:51 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/08/2024 19:52:52 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_aric_train_neo.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 94612, 8278, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 10765, 5811, 4398, 1990, 5811, 6956, 304, 279, 8278, 1495, 323, 49229, 872, 12976, 955, 439, 3060, 330, 24249, 1, 477, 330, 21208, 3343, 1472, 2011, 471, 264, 1160, 315, 24657, 2641, 304, 279, 2768, 4823, 3645, 25, 5324, 1638, 9202, 48084, 9962, 794, 4416, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 705, 12976, 1857, 320, 496, 26090, 61453, 510, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 705, 12976, 1857, 320, 496, 8, 5163, 92, 1405, 1855, 2449, 330, 23013, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 24249, 1, 477, 330, 21208, 11690, 14711, 5810, 374, 279, 8278, 1495, 25, 220, 11579, 43035, 15419, 449, 293, 950, 332, 66796, 11, 264, 2536, 3751, 71916, 7294, 438, 26252, 11, 574, 7863, 449, 2211, 55681, 11, 3060, 34933, 477, 6593, 11, 304, 6978, 449, 83920, 22891, 423, 17, 47447, 9572, 13, 763, 459, 1825, 11, 47341, 11, 92520, 1992, 9269, 11, 6978, 1051, 47341, 311, 6514, 449, 220, 1135, 14060, 293, 950, 332, 66796, 320, 77, 284, 220, 14052, 8, 3131, 7446, 477, 311, 2211, 55681, 320, 77, 284, 220, 14052, 705, 3060, 477, 14946, 72783, 477, 45719, 26127, 315, 342, 24332, 33830, 65802, 349, 1475, 220, 1591, 2919, 13, 26150, 41265, 37442, 1051, 3115, 311, 6514, 8060, 323, 16945, 8624, 33824, 323, 20237, 13, 82935, 1392, 5343, 3477, 315, 66303, 68370, 2315, 11, 47447, 15696, 11, 18516, 86805, 77854, 2508, 5856, 5178, 2704, 11, 6784, 11, 3260, 4282, 292, 8670, 11, 323, 4367, 315, 2324, 14847, 13, 578, 23369, 8250, 315, 15419, 574, 220, 2137, 5672, 369, 293, 950, 332, 66796, 88186, 6978, 323, 220, 2983, 5672, 369, 2211, 496, 660, 6978, 26, 6514, 8060, 10222, 304, 220, 4331, 4, 323, 220, 2983, 4, 323, 8624, 33824, 304, 220, 3391, 4, 323, 220, 1644, 13689, 15947, 16134, 1741, 16, 29, 31969, 6372, 46603, 2211, 55681, 369, 2225, 37442, 320, 47, 366, 477, 284, 220, 15, 13, 6726, 705, 449, 31397, 42338, 320, 65, 950, 332, 66796, 25, 936, 55681, 8, 315, 220, 16, 13, 4370, 320, 2721, 4, 12410, 10074, 510, 11487, 1145, 220, 16, 13, 972, 311, 220, 17, 13, 410, 8, 369, 892, 311, 6514, 8060, 323, 220, 16, 13, 21, 320, 2721, 4, 21351, 11, 220, 16, 13, 777, 311, 220, 17, 13, 868, 8, 369, 892, 311, 8624, 33824, 13, 694, 1741, 16, 1822, 1741, 17, 29, 5659, 279, 220, 16, 4771, 20237, 6492, 11, 279, 31397, 11595, 369, 19463, 315, 4648, 574, 220, 16, 13, 1682, 320, 2721, 4, 21351, 11, 220, 15, 13, 4161, 311, 220, 16, 13, 5332, 570, 694, 1741, 17, 1822, 1741, 18, 29, 14636, 3117, 11, 449, 264, 23369, 1833, 5352, 315, 220, 4218, 5672, 11, 23369, 20237, 706, 539, 1027, 8813, 304, 3060, 1912, 13, 694, 1741, 18, 1822, 1741, 19, 29, 29240, 505, 26954, 304, 3892, 4367, 315, 2324, 7482, 1051, 12207, 2204, 320, 47, 366, 477, 284, 220, 15, 13, 1721, 8, 1990, 6514, 5315, 48582, 505, 4038, 220, 16, 311, 220, 21, 11, 323, 682, 46603, 293, 950, 332, 66796, 13, 694, 1741, 19, 1822, 1741, 20, 29, 28993, 11, 279, 7294, 438, 26252, 574, 1664, 66441, 7863, 449, 2211, 55681, 26, 694, 1741, 20, 1822, 1741, 21, 29, 449, 293, 950, 332, 66796, 11, 4106, 18698, 288, 10222, 2753, 3629, 323, 17659, 8541, 29668, 323, 342, 1910, 66274, 561, 689, 810, 3629, 13, 694, 1741, 21, 1822, 1741, 22, 29, 10541, 264, 47040, 315, 220, 1135, 14060, 315, 293, 950, 332, 66796, 3131, 7446, 574, 539, 439, 7524, 439, 2211, 55681, 11, 694, 1741, 22, 1822, 1741, 23, 29, 279, 37849, 4367, 315, 2324, 20124, 323, 279, 3428, 39775, 315, 2536, 71, 494, 25180, 31959, 4455, 3493, 8125, 311, 15806, 293, 950, 332, 66796, 11, 439, 264, 3254, 37471, 8479, 11, 520, 5190, 35130, 13, 694, 1741, 23, 29, 128009, 128006, 78191, 128007, 271, 5018, 1638, 9202, 48084, 9962, 794, 4416, 16, 11, 220, 23, 11, 330, 21208, 8073, 510, 17, 11, 220, 23, 11, 330, 24249, 8073, 510, 18, 11, 220, 17, 11, 330, 21208, 8073, 510, 19, 11, 220, 23, 11, 330, 24249, 8073, 510, 20, 11, 220, 23, 11, 330, 24249, 8073, 510, 21, 11, 220, 23, 11, 330, 24249, 8073, 510, 22, 11, 220, 23, 11, 330, 21208, 1365, 14316, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a biomedical abstract text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to identify argument relations between argument components in the abstract text and classify their relation type as either "support" or "attack". You must return a list of triplets in the following JSON format: {"list_argument_relation_types": [[source AC (int), target AC (int), relation_type (str)],..., [source AC (int), target AC (int), relation_type (str)]]} where each element "relation_type (str)" is replaced by either "support" or "attack".

### Here is the abstract text:  Single-agent therapy with bicalutamide, a nonsteroidal antiandrogen, was compared with castration, either surgical or medical, in patients with untreated Stage D2 prostate cancer. In an open, randomized, multicenter trial, patients were randomized to treatment with 50 mg bicalutamide (n = 243) once daily or to castration (n = 243), either orchiectomy or depot injection of goserelin acetate every 28 days. Primary efficacy endpoints were times to treatment failure and objective disease progression and survival. Assessments included review of measurable metastases, prostate dimensions, Eastern Cooperative Oncology Group performance status, pain, analgesic requirements, and quality of life responses. The median duration of therapy was 39 weeks for bicalutamide-treated patients and 42 weeks for castrated patients; treatment failure occurred in 53% and 42% and disease progression in 43% and 33%, respectively.<AC1> Treatment effects favored castration for both endpoints (P < or = 0.002), with hazard ratios (bicalutamide:castration) of 1.54 (95% confidence interval [CI], 1.18 to 2.00) for time to treatment failure and 1.6 (95% CI, 1.19 to 2.15) for time to disease progression. </AC1><AC2> From the 1-year survival analysis, the hazard ratio for probability of death was 1.29 (95% CI, 0.96 to 1.72). </AC2><AC3> Thus far, with a median follow-up of 86 weeks, median survival has not been reached in either group. </AC3><AC4> Changes from baseline in several quality of life variables were significantly different (P < or = 0.01) between treatment groups periodically from months 1 to 6, and all favored bicalutamide. </AC4><AC5> Overall, the antiandrogen was well tolerated compared with castration; </AC5><AC6> with bicalutamide, hot flushes occurred less often and breast tenderness and gynecomastia more often. </AC6><AC7> Although a dosage of 50 mg of bicalutamide once daily was not as effective as castration, </AC7><AC8> the favorable quality of life outcomes and the low incidence of nonhormonal adverse events provide reasons to evaluate bicalutamide, as a single therapeutic agent, at higher doses. </AC8><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"list_argument_relation_types": [[1, 8, "attack"], [2, 8, "support"], [3, 2, "attack"], [4, 8, "support"], [5, 8, "support"], [6, 8, "support"], [7, 8, "attack"]]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 1638, 9202, 48084, 9962, 794, 4416, 16, 11, 220, 23, 11, 330, 21208, 8073, 510, 17, 11, 220, 23, 11, 330, 24249, 8073, 510, 18, 11, 220, 17, 11, 330, 21208, 8073, 510, 19, 11, 220, 23, 11, 330, 24249, 8073, 510, 20, 11, 220, 23, 11, 330, 24249, 8073, 510, 21, 11, 220, 23, 11, 330, 24249, 8073, 510, 22, 11, 220, 23, 11, 330, 21208, 1365, 14316, 128009]
labels:
{"list_argument_relation_types": [[1, 8, "attack"], [2, 8, "support"], [3, 2, "attack"], [4, 8, "support"], [5, 8, "support"], [6, 8, "support"], [7, 8, "attack"]]}<|eot_id|>
09/08/2024 19:52:52 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/08/2024 19:52:52 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/08/2024 19:53:13 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/08/2024 19:53:13 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/08/2024 19:53:13 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/08/2024 19:53:13 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/08/2024 19:53:13 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,down_proj,up_proj,v_proj,o_proj,q_proj,gate_proj
09/08/2024 19:53:13 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/08/2024 19:53:13 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/08/2024 19:53:13 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/08/2024 19:53:13 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/08/2024 19:53:13 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,q_proj,k_proj,gate_proj,down_proj,up_proj,o_proj
09/08/2024 19:53:14 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/08/2024 19:53:14 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/08/2024 19:53:14 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/08/2024 19:53:14 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.3337, 'grad_norm': 0.3666394054889679, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.45}
{'loss': 0.1701, 'grad_norm': 0.359036922454834, 'learning_rate': 4.898732434036244e-05, 'epoch': 0.91}
{'loss': 0.1142, 'grad_norm': 0.3442426919937134, 'learning_rate': 4.559191453574582e-05, 'epoch': 1.36}
{'loss': 0.1118, 'grad_norm': 0.38507208228111267, 'learning_rate': 4.014024217844167e-05, 'epoch': 1.82}
{'loss': 0.0879, 'grad_norm': 0.3103371560573578, 'learning_rate': 3.3176699082935545e-05, 'epoch': 2.27}
{'loss': 0.0662, 'grad_norm': 0.3106318712234497, 'learning_rate': 2.5396649095870202e-05, 'epoch': 2.73}
{'loss': 0.0557, 'grad_norm': 0.3103596270084381, 'learning_rate': 1.7576990616793137e-05, 'epoch': 3.18}
{'loss': 0.0342, 'grad_norm': 0.22226642072200775, 'learning_rate': 1.049857726072005e-05, 'epoch': 3.64}
{'loss': 0.0307, 'grad_norm': 0.35667163133621216, 'learning_rate': 4.868243561723535e-06, 'epoch': 4.09}
{'loss': 0.0155, 'grad_norm': 0.1799701303243637, 'learning_rate': 1.248222056476367e-06, 'epoch': 4.55}
{'loss': 0.0158, 'grad_norm': 0.20151600241661072, 'learning_rate': 0.0, 'epoch': 5.0}
{'train_runtime': 602.0112, 'train_samples_per_second': 2.907, 'train_steps_per_second': 0.183, 'train_loss': 0.09416000165722586, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  total_flos               = 54095185GF
  train_loss               =     0.0942
  train_runtime            = 0:10:02.01
  train_samples_per_second =      2.907
  train_steps_per_second   =      0.183
09/08/2024 20:03:33 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/08/2024 20:03:33 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/08/2024 20:03:33 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/08/2024 20:03:39 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/08/2024 20:03:39 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/finetuned_models_run2/abstRCT_aric_llama-3-8b-Instruct
09/08/2024 20:03:39 - INFO - llamafactory.model.loader - all params: 8,051,232,768
              precision    recall  f1-score   support

        None      0.982     0.976     0.979      6435
      attack      0.673     0.593     0.631        59
     support      0.637     0.717     0.675       360

    accuracy                          0.959      6854
   macro avg      0.764     0.762     0.761      6854
weighted avg      0.961     0.959     0.960      6854

Error encountered with arguments: unsloth/llama-3-8b-Instruct aric. Skipping to the next pair. 
 
  ************* 

Running abstRCT_finetune.py with arguments: unsloth/llama-3-70b-Instruct-bnb-4bit acc
09/08/2024 20:23:27 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:26986
09/08/2024 20:23:37 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/08/2024 20:23:37 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/08/2024 20:23:37 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/08/2024 20:23:37 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/08/2024 20:23:37 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/08/2024 20:23:37 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/08/2024 20:23:37 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/08/2024 20:23:37 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/08/2024 20:23:37 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_acc_train_neo.json...
09/08/2024 20:23:39 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_acc_train_neo.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 49229, 1855, 5811, 3777, 304, 279, 1495, 439, 3060, 330, 46644, 1, 477, 330, 42562, 1082, 3343, 1472, 2011, 471, 264, 1160, 315, 5811, 3777, 4595, 11, 26549, 315, 3160, 220, 23, 11, 304, 2768, 4823, 3645, 25, 5324, 8739, 9962, 794, 4482, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 8, 93546, 1405, 1855, 2449, 330, 8739, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 46644, 1, 477, 330, 42562, 1082, 3343, 4815, 14711, 5810, 374, 279, 8278, 1495, 25, 220, 11579, 43035, 15419, 449, 293, 950, 332, 66796, 11, 264, 2536, 3751, 71916, 7294, 438, 26252, 11, 574, 7863, 449, 2211, 55681, 11, 3060, 34933, 477, 6593, 11, 304, 6978, 449, 83920, 22891, 423, 17, 47447, 9572, 13, 763, 459, 1825, 11, 47341, 11, 92520, 1992, 9269, 11, 6978, 1051, 47341, 311, 6514, 449, 220, 1135, 14060, 293, 950, 332, 66796, 320, 77, 284, 220, 14052, 8, 3131, 7446, 477, 311, 2211, 55681, 320, 77, 284, 220, 14052, 705, 3060, 477, 14946, 72783, 477, 45719, 26127, 315, 342, 24332, 33830, 65802, 349, 1475, 220, 1591, 2919, 13, 26150, 41265, 37442, 1051, 3115, 311, 6514, 8060, 323, 16945, 8624, 33824, 323, 20237, 13, 82935, 1392, 5343, 3477, 315, 66303, 68370, 2315, 11, 47447, 15696, 11, 18516, 86805, 77854, 2508, 5856, 5178, 2704, 11, 6784, 11, 3260, 4282, 292, 8670, 11, 323, 4367, 315, 2324, 14847, 13, 578, 23369, 8250, 315, 15419, 574, 220, 2137, 5672, 369, 293, 950, 332, 66796, 88186, 6978, 323, 220, 2983, 5672, 369, 2211, 496, 660, 6978, 26, 6514, 8060, 10222, 304, 220, 4331, 4, 323, 220, 2983, 4, 323, 8624, 33824, 304, 220, 3391, 4, 323, 220, 1644, 13689, 15947, 16134, 1741, 16, 29, 31969, 6372, 46603, 2211, 55681, 369, 2225, 37442, 320, 47, 366, 477, 284, 220, 15, 13, 6726, 705, 449, 31397, 42338, 320, 65, 950, 332, 66796, 25, 936, 55681, 8, 315, 220, 16, 13, 4370, 320, 2721, 4, 12410, 10074, 510, 11487, 1145, 220, 16, 13, 972, 311, 220, 17, 13, 410, 8, 369, 892, 311, 6514, 8060, 323, 220, 16, 13, 21, 320, 2721, 4, 21351, 11, 220, 16, 13, 777, 311, 220, 17, 13, 868, 8, 369, 892, 311, 8624, 33824, 13, 694, 1741, 16, 1822, 1741, 17, 29, 5659, 279, 220, 16, 4771, 20237, 6492, 11, 279, 31397, 11595, 369, 19463, 315, 4648, 574, 220, 16, 13, 1682, 320, 2721, 4, 21351, 11, 220, 15, 13, 4161, 311, 220, 16, 13, 5332, 570, 694, 1741, 17, 1822, 1741, 18, 29, 14636, 3117, 11, 449, 264, 23369, 1833, 5352, 315, 220, 4218, 5672, 11, 23369, 20237, 706, 539, 1027, 8813, 304, 3060, 1912, 13, 694, 1741, 18, 1822, 1741, 19, 29, 29240, 505, 26954, 304, 3892, 4367, 315, 2324, 7482, 1051, 12207, 2204, 320, 47, 366, 477, 284, 220, 15, 13, 1721, 8, 1990, 6514, 5315, 48582, 505, 4038, 220, 16, 311, 220, 21, 11, 323, 682, 46603, 293, 950, 332, 66796, 13, 694, 1741, 19, 1822, 1741, 20, 29, 28993, 11, 279, 7294, 438, 26252, 574, 1664, 66441, 7863, 449, 2211, 55681, 26, 694, 1741, 20, 1822, 1741, 21, 29, 449, 293, 950, 332, 66796, 11, 4106, 18698, 288, 10222, 2753, 3629, 323, 17659, 8541, 29668, 323, 342, 1910, 66274, 561, 689, 810, 3629, 13, 694, 1741, 21, 1822, 1741, 22, 29, 10541, 264, 47040, 315, 220, 1135, 14060, 315, 293, 950, 332, 66796, 3131, 7446, 574, 539, 439, 7524, 439, 2211, 55681, 11, 694, 1741, 22, 1822, 1741, 23, 29, 279, 37849, 4367, 315, 2324, 20124, 323, 279, 3428, 39775, 315, 2536, 71, 494, 25180, 31959, 4455, 3493, 8125, 311, 15806, 293, 950, 332, 66796, 11, 439, 264, 3254, 37471, 8479, 11, 520, 5190, 35130, 13, 694, 1741, 23, 29, 128009, 128006, 78191, 128007, 271, 5018, 8739, 9962, 794, 4482, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 46644, 93546, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to classify each argument component in the text as either "Claim" or "Premise". You must return a list of argument component types, strictly of length 8, in following JSON format: {"component_types": ["component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)"]} where each element "component_type (str)" is replaced by either "Claim" or "Premise". 

### Here is the abstract text:  Single-agent therapy with bicalutamide, a nonsteroidal antiandrogen, was compared with castration, either surgical or medical, in patients with untreated Stage D2 prostate cancer. In an open, randomized, multicenter trial, patients were randomized to treatment with 50 mg bicalutamide (n = 243) once daily or to castration (n = 243), either orchiectomy or depot injection of goserelin acetate every 28 days. Primary efficacy endpoints were times to treatment failure and objective disease progression and survival. Assessments included review of measurable metastases, prostate dimensions, Eastern Cooperative Oncology Group performance status, pain, analgesic requirements, and quality of life responses. The median duration of therapy was 39 weeks for bicalutamide-treated patients and 42 weeks for castrated patients; treatment failure occurred in 53% and 42% and disease progression in 43% and 33%, respectively.<AC1> Treatment effects favored castration for both endpoints (P < or = 0.002), with hazard ratios (bicalutamide:castration) of 1.54 (95% confidence interval [CI], 1.18 to 2.00) for time to treatment failure and 1.6 (95% CI, 1.19 to 2.15) for time to disease progression. </AC1><AC2> From the 1-year survival analysis, the hazard ratio for probability of death was 1.29 (95% CI, 0.96 to 1.72). </AC2><AC3> Thus far, with a median follow-up of 86 weeks, median survival has not been reached in either group. </AC3><AC4> Changes from baseline in several quality of life variables were significantly different (P < or = 0.01) between treatment groups periodically from months 1 to 6, and all favored bicalutamide. </AC4><AC5> Overall, the antiandrogen was well tolerated compared with castration; </AC5><AC6> with bicalutamide, hot flushes occurred less often and breast tenderness and gynecomastia more often. </AC6><AC7> Although a dosage of 50 mg of bicalutamide once daily was not as effective as castration, </AC7><AC8> the favorable quality of life outcomes and the low incidence of nonhormonal adverse events provide reasons to evaluate bicalutamide, as a single therapeutic agent, at higher doses. </AC8><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"component_types": ["Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Claim"]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 8739, 9962, 794, 4482, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 46644, 93546, 128009]
labels:
{"component_types": ["Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Claim"]}<|eot_id|>
09/08/2024 20:23:39 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/08/2024 20:23:39 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/08/2024 20:23:39 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/08/2024 20:23:39 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/08/2024 20:29:31 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/08/2024 20:29:32 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/08/2024 20:29:32 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/08/2024 20:29:32 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/08/2024 20:29:32 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,q_proj,o_proj,up_proj,v_proj,gate_proj,down_proj
09/08/2024 20:29:32 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/08/2024 20:29:32 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/08/2024 20:29:32 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/08/2024 20:29:32 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/08/2024 20:29:32 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,up_proj,v_proj,q_proj,down_proj,o_proj,k_proj
09/08/2024 20:29:34 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
09/08/2024 20:29:34 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
09/08/2024 20:29:34 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/08/2024 20:29:34 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.2428, 'grad_norm': 0.5723913311958313, 'learning_rate': 3.6363636363636364e-05, 'epoch': 0.45}
{'loss': 0.0644, 'grad_norm': 0.3616635501384735, 'learning_rate': 4.9548217431567665e-05, 'epoch': 0.91}
{'loss': 0.0312, 'grad_norm': 0.11254473030567169, 'learning_rate': 4.684623442674463e-05, 'epoch': 1.36}
{'loss': 0.0248, 'grad_norm': 0.20391422510147095, 'learning_rate': 4.1962735288928305e-05, 'epoch': 1.82}
{'loss': 0.0274, 'grad_norm': 0.17182275652885437, 'learning_rate': 3.5385375325047166e-05, 'epoch': 2.27}
{'loss': 0.0244, 'grad_norm': 0.13175493478775024, 'learning_rate': 2.7770954997525277e-05, 'epoch': 2.73}
{'loss': 0.0139, 'grad_norm': 0.11315827071666718, 'learning_rate': 1.9879833298370238e-05, 'epoch': 3.18}
{'loss': 0.011, 'grad_norm': 0.04918109253048897, 'learning_rate': 1.2500000000000006e-05, 'epoch': 3.64}
{'loss': 0.0112, 'grad_norm': 0.08633320778608322, 'learning_rate': 6.368388758106133e-06, 'epoch': 4.09}
{'loss': 0.0052, 'grad_norm': 0.1259118616580963, 'learning_rate': 2.09728856419826e-06, 'epoch': 4.55}
{'loss': 0.0062, 'grad_norm': 0.1014217734336853, 'learning_rate': 1.1320193567288529e-07, 'epoch': 5.0}
{'train_runtime': 4820.8185, 'train_samples_per_second': 0.363, 'train_steps_per_second': 0.023, 'train_loss': 0.042061504755507816, 'epoch': 5.0}
***** train metrics *****
  epoch                    =         5.0
  total_flos               = 499227956GF
  train_loss               =      0.0421
  train_runtime            =  1:20:20.81
  train_samples_per_second =       0.363
  train_steps_per_second   =       0.023
09/08/2024 21:50:14 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/08/2024 21:50:15 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/08/2024 21:50:15 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/08/2024 21:50:15 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/08/2024 21:54:13 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/08/2024 21:54:18 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/finetuned_models_run2/abstRCT_acc_llama-3-70b-Instruct-bnb-4bit
09/08/2024 21:54:18 - INFO - llamafactory.model.loader - all params: 70,657,253,376
              precision    recall  f1-score   support

       Claim      0.922     0.907     0.915       248
     Premise      0.949     0.957     0.953       443

    accuracy                          0.939       691
   macro avg      0.935     0.932     0.934       691
weighted avg      0.939     0.939     0.939       691

              precision    recall  f1-score   support

       Claim      0.895     0.895     0.895       191
     Premise      0.953     0.953     0.953       424

    accuracy                          0.935       615
   macro avg      0.924     0.924     0.924       615
weighted avg      0.935     0.935     0.935       615

              precision    recall  f1-score   support

       Claim      0.935     0.943     0.939       212
     Premise      0.970     0.965     0.967       397

    accuracy                          0.957       609
   macro avg      0.952     0.954     0.953       609
weighted avg      0.957     0.957     0.957       609

Successfully ran abstRCT_finetune.py with arguments: unsloth/llama-3-70b-Instruct-bnb-4bit acc 
 
  *************** 

Running abstRCT_finetune.py with arguments: unsloth/llama-3-70b-Instruct-bnb-4bit aric
09/08/2024 22:42:56 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:26133
09/08/2024 22:43:07 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/08/2024 22:43:07 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/08/2024 22:43:07 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/08/2024 22:43:07 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/08/2024 22:43:07 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/08/2024 22:43:07 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/08/2024 22:43:07 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/08/2024 22:43:07 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/08/2024 22:43:07 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_aric_train_neo.json...
09/08/2024 22:43:08 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_aric_train_neo.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 94612, 8278, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 10765, 5811, 4398, 1990, 5811, 6956, 304, 279, 8278, 1495, 323, 49229, 872, 12976, 955, 439, 3060, 330, 24249, 1, 477, 330, 21208, 3343, 1472, 2011, 471, 264, 1160, 315, 24657, 2641, 304, 279, 2768, 4823, 3645, 25, 5324, 1638, 9202, 48084, 9962, 794, 4416, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 705, 12976, 1857, 320, 496, 26090, 61453, 510, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 705, 12976, 1857, 320, 496, 8, 5163, 92, 1405, 1855, 2449, 330, 23013, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 24249, 1, 477, 330, 21208, 11690, 14711, 5810, 374, 279, 8278, 1495, 25, 220, 11579, 43035, 15419, 449, 293, 950, 332, 66796, 11, 264, 2536, 3751, 71916, 7294, 438, 26252, 11, 574, 7863, 449, 2211, 55681, 11, 3060, 34933, 477, 6593, 11, 304, 6978, 449, 83920, 22891, 423, 17, 47447, 9572, 13, 763, 459, 1825, 11, 47341, 11, 92520, 1992, 9269, 11, 6978, 1051, 47341, 311, 6514, 449, 220, 1135, 14060, 293, 950, 332, 66796, 320, 77, 284, 220, 14052, 8, 3131, 7446, 477, 311, 2211, 55681, 320, 77, 284, 220, 14052, 705, 3060, 477, 14946, 72783, 477, 45719, 26127, 315, 342, 24332, 33830, 65802, 349, 1475, 220, 1591, 2919, 13, 26150, 41265, 37442, 1051, 3115, 311, 6514, 8060, 323, 16945, 8624, 33824, 323, 20237, 13, 82935, 1392, 5343, 3477, 315, 66303, 68370, 2315, 11, 47447, 15696, 11, 18516, 86805, 77854, 2508, 5856, 5178, 2704, 11, 6784, 11, 3260, 4282, 292, 8670, 11, 323, 4367, 315, 2324, 14847, 13, 578, 23369, 8250, 315, 15419, 574, 220, 2137, 5672, 369, 293, 950, 332, 66796, 88186, 6978, 323, 220, 2983, 5672, 369, 2211, 496, 660, 6978, 26, 6514, 8060, 10222, 304, 220, 4331, 4, 323, 220, 2983, 4, 323, 8624, 33824, 304, 220, 3391, 4, 323, 220, 1644, 13689, 15947, 16134, 1741, 16, 29, 31969, 6372, 46603, 2211, 55681, 369, 2225, 37442, 320, 47, 366, 477, 284, 220, 15, 13, 6726, 705, 449, 31397, 42338, 320, 65, 950, 332, 66796, 25, 936, 55681, 8, 315, 220, 16, 13, 4370, 320, 2721, 4, 12410, 10074, 510, 11487, 1145, 220, 16, 13, 972, 311, 220, 17, 13, 410, 8, 369, 892, 311, 6514, 8060, 323, 220, 16, 13, 21, 320, 2721, 4, 21351, 11, 220, 16, 13, 777, 311, 220, 17, 13, 868, 8, 369, 892, 311, 8624, 33824, 13, 694, 1741, 16, 1822, 1741, 17, 29, 5659, 279, 220, 16, 4771, 20237, 6492, 11, 279, 31397, 11595, 369, 19463, 315, 4648, 574, 220, 16, 13, 1682, 320, 2721, 4, 21351, 11, 220, 15, 13, 4161, 311, 220, 16, 13, 5332, 570, 694, 1741, 17, 1822, 1741, 18, 29, 14636, 3117, 11, 449, 264, 23369, 1833, 5352, 315, 220, 4218, 5672, 11, 23369, 20237, 706, 539, 1027, 8813, 304, 3060, 1912, 13, 694, 1741, 18, 1822, 1741, 19, 29, 29240, 505, 26954, 304, 3892, 4367, 315, 2324, 7482, 1051, 12207, 2204, 320, 47, 366, 477, 284, 220, 15, 13, 1721, 8, 1990, 6514, 5315, 48582, 505, 4038, 220, 16, 311, 220, 21, 11, 323, 682, 46603, 293, 950, 332, 66796, 13, 694, 1741, 19, 1822, 1741, 20, 29, 28993, 11, 279, 7294, 438, 26252, 574, 1664, 66441, 7863, 449, 2211, 55681, 26, 694, 1741, 20, 1822, 1741, 21, 29, 449, 293, 950, 332, 66796, 11, 4106, 18698, 288, 10222, 2753, 3629, 323, 17659, 8541, 29668, 323, 342, 1910, 66274, 561, 689, 810, 3629, 13, 694, 1741, 21, 1822, 1741, 22, 29, 10541, 264, 47040, 315, 220, 1135, 14060, 315, 293, 950, 332, 66796, 3131, 7446, 574, 539, 439, 7524, 439, 2211, 55681, 11, 694, 1741, 22, 1822, 1741, 23, 29, 279, 37849, 4367, 315, 2324, 20124, 323, 279, 3428, 39775, 315, 2536, 71, 494, 25180, 31959, 4455, 3493, 8125, 311, 15806, 293, 950, 332, 66796, 11, 439, 264, 3254, 37471, 8479, 11, 520, 5190, 35130, 13, 694, 1741, 23, 29, 128009, 128006, 78191, 128007, 271, 5018, 1638, 9202, 48084, 9962, 794, 4416, 16, 11, 220, 23, 11, 330, 21208, 8073, 510, 17, 11, 220, 23, 11, 330, 24249, 8073, 510, 18, 11, 220, 17, 11, 330, 21208, 8073, 510, 19, 11, 220, 23, 11, 330, 24249, 8073, 510, 20, 11, 220, 23, 11, 330, 24249, 8073, 510, 21, 11, 220, 23, 11, 330, 24249, 8073, 510, 22, 11, 220, 23, 11, 330, 21208, 1365, 14316, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a biomedical abstract text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to identify argument relations between argument components in the abstract text and classify their relation type as either "support" or "attack". You must return a list of triplets in the following JSON format: {"list_argument_relation_types": [[source AC (int), target AC (int), relation_type (str)],..., [source AC (int), target AC (int), relation_type (str)]]} where each element "relation_type (str)" is replaced by either "support" or "attack".

### Here is the abstract text:  Single-agent therapy with bicalutamide, a nonsteroidal antiandrogen, was compared with castration, either surgical or medical, in patients with untreated Stage D2 prostate cancer. In an open, randomized, multicenter trial, patients were randomized to treatment with 50 mg bicalutamide (n = 243) once daily or to castration (n = 243), either orchiectomy or depot injection of goserelin acetate every 28 days. Primary efficacy endpoints were times to treatment failure and objective disease progression and survival. Assessments included review of measurable metastases, prostate dimensions, Eastern Cooperative Oncology Group performance status, pain, analgesic requirements, and quality of life responses. The median duration of therapy was 39 weeks for bicalutamide-treated patients and 42 weeks for castrated patients; treatment failure occurred in 53% and 42% and disease progression in 43% and 33%, respectively.<AC1> Treatment effects favored castration for both endpoints (P < or = 0.002), with hazard ratios (bicalutamide:castration) of 1.54 (95% confidence interval [CI], 1.18 to 2.00) for time to treatment failure and 1.6 (95% CI, 1.19 to 2.15) for time to disease progression. </AC1><AC2> From the 1-year survival analysis, the hazard ratio for probability of death was 1.29 (95% CI, 0.96 to 1.72). </AC2><AC3> Thus far, with a median follow-up of 86 weeks, median survival has not been reached in either group. </AC3><AC4> Changes from baseline in several quality of life variables were significantly different (P < or = 0.01) between treatment groups periodically from months 1 to 6, and all favored bicalutamide. </AC4><AC5> Overall, the antiandrogen was well tolerated compared with castration; </AC5><AC6> with bicalutamide, hot flushes occurred less often and breast tenderness and gynecomastia more often. </AC6><AC7> Although a dosage of 50 mg of bicalutamide once daily was not as effective as castration, </AC7><AC8> the favorable quality of life outcomes and the low incidence of nonhormonal adverse events provide reasons to evaluate bicalutamide, as a single therapeutic agent, at higher doses. </AC8><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"list_argument_relation_types": [[1, 8, "attack"], [2, 8, "support"], [3, 2, "attack"], [4, 8, "support"], [5, 8, "support"], [6, 8, "support"], [7, 8, "attack"]]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 1638, 9202, 48084, 9962, 794, 4416, 16, 11, 220, 23, 11, 330, 21208, 8073, 510, 17, 11, 220, 23, 11, 330, 24249, 8073, 510, 18, 11, 220, 17, 11, 330, 21208, 8073, 510, 19, 11, 220, 23, 11, 330, 24249, 8073, 510, 20, 11, 220, 23, 11, 330, 24249, 8073, 510, 21, 11, 220, 23, 11, 330, 24249, 8073, 510, 22, 11, 220, 23, 11, 330, 21208, 1365, 14316, 128009]
labels:
{"list_argument_relation_types": [[1, 8, "attack"], [2, 8, "support"], [3, 2, "attack"], [4, 8, "support"], [5, 8, "support"], [6, 8, "support"], [7, 8, "attack"]]}<|eot_id|>
09/08/2024 22:43:09 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/08/2024 22:43:09 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/08/2024 22:43:09 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/08/2024 22:43:09 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/08/2024 22:49:01 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/08/2024 22:49:01 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/08/2024 22:49:01 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/08/2024 22:49:01 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/08/2024 22:49:01 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,down_proj,o_proj,k_proj,gate_proj,up_proj,q_proj
09/08/2024 22:49:01 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/08/2024 22:49:01 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/08/2024 22:49:01 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/08/2024 22:49:01 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/08/2024 22:49:01 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,v_proj,o_proj,down_proj,up_proj,gate_proj,k_proj
09/08/2024 22:49:03 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
09/08/2024 22:49:04 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
09/08/2024 22:49:04 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/08/2024 22:49:04 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.6385, 'grad_norm': 0.7812301516532898, 'learning_rate': 3.181818181818182e-05, 'epoch': 0.45}
{'loss': 0.1721, 'grad_norm': 0.18219034373760223, 'learning_rate': 4.9548217431567665e-05, 'epoch': 0.91}
{'loss': 0.1198, 'grad_norm': 0.30048853158950806, 'learning_rate': 4.684623442674463e-05, 'epoch': 1.36}
{'loss': 0.1111, 'grad_norm': 0.18805734813213348, 'learning_rate': 4.1962735288928305e-05, 'epoch': 1.82}
{'loss': 0.0903, 'grad_norm': 0.17679208517074585, 'learning_rate': 3.5385375325047166e-05, 'epoch': 2.27}
{'loss': 0.0787, 'grad_norm': 0.21888922154903412, 'learning_rate': 2.7770954997525277e-05, 'epoch': 2.73}
{'loss': 0.0677, 'grad_norm': 0.3158656060695648, 'learning_rate': 1.9879833298370238e-05, 'epoch': 3.18}
{'loss': 0.0539, 'grad_norm': 0.12873397767543793, 'learning_rate': 1.2500000000000006e-05, 'epoch': 3.64}
{'loss': 0.0516, 'grad_norm': 0.23539751768112183, 'learning_rate': 6.368388758106133e-06, 'epoch': 4.09}
{'loss': 0.033, 'grad_norm': 0.1744159758090973, 'learning_rate': 2.4268365428344736e-06, 'epoch': 4.55}
{'loss': 0.0333, 'grad_norm': 0.2887040972709656, 'learning_rate': 2.011296792301165e-07, 'epoch': 5.0}
{'train_runtime': 4826.879, 'train_samples_per_second': 0.363, 'train_steps_per_second': 0.023, 'train_loss': 0.13182491849769246, 'epoch': 5.0}
***** train metrics *****
  epoch                    =         5.0
  total_flos               = 500323264GF
  train_loss               =      0.1318
  train_runtime            =  1:20:26.87
  train_samples_per_second =       0.363
  train_steps_per_second   =       0.023
09/09/2024 00:09:44 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 00:09:44 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 00:09:44 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 00:09:44 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/09/2024 00:13:48 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 00:13:53 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/finetuned_models_run2/abstRCT_aric_llama-3-70b-Instruct-bnb-4bit
09/09/2024 00:13:53 - INFO - llamafactory.model.loader - all params: 70,657,253,376
              precision    recall  f1-score   support

        None      0.982     0.977     0.980      6435
      attack      0.617     0.627     0.622        59
     support      0.662     0.719     0.690       360

    accuracy                          0.961      6854
   macro avg      0.754     0.775     0.764      6854
weighted avg      0.962     0.961     0.962      6854

              precision    recall  f1-score   support

        None      0.984     0.975     0.979      5219
      attack      0.519     0.500     0.509        28
     support      0.668     0.770     0.716       317

    accuracy                          0.961      5564
   macro avg      0.724     0.748     0.735      5564
weighted avg      0.963     0.961     0.962      5564

              precision    recall  f1-score   support

        None      0.986     0.973     0.979      5176
      attack      0.481     0.542     0.510        24
     support      0.643     0.784     0.706       296

    accuracy                          0.961      5496
   macro avg      0.703     0.766     0.732      5496
weighted avg      0.965     0.961     0.963      5496

Successfully ran abstRCT_finetune.py with arguments: unsloth/llama-3-70b-Instruct-bnb-4bit aric 
 
  *************** 

