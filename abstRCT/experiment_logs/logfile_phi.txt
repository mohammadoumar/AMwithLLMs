Running abstRCT_finetune.py with arguments: unsloth/Phi-3-mini-4k-instruct-bnb-4bit acc
09/11/2024 23:42:51 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:27017
09/11/2024 23:43:01 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/11/2024 23:43:01 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/11/2024 23:43:01 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/11/2024 23:43:01 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/11/2024 23:43:01 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/11/2024 23:43:01 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/11/2024 23:43:01 - INFO - llamafactory.data.template - Replace eos token: <|end|>
09/11/2024 23:43:01 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.
09/11/2024 23:43:01 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_acc_train_neo.json...
09/11/2024 23:43:01 - INFO - llamafactory.data.template - Replace eos token: <|end|>
09/11/2024 23:43:01 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.
09/11/2024 23:43:03 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_acc_train_neo.json...
training example:
input_ids:
[1, 32010, 835, 887, 526, 385, 17924, 297, 23125, 341, 2827, 29889, 887, 526, 2183, 263, 1426, 607, 3743, 1353, 287, 2980, 7117, 427, 15603, 491, 529, 2477, 2565, 2477, 29958, 8282, 29889, 3575, 3414, 338, 304, 770, 1598, 1269, 2980, 4163, 297, 278, 1426, 408, 2845, 376, 29907, 8342, 29908, 470, 376, 29925, 1745, 895, 1642, 887, 1818, 736, 263, 1051, 310, 2980, 4163, 4072, 29892, 18719, 310, 3309, 29871, 29947, 29892, 297, 1494, 4663, 3402, 29901, 8853, 9700, 29918, 8768, 1115, 6796, 9700, 29918, 1853, 313, 710, 19123, 376, 9700, 29918, 1853, 313, 710, 19123, 376, 9700, 29918, 1853, 313, 710, 19123, 376, 9700, 29918, 1853, 313, 710, 19123, 376, 9700, 29918, 1853, 313, 710, 19123, 376, 9700, 29918, 1853, 313, 710, 19123, 376, 9700, 29918, 1853, 313, 710, 19123, 376, 9700, 29918, 1853, 313, 710, 29897, 3108, 29913, 988, 1269, 1543, 376, 9700, 29918, 1853, 313, 710, 5513, 338, 8611, 491, 2845, 376, 29907, 8342, 29908, 470, 376, 29925, 1745, 895, 1642, 29871, 13, 13, 2277, 29937, 2266, 338, 278, 9846, 1426, 29901, 29871, 16740, 29899, 14748, 29220, 27580, 411, 289, 936, 329, 314, 680, 29892, 263, 1661, 303, 1489, 23670, 9418, 14907, 1885, 29892, 471, 9401, 411, 3209, 509, 362, 29892, 2845, 25300, 936, 470, 16083, 29892, 297, 22069, 411, 443, 2484, 630, 24906, 360, 29906, 410, 3859, 23900, 29889, 512, 385, 1722, 29892, 4036, 1891, 29892, 1773, 293, 5893, 14260, 29892, 22069, 892, 4036, 1891, 304, 14502, 411, 29871, 29945, 29900, 286, 29887, 289, 936, 329, 314, 680, 313, 29876, 353, 29871, 29906, 29946, 29941, 29897, 2748, 14218, 470, 304, 3209, 509, 362, 313, 29876, 353, 29871, 29906, 29946, 29941, 511, 2845, 22624, 347, 312, 16103, 470, 1401, 327, 20859, 310, 330, 22969, 24446, 1274, 300, 403, 1432, 29871, 29906, 29947, 3841, 29889, 28267, 6366, 4135, 1095, 9748, 892, 3064, 304, 14502, 10672, 322, 12091, 17135, 410, 11476, 322, 10503, 2561, 29889, 4007, 404, 1860, 5134, 9076, 310, 7540, 21115, 1539, 579, 2129, 29892, 410, 3859, 13391, 29892, 16162, 21851, 1230, 1551, 1054, 6933, 6431, 4180, 4660, 29892, 6788, 29892, 3483, 2710, 293, 11780, 29892, 322, 11029, 310, 2834, 20890, 29889, 450, 19194, 14385, 310, 29220, 27580, 471, 29871, 29941, 29929, 11405, 363, 289, 936, 329, 314, 680, 29899, 2484, 630, 22069, 322, 29871, 29946, 29906, 11405, 363, 3209, 509, 630, 22069, 29936, 14502, 10672, 10761, 297, 29871, 29945, 29941, 29995, 322, 29871, 29946, 29906, 29995, 322, 17135, 410, 11476, 297, 29871, 29946, 29941, 29995, 322, 29871, 29941, 29941, 13667, 8307, 19423, 2477, 29896, 29958, 6479, 271, 358, 9545, 5025, 4395, 3209, 509, 362, 363, 1716, 1095, 9748, 313, 29925, 529, 470, 353, 29871, 29900, 29889, 29900, 29900, 29906, 511, 411, 447, 29920, 538, 364, 2219, 359, 313, 29890, 936, 329, 314, 680, 29901, 29883, 7614, 362, 29897, 310, 29871, 29896, 29889, 29945, 29946, 313, 29929, 29945, 29995, 16420, 7292, 518, 8426, 1402, 29871, 29896, 29889, 29896, 29947, 304, 29871, 29906, 29889, 29900, 29900, 29897, 363, 931, 304, 14502, 10672, 322, 29871, 29896, 29889, 29953, 313, 29929, 29945, 29995, 25781, 29892, 29871, 29896, 29889, 29896, 29929, 304, 29871, 29906, 29889, 29896, 29945, 29897, 363, 931, 304, 17135, 410, 11476, 29889, 1533, 2477, 29896, 5299, 2477, 29906, 29958, 3645, 278, 29871, 29896, 29899, 6360, 10503, 2561, 7418, 29892, 278, 447, 29920, 538, 11959, 363, 6976, 310, 4892, 471, 29871, 29896, 29889, 29906, 29929, 313, 29929, 29945, 29995, 25781, 29892, 29871, 29900, 29889, 29929, 29953, 304, 29871, 29896, 29889, 29955, 29906, 467, 1533, 2477, 29906, 5299, 2477, 29941, 29958, 6549, 2215, 29892, 411, 263, 19194, 1101, 29899, 786, 310, 29871, 29947, 29953, 11405, 29892, 19194, 10503, 2561, 756, 451, 1063, 7450, 297, 2845, 2318, 29889, 1533, 2477, 29941, 5299, 2477, 29946, 29958, 678, 6916, 515, 2362, 5570, 297, 3196, 11029, 310, 2834, 3651, 892, 16951, 1422, 313, 29925, 529, 470, 353, 29871, 29900, 29889, 29900, 29896, 29897, 1546, 14502, 6471, 3785, 1711, 515, 7378, 29871, 29896, 304, 29871, 29953, 29892, 322, 599, 5025, 4395, 289, 936, 329, 314, 680, 29889, 1533, 2477, 29946, 5299, 2477, 29945, 29958, 6811, 497, 29892, 278, 9418, 14907, 1885, 471, 1532, 20341, 630, 9401, 411, 3209, 509, 362, 29936, 1533, 2477, 29945, 5299, 2477, 29953, 29958, 411, 289, 936, 329, 314, 680, 29892, 7375, 28371, 267, 10761, 3109, 4049, 322, 24207, 10331, 824, 404, 322, 10966, 484, 510, 579, 423, 901, 4049, 29889, 1533, 2477, 29953, 5299, 2477, 29955, 29958, 8512, 263, 3248, 482, 310, 29871, 29945, 29900, 286, 29887, 310, 289, 936, 329, 314, 680, 2748, 14218, 471, 451, 408, 11828, 408, 3209, 509, 362, 29892, 1533, 2477, 29955, 5299, 2477, 29947, 29958, 278, 7853, 519, 11029, 310, 2834, 714, 26807, 322, 278, 4482, 5528, 5084, 310, 1661, 29882, 555, 7177, 594, 3901, 4959, 3867, 9590, 304, 14707, 289, 936, 329, 314, 680, 29892, 408, 263, 2323, 266, 1572, 412, 329, 293, 10823, 29892, 472, 6133, 3248, 267, 29889, 1533, 2477, 29947, 29958, 32007, 29871, 13, 32001, 8853, 9700, 29918, 8768, 1115, 6796, 29925, 1745, 895, 613, 376, 29925, 1745, 895, 613, 376, 29925, 1745, 895, 613, 376, 29925, 1745, 895, 613, 376, 29925, 1745, 895, 613, 376, 29925, 1745, 895, 613, 376, 29925, 1745, 895, 613, 376, 29907, 8342, 3108, 29913, 32007]
inputs:
<s><|user|> ### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to classify each argument component in the text as either "Claim" or "Premise". You must return a list of argument component types, strictly of length 8, in following JSON format: {"component_types": ["component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)"]} where each element "component_type (str)" is replaced by either "Claim" or "Premise". 

### Here is the abstract text:  Single-agent therapy with bicalutamide, a nonsteroidal antiandrogen, was compared with castration, either surgical or medical, in patients with untreated Stage D2 prostate cancer. In an open, randomized, multicenter trial, patients were randomized to treatment with 50 mg bicalutamide (n = 243) once daily or to castration (n = 243), either orchiectomy or depot injection of goserelin acetate every 28 days. Primary efficacy endpoints were times to treatment failure and objective disease progression and survival. Assessments included review of measurable metastases, prostate dimensions, Eastern Cooperative Oncology Group performance status, pain, analgesic requirements, and quality of life responses. The median duration of therapy was 39 weeks for bicalutamide-treated patients and 42 weeks for castrated patients; treatment failure occurred in 53% and 42% and disease progression in 43% and 33%, respectively.<AC1> Treatment effects favored castration for both endpoints (P < or = 0.002), with hazard ratios (bicalutamide:castration) of 1.54 (95% confidence interval [CI], 1.18 to 2.00) for time to treatment failure and 1.6 (95% CI, 1.19 to 2.15) for time to disease progression. </AC1><AC2> From the 1-year survival analysis, the hazard ratio for probability of death was 1.29 (95% CI, 0.96 to 1.72). </AC2><AC3> Thus far, with a median follow-up of 86 weeks, median survival has not been reached in either group. </AC3><AC4> Changes from baseline in several quality of life variables were significantly different (P < or = 0.01) between treatment groups periodically from months 1 to 6, and all favored bicalutamide. </AC4><AC5> Overall, the antiandrogen was well tolerated compared with castration; </AC5><AC6> with bicalutamide, hot flushes occurred less often and breast tenderness and gynecomastia more often. </AC6><AC7> Although a dosage of 50 mg of bicalutamide once daily was not as effective as castration, </AC7><AC8> the favorable quality of life outcomes and the low incidence of nonhormonal adverse events provide reasons to evaluate bicalutamide, as a single therapeutic agent, at higher doses. </AC8><|end|> 
<|assistant|> {"component_types": ["Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Claim"]}<|end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 8853, 9700, 29918, 8768, 1115, 6796, 29925, 1745, 895, 613, 376, 29925, 1745, 895, 613, 376, 29925, 1745, 895, 613, 376, 29925, 1745, 895, 613, 376, 29925, 1745, 895, 613, 376, 29925, 1745, 895, 613, 376, 29925, 1745, 895, 613, 376, 29907, 8342, 3108, 29913, 32007]
labels:
{"component_types": ["Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Claim"]}<|end|>
09/11/2024 23:43:08 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/11/2024 23:43:08 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/11/2024 23:43:08 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/11/2024 23:43:08 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/11/2024 23:43:12 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/11/2024 23:43:12 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/11/2024 23:43:12 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/11/2024 23:43:12 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/11/2024 23:43:12 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,down_proj,up_proj,v_proj,gate_proj,o_proj,k_proj
09/11/2024 23:43:12 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/11/2024 23:43:12 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/11/2024 23:43:12 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/11/2024 23:43:12 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/11/2024 23:43:12 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,up_proj,o_proj,q_proj,k_proj,down_proj,gate_proj
09/11/2024 23:43:12 - INFO - llamafactory.model.loader - trainable params: 14,942,208 || all params: 3,836,021,760 || trainable%: 0.3895
09/11/2024 23:43:12 - INFO - llamafactory.model.loader - trainable params: 14,942,208 || all params: 3,836,021,760 || trainable%: 0.3895
09/11/2024 23:43:13 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/11/2024 23:43:13 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.1519, 'grad_norm': 0.17646777629852295, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.45}
{'loss': 0.0681, 'grad_norm': 0.08213717490434647, 'learning_rate': 4.898732434036244e-05, 'epoch': 0.91}
{'loss': 0.0472, 'grad_norm': 0.1306925266981125, 'learning_rate': 4.559191453574582e-05, 'epoch': 1.36}
{'loss': 0.0327, 'grad_norm': 0.06693117320537567, 'learning_rate': 4.014024217844167e-05, 'epoch': 1.82}
{'loss': 0.0277, 'grad_norm': 0.09046623110771179, 'learning_rate': 3.3176699082935545e-05, 'epoch': 2.27}
{'loss': 0.0209, 'grad_norm': 0.06187070533633232, 'learning_rate': 2.5396649095870202e-05, 'epoch': 2.73}
{'loss': 0.0162, 'grad_norm': 0.05393889546394348, 'learning_rate': 1.7576990616793137e-05, 'epoch': 3.18}
{'loss': 0.0143, 'grad_norm': 0.08517742156982422, 'learning_rate': 1.049857726072005e-05, 'epoch': 3.64}
{'loss': 0.0132, 'grad_norm': 0.02651398815214634, 'learning_rate': 4.868243561723535e-06, 'epoch': 4.09}
{'loss': 0.0106, 'grad_norm': 0.08070164173841476, 'learning_rate': 1.248222056476367e-06, 'epoch': 4.55}
{'loss': 0.0117, 'grad_norm': 0.07181530445814133, 'learning_rate': 0.0, 'epoch': 5.0}
{'train_runtime': 445.0686, 'train_samples_per_second': 3.932, 'train_steps_per_second': 0.247, 'train_loss': 0.037694615057923576, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  total_flos               = 31044172GF
  train_loss               =     0.0377
  train_runtime            = 0:07:25.06
  train_samples_per_second =      3.932
  train_steps_per_second   =      0.247
09/11/2024 23:50:44 - INFO - llamafactory.data.template - Replace eos token: <|end|>
09/11/2024 23:50:44 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.
09/11/2024 23:50:44 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/11/2024 23:50:44 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/11/2024 23:50:44 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/11/2024 23:50:46 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/11/2024 23:50:47 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/finetuned_models_run3/abstRCT_acc_Phi-3-mini-4k-instruct-bnb-4bit
09/11/2024 23:50:47 - INFO - llamafactory.model.loader - all params: 3,836,021,760
              precision    recall  f1-score   support

       Claim      0.929     0.899     0.914       248
     Premise      0.945     0.962     0.953       443

    accuracy                          0.939       691
   macro avg      0.937     0.930     0.933       691
weighted avg      0.939     0.939     0.939       691

              precision    recall  f1-score   support

       Claim      0.888     0.874     0.881       191
     Premise      0.944     0.950     0.947       424

    accuracy                          0.927       615
   macro avg      0.916     0.912     0.914       615
weighted avg      0.927     0.927     0.927       615

              precision    recall  f1-score   support

       Claim      0.921     0.939     0.930       212
     Premise      0.967     0.957     0.962       397

    accuracy                          0.951       609
   macro avg      0.944     0.948     0.946       609
weighted avg      0.951     0.951     0.951       609

Successfully ran abstRCT_finetune.py with arguments: unsloth/Phi-3-mini-4k-instruct-bnb-4bit acc 
 
  *************** 

Running abstRCT_finetune.py with arguments: unsloth/Phi-3-mini-4k-instruct-bnb-4bit aric
09/12/2024 00:06:39 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:25588
09/12/2024 00:06:49 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/12/2024 00:06:49 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/12/2024 00:06:49 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/12/2024 00:06:49 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/12/2024 00:06:49 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/12/2024 00:06:49 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/12/2024 00:06:49 - INFO - llamafactory.data.template - Replace eos token: <|end|>
09/12/2024 00:06:49 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.
09/12/2024 00:06:49 - INFO - llamafactory.data.template - Replace eos token: <|end|>
09/12/2024 00:06:49 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.
09/12/2024 00:06:49 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_aric_train_neo.json...
09/12/2024 00:06:55 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_aric_train_neo.json...
training example:
input_ids:
[1, 32010, 835, 887, 526, 385, 17924, 297, 23125, 341, 2827, 29889, 887, 526, 2183, 263, 4768, 27067, 936, 9846, 1426, 607, 3743, 1353, 287, 2980, 7117, 427, 15603, 491, 529, 2477, 2565, 2477, 29958, 8282, 29889, 3575, 3414, 338, 304, 12439, 2980, 5302, 1546, 2980, 7117, 297, 278, 9846, 1426, 322, 770, 1598, 1009, 8220, 1134, 408, 2845, 376, 5924, 29908, 470, 376, 1131, 547, 1642, 887, 1818, 736, 263, 1051, 310, 21954, 1372, 297, 278, 1494, 4663, 3402, 29901, 8853, 1761, 29918, 23516, 29918, 23445, 29918, 8768, 1115, 5519, 4993, 14614, 313, 524, 511, 3646, 14614, 313, 524, 511, 8220, 29918, 1853, 313, 710, 29897, 1402, 2023, 29892, 518, 4993, 14614, 313, 524, 511, 3646, 14614, 313, 524, 511, 8220, 29918, 1853, 313, 710, 4638, 12258, 988, 1269, 1543, 376, 23445, 29918, 1853, 313, 710, 5513, 338, 8611, 491, 2845, 376, 5924, 29908, 470, 376, 1131, 547, 1642, 13, 13, 2277, 29937, 2266, 338, 278, 9846, 1426, 29901, 29871, 16740, 29899, 14748, 29220, 27580, 411, 289, 936, 329, 314, 680, 29892, 263, 1661, 303, 1489, 23670, 9418, 14907, 1885, 29892, 471, 9401, 411, 3209, 509, 362, 29892, 2845, 25300, 936, 470, 16083, 29892, 297, 22069, 411, 443, 2484, 630, 24906, 360, 29906, 410, 3859, 23900, 29889, 512, 385, 1722, 29892, 4036, 1891, 29892, 1773, 293, 5893, 14260, 29892, 22069, 892, 4036, 1891, 304, 14502, 411, 29871, 29945, 29900, 286, 29887, 289, 936, 329, 314, 680, 313, 29876, 353, 29871, 29906, 29946, 29941, 29897, 2748, 14218, 470, 304, 3209, 509, 362, 313, 29876, 353, 29871, 29906, 29946, 29941, 511, 2845, 22624, 347, 312, 16103, 470, 1401, 327, 20859, 310, 330, 22969, 24446, 1274, 300, 403, 1432, 29871, 29906, 29947, 3841, 29889, 28267, 6366, 4135, 1095, 9748, 892, 3064, 304, 14502, 10672, 322, 12091, 17135, 410, 11476, 322, 10503, 2561, 29889, 4007, 404, 1860, 5134, 9076, 310, 7540, 21115, 1539, 579, 2129, 29892, 410, 3859, 13391, 29892, 16162, 21851, 1230, 1551, 1054, 6933, 6431, 4180, 4660, 29892, 6788, 29892, 3483, 2710, 293, 11780, 29892, 322, 11029, 310, 2834, 20890, 29889, 450, 19194, 14385, 310, 29220, 27580, 471, 29871, 29941, 29929, 11405, 363, 289, 936, 329, 314, 680, 29899, 2484, 630, 22069, 322, 29871, 29946, 29906, 11405, 363, 3209, 509, 630, 22069, 29936, 14502, 10672, 10761, 297, 29871, 29945, 29941, 29995, 322, 29871, 29946, 29906, 29995, 322, 17135, 410, 11476, 297, 29871, 29946, 29941, 29995, 322, 29871, 29941, 29941, 13667, 8307, 19423, 2477, 29896, 29958, 6479, 271, 358, 9545, 5025, 4395, 3209, 509, 362, 363, 1716, 1095, 9748, 313, 29925, 529, 470, 353, 29871, 29900, 29889, 29900, 29900, 29906, 511, 411, 447, 29920, 538, 364, 2219, 359, 313, 29890, 936, 329, 314, 680, 29901, 29883, 7614, 362, 29897, 310, 29871, 29896, 29889, 29945, 29946, 313, 29929, 29945, 29995, 16420, 7292, 518, 8426, 1402, 29871, 29896, 29889, 29896, 29947, 304, 29871, 29906, 29889, 29900, 29900, 29897, 363, 931, 304, 14502, 10672, 322, 29871, 29896, 29889, 29953, 313, 29929, 29945, 29995, 25781, 29892, 29871, 29896, 29889, 29896, 29929, 304, 29871, 29906, 29889, 29896, 29945, 29897, 363, 931, 304, 17135, 410, 11476, 29889, 1533, 2477, 29896, 5299, 2477, 29906, 29958, 3645, 278, 29871, 29896, 29899, 6360, 10503, 2561, 7418, 29892, 278, 447, 29920, 538, 11959, 363, 6976, 310, 4892, 471, 29871, 29896, 29889, 29906, 29929, 313, 29929, 29945, 29995, 25781, 29892, 29871, 29900, 29889, 29929, 29953, 304, 29871, 29896, 29889, 29955, 29906, 467, 1533, 2477, 29906, 5299, 2477, 29941, 29958, 6549, 2215, 29892, 411, 263, 19194, 1101, 29899, 786, 310, 29871, 29947, 29953, 11405, 29892, 19194, 10503, 2561, 756, 451, 1063, 7450, 297, 2845, 2318, 29889, 1533, 2477, 29941, 5299, 2477, 29946, 29958, 678, 6916, 515, 2362, 5570, 297, 3196, 11029, 310, 2834, 3651, 892, 16951, 1422, 313, 29925, 529, 470, 353, 29871, 29900, 29889, 29900, 29896, 29897, 1546, 14502, 6471, 3785, 1711, 515, 7378, 29871, 29896, 304, 29871, 29953, 29892, 322, 599, 5025, 4395, 289, 936, 329, 314, 680, 29889, 1533, 2477, 29946, 5299, 2477, 29945, 29958, 6811, 497, 29892, 278, 9418, 14907, 1885, 471, 1532, 20341, 630, 9401, 411, 3209, 509, 362, 29936, 1533, 2477, 29945, 5299, 2477, 29953, 29958, 411, 289, 936, 329, 314, 680, 29892, 7375, 28371, 267, 10761, 3109, 4049, 322, 24207, 10331, 824, 404, 322, 10966, 484, 510, 579, 423, 901, 4049, 29889, 1533, 2477, 29953, 5299, 2477, 29955, 29958, 8512, 263, 3248, 482, 310, 29871, 29945, 29900, 286, 29887, 310, 289, 936, 329, 314, 680, 2748, 14218, 471, 451, 408, 11828, 408, 3209, 509, 362, 29892, 1533, 2477, 29955, 5299, 2477, 29947, 29958, 278, 7853, 519, 11029, 310, 2834, 714, 26807, 322, 278, 4482, 5528, 5084, 310, 1661, 29882, 555, 7177, 594, 3901, 4959, 3867, 9590, 304, 14707, 289, 936, 329, 314, 680, 29892, 408, 263, 2323, 266, 1572, 412, 329, 293, 10823, 29892, 472, 6133, 3248, 267, 29889, 1533, 2477, 29947, 29958, 32007, 29871, 13, 32001, 8853, 1761, 29918, 23516, 29918, 23445, 29918, 8768, 1115, 5519, 29896, 29892, 29871, 29947, 29892, 376, 1131, 547, 12436, 518, 29906, 29892, 29871, 29947, 29892, 376, 5924, 12436, 518, 29941, 29892, 29871, 29906, 29892, 376, 1131, 547, 12436, 518, 29946, 29892, 29871, 29947, 29892, 376, 5924, 12436, 518, 29945, 29892, 29871, 29947, 29892, 376, 5924, 12436, 518, 29953, 29892, 29871, 29947, 29892, 376, 5924, 12436, 518, 29955, 29892, 29871, 29947, 29892, 376, 1131, 547, 3108, 12258, 32007]
inputs:
<s><|user|> ### You are an expert in Argument Mining. You are given a biomedical abstract text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to identify argument relations between argument components in the abstract text and classify their relation type as either "support" or "attack". You must return a list of triplets in the following JSON format: {"list_argument_relation_types": [[source AC (int), target AC (int), relation_type (str)], ..., [source AC (int), target AC (int), relation_type (str)]]} where each element "relation_type (str)" is replaced by either "support" or "attack".

### Here is the abstract text:  Single-agent therapy with bicalutamide, a nonsteroidal antiandrogen, was compared with castration, either surgical or medical, in patients with untreated Stage D2 prostate cancer. In an open, randomized, multicenter trial, patients were randomized to treatment with 50 mg bicalutamide (n = 243) once daily or to castration (n = 243), either orchiectomy or depot injection of goserelin acetate every 28 days. Primary efficacy endpoints were times to treatment failure and objective disease progression and survival. Assessments included review of measurable metastases, prostate dimensions, Eastern Cooperative Oncology Group performance status, pain, analgesic requirements, and quality of life responses. The median duration of therapy was 39 weeks for bicalutamide-treated patients and 42 weeks for castrated patients; treatment failure occurred in 53% and 42% and disease progression in 43% and 33%, respectively.<AC1> Treatment effects favored castration for both endpoints (P < or = 0.002), with hazard ratios (bicalutamide:castration) of 1.54 (95% confidence interval [CI], 1.18 to 2.00) for time to treatment failure and 1.6 (95% CI, 1.19 to 2.15) for time to disease progression. </AC1><AC2> From the 1-year survival analysis, the hazard ratio for probability of death was 1.29 (95% CI, 0.96 to 1.72). </AC2><AC3> Thus far, with a median follow-up of 86 weeks, median survival has not been reached in either group. </AC3><AC4> Changes from baseline in several quality of life variables were significantly different (P < or = 0.01) between treatment groups periodically from months 1 to 6, and all favored bicalutamide. </AC4><AC5> Overall, the antiandrogen was well tolerated compared with castration; </AC5><AC6> with bicalutamide, hot flushes occurred less often and breast tenderness and gynecomastia more often. </AC6><AC7> Although a dosage of 50 mg of bicalutamide once daily was not as effective as castration, </AC7><AC8> the favorable quality of life outcomes and the low incidence of nonhormonal adverse events provide reasons to evaluate bicalutamide, as a single therapeutic agent, at higher doses. </AC8><|end|> 
<|assistant|> {"list_argument_relation_types": [[1, 8, "attack"], [2, 8, "support"], [3, 2, "attack"], [4, 8, "support"], [5, 8, "support"], [6, 8, "support"], [7, 8, "attack"]]}<|end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 8853, 1761, 29918, 23516, 29918, 23445, 29918, 8768, 1115, 5519, 29896, 29892, 29871, 29947, 29892, 376, 1131, 547, 12436, 518, 29906, 29892, 29871, 29947, 29892, 376, 5924, 12436, 518, 29941, 29892, 29871, 29906, 29892, 376, 1131, 547, 12436, 518, 29946, 29892, 29871, 29947, 29892, 376, 5924, 12436, 518, 29945, 29892, 29871, 29947, 29892, 376, 5924, 12436, 518, 29953, 29892, 29871, 29947, 29892, 376, 5924, 12436, 518, 29955, 29892, 29871, 29947, 29892, 376, 1131, 547, 3108, 12258, 32007]
labels:
{"list_argument_relation_types": [[1, 8, "attack"], [2, 8, "support"], [3, 2, "attack"], [4, 8, "support"], [5, 8, "support"], [6, 8, "support"], [7, 8, "attack"]]}<|end|>
09/12/2024 00:06:56 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/12/2024 00:06:56 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/12/2024 00:06:56 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/12/2024 00:06:56 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/12/2024 00:07:00 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/12/2024 00:07:00 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/12/2024 00:07:00 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/12/2024 00:07:00 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/12/2024 00:07:00 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,down_proj,up_proj,o_proj,k_proj,gate_proj,v_proj
09/12/2024 00:07:00 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/12/2024 00:07:00 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/12/2024 00:07:00 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/12/2024 00:07:00 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/12/2024 00:07:00 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,up_proj,q_proj,k_proj,gate_proj,down_proj,o_proj
09/12/2024 00:07:00 - INFO - llamafactory.model.loader - trainable params: 14,942,208 || all params: 3,836,021,760 || trainable%: 0.3895
09/12/2024 00:07:00 - INFO - llamafactory.model.loader - trainable params: 14,942,208 || all params: 3,836,021,760 || trainable%: 0.3895
09/12/2024 00:07:01 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/12/2024 00:07:01 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.3158, 'grad_norm': 0.3748663663864136, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.45}
{'loss': 0.2002, 'grad_norm': 0.14938023686408997, 'learning_rate': 4.898732434036244e-05, 'epoch': 0.91}
{'loss': 0.141, 'grad_norm': 0.157593235373497, 'learning_rate': 4.559191453574582e-05, 'epoch': 1.36}
{'loss': 0.138, 'grad_norm': 0.19552718102931976, 'learning_rate': 4.014024217844167e-05, 'epoch': 1.82}
{'loss': 0.1138, 'grad_norm': 0.12923233211040497, 'learning_rate': 3.3176699082935545e-05, 'epoch': 2.27}
{'loss': 0.0984, 'grad_norm': 0.18281883001327515, 'learning_rate': 2.5396649095870202e-05, 'epoch': 2.73}
{'loss': 0.084, 'grad_norm': 0.19651536643505096, 'learning_rate': 1.7576990616793137e-05, 'epoch': 3.18}
{'loss': 0.0698, 'grad_norm': 0.1396339386701584, 'learning_rate': 1.049857726072005e-05, 'epoch': 3.64}
{'loss': 0.0682, 'grad_norm': 0.13446038961410522, 'learning_rate': 4.868243561723535e-06, 'epoch': 4.09}
{'loss': 0.0497, 'grad_norm': 0.17806868255138397, 'learning_rate': 1.248222056476367e-06, 'epoch': 4.55}
{'loss': 0.0591, 'grad_norm': 0.18445797264575958, 'learning_rate': 0.0, 'epoch': 5.0}
{'train_runtime': 441.6995, 'train_samples_per_second': 3.962, 'train_steps_per_second': 0.249, 'train_loss': 0.12163961638103832, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  total_flos               = 30881771GF
  train_loss               =     0.1216
  train_runtime            = 0:07:21.69
  train_samples_per_second =      3.962
  train_steps_per_second   =      0.249
09/12/2024 00:14:31 - INFO - llamafactory.data.template - Replace eos token: <|end|>
09/12/2024 00:14:31 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.
09/12/2024 00:14:31 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/12/2024 00:14:31 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/12/2024 00:14:31 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/12/2024 00:14:34 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/12/2024 00:14:34 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/finetuned_models_run3/abstRCT_aric_Phi-3-mini-4k-instruct-bnb-4bit
09/12/2024 00:14:34 - INFO - llamafactory.model.loader - all params: 3,836,021,760
              precision    recall  f1-score   support

        None      0.980     0.970     0.975      6435
      attack      0.537     0.492     0.513        59
     support      0.592     0.706     0.644       360

    accuracy                          0.952      6854
   macro avg      0.703     0.722     0.711      6854
weighted avg      0.956     0.952     0.954      6854

              precision    recall  f1-score   support

        None      0.985     0.966     0.975      5219
      attack      0.333     0.321     0.327        28
     support      0.604     0.795     0.687       317

    accuracy                          0.953      5564
   macro avg      0.641     0.694     0.663      5564
weighted avg      0.960     0.953     0.956      5564

              precision    recall  f1-score   support

        None      0.985     0.967     0.976      5176
      attack      0.346     0.375     0.360        24
     support      0.589     0.770     0.668       296

    accuracy                          0.954      5496
   macro avg      0.640     0.704     0.668      5496
weighted avg      0.961     0.954     0.956      5496

Successfully ran abstRCT_finetune.py with arguments: unsloth/Phi-3-mini-4k-instruct-bnb-4bit aric 
 
  *************** 

