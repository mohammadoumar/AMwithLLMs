{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671272c9-0c9d-4265-a75b-480471345729",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1165cbdc-bb7a-48d3-b17d-442687c00173",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3de7add-5182-4d9f-af93-5446726c9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e11ea0-ff3f-44eb-9577-5eedba91e3cc",
   "metadata": {},
   "source": [
    "## Run notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d31e870a-6452-42fb-9206-19e4e4da5e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\", \n",
    "    \"unsloth/llama-3-8b-Instruct\", \n",
    "    # \"unsloth/llama-3-70b-Instruct-bnb-4bit\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add117f7-cd0b-45d1-8ace-69f45d391305",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\n",
    "    [\"PE_LTC_paragraph_train.json\", \"PE_LTC_paragraph_test.json\"],\n",
    "    [\"PE_LTC_paragraph_wo_tags_train.json\", \"PE_LTC_paragraph_wo_tags_test.json\"],\n",
    "    # [\"PE_LTC_essay_train.json\", \"PE_LTC_essay_test.json\"],\n",
    "    # [\"PE_LTC_essay_wo_tags_train.json\", \"PE_LTC_essay_wo_tags_test.json\"]    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a774f885-66d0-45bd-a3db-d72789babe3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment ['unsloth/llama-3-8b-Instruct-bnb-4bit', 'PE_LTC_paragraph_train.json', 'PE_LTC_paragraph_test.json']\n",
      "/Utilisateurs/umushtaq/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/llama-factory-notebook/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13/2024 13:19:32 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "06/13/2024 13:19:32 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 13:19:32,246 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 13:19:32,246 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 13:19:32,246 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 13:19:32,247 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-06-13 13:19:32,500 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "06/13/2024 13:19:32 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "06/13/2024 13:19:32 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/datasets/PE_LTC_paragraph_train.json...\n",
      "input_ids:\n",
      "[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 11190, 18328, 13, 128009, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 14646, 902, 5727, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 1472, 527, 1101, 2728, 264, 1160, 315, 13840, 315, 5552, 5811, 6956, 304, 279, 1376, 25, 18305, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 5850, 320, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 5850, 61453, 320, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 595, 948, 4718, 3465, 374, 311, 49229, 1855, 6857, 315, 5552, 5811, 6956, 304, 279, 1160, 439, 3060, 330, 8075, 1, 477, 330, 29702, 3343, 1472, 2011, 471, 264, 1160, 315, 12976, 4595, 304, 2768, 4823, 3645, 25, 5324, 23013, 9962, 794, 510, 23013, 1857, 320, 496, 705, 12976, 1857, 320, 496, 705, 61453, 12976, 1857, 320, 496, 7400, 633, 14711, 5810, 374, 279, 14646, 1495, 25, 366, 16816, 29, 12540, 4236, 387, 15972, 311, 20874, 477, 311, 47903, 949, 694, 16816, 397, 14711, 8586, 374, 279, 1160, 315, 13840, 315, 5552, 5811, 6956, 304, 420, 14646, 25, 3132, 128009, 128006, 78191, 128007, 271, 5018, 23013, 9962, 794, 3132, 92, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "### You are an expert in Argument Mining. You are given a paragraph which contains argument components enclosed by <AC></AC> tags. You are also given a list of pairs of related argument components in the form: [(target AC (int), source AC (int)), (target AC (int), source AC (int)),..., (target AC (int), source AC (int))]. Your task is to classify each pair of related argument components in the list as either \"Support\" or \"Attack\". You must return a list of relation types in following JSON format: {\"relation_types\": [relation_type (str), relation_type (str),..., relation_type (str)]}\n",
      "\n",
      "### Here is the paragraph text: <topic> Should students be taught to compete or to cooperate? </topic>\n",
      "###Here is the list of pairs of related argument components in this paragraph: []<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\"relation_types\": []}<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 23013, 9962, 794, 3132, 92, 128009]\n",
      "labels:\n",
      "{\"relation_types\": []}<|eot_id|>\n",
      "/Utilisateurs/umushtaq/.conda/envs/llama-factory-notebook/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-13 13:19:33,079 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-13 13:19:33,080 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "06/13/2024 13:19:33 - INFO - llamafactory.model.utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "[WARNING|quantization_config.py:393] 2024-06-13 13:19:33,143 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:3474] 2024-06-13 13:19:33,145 >> loading weights file model.safetensors from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/model.safetensors\n",
      "[INFO|modeling_utils.py:1519] 2024-06-13 13:19:33,175 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:962] 2024-06-13 13:19:33,178 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4280] 2024-06-13 13:19:36,535 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-06-13 13:19:36,536 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-06-13 13:19:36,642 >> loading configuration file generation_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-06-13 13:19:36,642 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "06/13/2024 13:19:36 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\n",
      "06/13/2024 13:19:36 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "06/13/2024 13:19:36 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "06/13/2024 13:19:36 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "06/13/2024 13:19:36 - INFO - llamafactory.model.utils.misc - Found linear modules: q_proj,v_proj,k_proj,down_proj,gate_proj,up_proj,o_proj\n",
      "06/13/2024 13:19:37 - INFO - llamafactory.model.loader - trainable params: 20971520 || all params: 8051232768 || trainable%: 0.2605\n",
      "[INFO|trainer.py:641] 2024-06-13 13:19:37,103 >> Using auto half precision backend\n",
      "06/13/2024 13:19:37 - INFO - llamafactory.train.utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
      "[INFO|trainer.py:2078] 2024-06-13 13:19:37,315 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-06-13 13:19:37,315 >>   Num examples = 1,796\n",
      "[INFO|trainer.py:2080] 2024-06-13 13:19:37,315 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:2081] 2024-06-13 13:19:37,315 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2084] 2024-06-13 13:19:37,315 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2085] 2024-06-13 13:19:37,315 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2086] 2024-06-13 13:19:37,315 >>   Total optimization steps = 2,240\n",
      "[INFO|trainer.py:2087] 2024-06-13 13:19:37,319 >>   Number of trainable parameters = 20,971,520\n",
      "{'loss': 0.727, 'grad_norm': 2.378363609313965, 'learning_rate': 2.0089285714285715e-06, 'epoch': 0.04}\n",
      "{'loss': 0.0428, 'grad_norm': 3.187858819961548, 'learning_rate': 4.241071428571429e-06, 'epoch': 0.09}\n",
      "{'loss': 0.0203, 'grad_norm': 0.9291737079620361, 'learning_rate': 6.473214285714287e-06, 'epoch': 0.13}\n",
      "{'loss': 0.0258, 'grad_norm': 0.3008441925048828, 'learning_rate': 8.705357142857143e-06, 'epoch': 0.18}\n",
      "{'loss': 0.0373, 'grad_norm': 0.7530893683433533, 'learning_rate': 1.09375e-05, 'epoch': 0.22}\n",
      "{'loss': 0.013, 'grad_norm': 0.020105043426156044, 'learning_rate': 1.3169642857142858e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0295, 'grad_norm': 0.23436348140239716, 'learning_rate': 1.5401785714285715e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0156, 'grad_norm': 0.02863682620227337, 'learning_rate': 1.7633928571428573e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0201, 'grad_norm': 0.3596629500389099, 'learning_rate': 1.9866071428571427e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0288, 'grad_norm': 0.39835286140441895, 'learning_rate': 2.2098214285714286e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0213, 'grad_norm': 1.54639732837677, 'learning_rate': 2.4330357142857144e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0204, 'grad_norm': 0.026316538453102112, 'learning_rate': 2.6562500000000002e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0167, 'grad_norm': 0.008099937811493874, 'learning_rate': 2.8794642857142857e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0077, 'grad_norm': 0.0017685280181467533, 'learning_rate': 3.102678571428572e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0016, 'grad_norm': 0.00032494249171577394, 'learning_rate': 3.325892857142857e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0122, 'grad_norm': 0.05857117474079132, 'learning_rate': 3.5491071428571435e-05, 'epoch': 0.71}\n",
      "{'loss': 0.016, 'grad_norm': 0.6732850074768066, 'learning_rate': 3.7723214285714286e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0151, 'grad_norm': 0.018794655799865723, 'learning_rate': 3.9955357142857144e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0156, 'grad_norm': 0.004907244350761175, 'learning_rate': 4.21875e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0248, 'grad_norm': 0.8715934753417969, 'learning_rate': 4.4419642857142854e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0177, 'grad_norm': 1.0861707925796509, 'learning_rate': 4.665178571428572e-05, 'epoch': 0.94}\n",
      "{'loss': 0.011, 'grad_norm': 0.14519202709197998, 'learning_rate': 4.888392857142857e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0392, 'grad_norm': 8.395116806030273, 'learning_rate': 4.9999241131520337e-05, 'epoch': 1.02}\n",
      "{'loss': 0.0029, 'grad_norm': 0.00032911464222706854, 'learning_rate': 4.999317046010329e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0549, 'grad_norm': 0.0008860756061039865, 'learning_rate': 4.998103059143599e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0149, 'grad_norm': 0.6101771593093872, 'learning_rate': 4.996282447349408e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0242, 'grad_norm': 0.0030298884958028793, 'learning_rate': 4.9938556527346155e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0414, 'grad_norm': 1.0163564682006836, 'learning_rate': 4.9908232646080166e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0093, 'grad_norm': 0.007668053265661001, 'learning_rate': 4.987186019337242e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0155, 'grad_norm': 1.7539904117584229, 'learning_rate': 4.9829448001699384e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0119, 'grad_norm': 0.0011279713362455368, 'learning_rate': 4.9781006370192876e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0282, 'grad_norm': 1.2441613674163818, 'learning_rate': 4.972654706213906e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0252, 'grad_norm': 0.7562580108642578, 'learning_rate': 4.967239947141803e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0706, 'grad_norm': 0.880180299282074, 'learning_rate': 4.960654421683386e-05, 'epoch': 1.51}\n",
      "{'loss': 0.038, 'grad_norm': 4.693727016448975, 'learning_rate': 4.9534713651084696e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0405, 'grad_norm': 0.019823916256427765, 'learning_rate': 4.94569252170905e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0205, 'grad_norm': 0.0014164707390591502, 'learning_rate': 4.937319780454559e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0565, 'grad_norm': 4.1404008865356445, 'learning_rate': 4.9283551745331534e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0168, 'grad_norm': 0.015623140148818493, 'learning_rate': 4.9188008808579914e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0155, 'grad_norm': 0.8666803240776062, 'learning_rate': 4.9086592195385974e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0369, 'grad_norm': 1.25254225730896, 'learning_rate': 4.89793265331747e-05, 'epoch': 1.83}\n",
      "{'loss': 0.0199, 'grad_norm': 0.6281648278236389, 'learning_rate': 4.886623786972033e-05, 'epoch': 1.87}\n",
      "{'loss': 0.0232, 'grad_norm': 1.2124433517456055, 'learning_rate': 4.874735366682115e-05, 'epoch': 1.92}\n",
      "{'loss': 0.0338, 'grad_norm': 0.013545002788305283, 'learning_rate': 4.8622702793630756e-05, 'epoch': 1.96}\n",
      "{'loss': 0.0184, 'grad_norm': 0.07465123385190964, 'learning_rate': 4.849231551964771e-05, 'epoch': 2.0}\n",
      "{'loss': 0.003, 'grad_norm': 0.018076397478580475, 'learning_rate': 4.8356223507364996e-05, 'epoch': 2.05}\n",
      "{'loss': 0.0086, 'grad_norm': 4.252668857574463, 'learning_rate': 4.821445980458134e-05, 'epoch': 2.09}\n",
      "{'loss': 0.0122, 'grad_norm': 0.07151485234498978, 'learning_rate': 4.8067058836376044e-05, 'epoch': 2.14}\n",
      "{'loss': 0.0234, 'grad_norm': 0.030489444732666016, 'learning_rate': 4.791405639674941e-05, 'epoch': 2.18}\n",
      "{'loss': 0.0127, 'grad_norm': 0.16144506633281708, 'learning_rate': 4.775548963993072e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0023, 'grad_norm': 0.023681312799453735, 'learning_rate': 4.759139707135592e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0007, 'grad_norm': 0.01578778214752674, 'learning_rate': 4.742181853831721e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0053, 'grad_norm': 0.054852861911058426, 'learning_rate': 4.724679522028672e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0032, 'grad_norm': 0.059850264340639114, 'learning_rate': 4.706636961891673e-05, 'epoch': 2.41}\n",
      "{'loss': 0.0002, 'grad_norm': 0.05557547137141228, 'learning_rate': 4.6880585547718845e-05, 'epoch': 2.45}\n",
      "{'loss': 0.0332, 'grad_norm': 0.08682910352945328, 'learning_rate': 4.668948812142453e-05, 'epoch': 2.49}\n",
      "{'loss': 0.0144, 'grad_norm': 0.030641663819551468, 'learning_rate': 4.649312374502976e-05, 'epoch': 2.54}\n",
      "{'loss': 0.0203, 'grad_norm': 0.01625363714993, 'learning_rate': 4.6291540102526235e-05, 'epoch': 2.58}\n",
      "{'loss': 0.0097, 'grad_norm': 0.23666781187057495, 'learning_rate': 4.608478614532215e-05, 'epoch': 2.63}\n",
      "{'loss': 0.0077, 'grad_norm': 0.0028498326428234577, 'learning_rate': 4.587291208035503e-05, 'epoch': 2.67}\n",
      "{'loss': 0.0127, 'grad_norm': 0.11728382110595703, 'learning_rate': 4.5655969357899874e-05, 'epoch': 2.72}\n",
      "{'loss': 0.0109, 'grad_norm': 0.006238183472305536, 'learning_rate': 4.543401065907516e-05, 'epoch': 2.76}\n",
      "{'loss': 0.0312, 'grad_norm': 0.8232539296150208, 'learning_rate': 4.5207089883050136e-05, 'epoch': 2.81}\n",
      "{'loss': 0.0009, 'grad_norm': 0.04990231245756149, 'learning_rate': 4.497526213395623e-05, 'epoch': 2.85}\n",
      "{'loss': 0.0006, 'grad_norm': 0.002106110565364361, 'learning_rate': 4.4738583707505885e-05, 'epoch': 2.9}\n",
      "{'loss': 0.0146, 'grad_norm': 0.0005205651978030801, 'learning_rate': 4.4497112077322044e-05, 'epoch': 2.94}\n",
      "{'loss': 0.0397, 'grad_norm': 0.002910554176196456, 'learning_rate': 4.4250905880981574e-05, 'epoch': 2.98}\n",
      "{'loss': 0.0102, 'grad_norm': 0.8841804265975952, 'learning_rate': 4.400002490577604e-05, 'epoch': 3.03}\n",
      "{'loss': 0.0053, 'grad_norm': 0.025912726297974586, 'learning_rate': 4.374453007419336e-05, 'epoch': 3.07}\n",
      "{'loss': 0.0028, 'grad_norm': 1.4967877864837646, 'learning_rate': 4.3484483429123656e-05, 'epoch': 3.12}\n",
      "{'loss': 0.0081, 'grad_norm': 1.0568846464157104, 'learning_rate': 4.3219948118793214e-05, 'epoch': 3.16}\n",
      "{'loss': 0.0043, 'grad_norm': 0.006695837713778019, 'learning_rate': 4.295098838142985e-05, 'epoch': 3.21}\n",
      "{'loss': 0.0119, 'grad_norm': 0.4034629762172699, 'learning_rate': 4.267766952966369e-05, 'epoch': 3.25}\n",
      "{'loss': 0.0095, 'grad_norm': 0.0104521494358778, 'learning_rate': 4.240005793466709e-05, 'epoch': 3.3}\n",
      "{'loss': 0.0053, 'grad_norm': 0.31183964014053345, 'learning_rate': 4.211822101003734e-05, 'epoch': 3.34}\n",
      "{'loss': 0.0042, 'grad_norm': 0.0006760687683708966, 'learning_rate': 4.183222719542643e-05, 'epoch': 3.39}\n",
      "{'loss': 0.0019, 'grad_norm': 0.12912774085998535, 'learning_rate': 4.154214593992149e-05, 'epoch': 3.43}\n",
      "{'loss': 0.0009, 'grad_norm': 0.00019080238416790962, 'learning_rate': 4.1248047685180215e-05, 'epoch': 3.47}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0010419400641694665, 'learning_rate': 4.095000384832522e-05, 'epoch': 3.52}\n",
      "{'loss': 0.0001, 'grad_norm': 0.007493139710277319, 'learning_rate': 4.064808680460148e-05, 'epoch': 3.56}\n",
      "{'loss': 0.0, 'grad_norm': 0.0006202083895914257, 'learning_rate': 4.034236986980119e-05, 'epoch': 3.61}\n",
      "{'loss': 0.0, 'grad_norm': 0.0035787655506283045, 'learning_rate': 4.0032927282460146e-05, 'epoch': 3.65}\n",
      "{'loss': 0.0081, 'grad_norm': 1.7457326650619507, 'learning_rate': 3.9719834185830116e-05, 'epoch': 3.7}\n",
      "{'loss': 0.0046, 'grad_norm': 0.0010263712611049414, 'learning_rate': 3.940316660963147e-05, 'epoch': 3.74}\n",
      "{'loss': 0.0198, 'grad_norm': 0.8431785702705383, 'learning_rate': 3.908300145159055e-05, 'epoch': 3.79}\n",
      "{'loss': 0.0034, 'grad_norm': 0.004067154135555029, 'learning_rate': 3.875941645876631e-05, 'epoch': 3.83}\n",
      "{'loss': 0.001, 'grad_norm': 0.0006533855339512229, 'learning_rate': 3.84324902086706e-05, 'epoch': 3.88}\n",
      "{'loss': 0.0093, 'grad_norm': 0.0004913372686132789, 'learning_rate': 3.810230209018694e-05, 'epoch': 3.92}\n",
      "{'loss': 0.0074, 'grad_norm': 0.0007536353659816086, 'learning_rate': 3.7768932284292146e-05, 'epoch': 3.96}\n",
      "{'loss': 0.0115, 'grad_norm': 0.0006730808527208865, 'learning_rate': 3.74324617445856e-05, 'epoch': 4.01}\n",
      "{'loss': 0.0036, 'grad_norm': 0.618332028388977, 'learning_rate': 3.7092972177631e-05, 'epoch': 4.05}\n",
      "{'loss': 0.0016, 'grad_norm': 0.0636911690235138, 'learning_rate': 3.6750546023115216e-05, 'epoch': 4.1}\n",
      "{'loss': 0.0019, 'grad_norm': 0.003257478354498744, 'learning_rate': 3.6405266433829075e-05, 'epoch': 4.14}\n",
      "{'loss': 0.0008, 'grad_norm': 0.002997857518494129, 'learning_rate': 3.6057217255475034e-05, 'epoch': 4.19}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0005061613046564162, 'learning_rate': 3.570648300630657e-05, 'epoch': 4.23}\n",
      "{'loss': 0.0011, 'grad_norm': 0.0029778641182929277, 'learning_rate': 3.535314885660426e-05, 'epoch': 4.28}\n",
      "{'loss': 0.0007, 'grad_norm': 0.0008468187879770994, 'learning_rate': 3.499730060799352e-05, 'epoch': 4.32}\n",
      "{'loss': 0.0056, 'grad_norm': 4.953709602355957, 'learning_rate': 3.463902467260905e-05, 'epoch': 4.37}\n",
      "{'loss': 0.0001, 'grad_norm': 0.009027657099068165, 'learning_rate': 3.4278408052110946e-05, 'epoch': 4.41}\n",
      "{'loss': 0.0003, 'grad_norm': 0.0002581670123618096, 'learning_rate': 3.391553831655782e-05, 'epoch': 4.45}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0216047465801239, 'learning_rate': 3.355050358314172e-05, 'epoch': 4.5}\n",
      "{'loss': 0.0048, 'grad_norm': 0.04171307757496834, 'learning_rate': 3.318339249479026e-05, 'epoch': 4.54}\n",
      "{'loss': 0.0002, 'grad_norm': 0.012524142861366272, 'learning_rate': 3.281429419864112e-05, 'epoch': 4.59}\n",
      "{'loss': 0.0007, 'grad_norm': 0.0008170740911737084, 'learning_rate': 3.244329832439404e-05, 'epoch': 4.63}\n",
      "{'loss': 0.0043, 'grad_norm': 0.00041046118712984025, 'learning_rate': 3.207049496254569e-05, 'epoch': 4.68}\n",
      "{'loss': 0.0002, 'grad_norm': 0.00914215948432684, 'learning_rate': 3.1695974642512585e-05, 'epoch': 4.72}\n",
      "{'loss': 0.0026, 'grad_norm': 0.8522589802742004, 'learning_rate': 3.131982831064744e-05, 'epoch': 4.77}\n",
      "{'loss': 0.0064, 'grad_norm': 0.0005753826699219644, 'learning_rate': 3.094214730815433e-05, 'epoch': 4.81}\n",
      "{'loss': 0.0098, 'grad_norm': 0.004452112130820751, 'learning_rate': 3.056302334890786e-05, 'epoch': 4.86}\n",
      "{'loss': 0.0062, 'grad_norm': 0.01856052689254284, 'learning_rate': 3.0182548497181946e-05, 'epoch': 4.9}\n",
      "{'loss': 0.0044, 'grad_norm': 0.08566506206989288, 'learning_rate': 2.980081514529341e-05, 'epoch': 4.94}\n",
      "{'loss': 0.0006, 'grad_norm': 0.0022823819890618324, 'learning_rate': 2.9417915991166008e-05, 'epoch': 4.99}\n",
      "{'loss': 0.0029, 'grad_norm': 0.012023874558508396, 'learning_rate': 2.903394401582017e-05, 'epoch': 5.03}\n",
      "{'loss': 0.0003, 'grad_norm': 0.0006097538280300796, 'learning_rate': 2.8648992460794056e-05, 'epoch': 5.08}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0004846699011977762, 'learning_rate': 2.8263154805501297e-05, 'epoch': 5.12}\n",
      "{'loss': 0.0017, 'grad_norm': 0.008892251178622246, 'learning_rate': 2.787652474453097e-05, 'epoch': 5.17}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0007591836620122194, 'learning_rate': 2.748919616489542e-05, 'epoch': 5.21}\n",
      "{'loss': 0.0, 'grad_norm': 0.005427319090813398, 'learning_rate': 2.710126312323119e-05, 'epoch': 5.26}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0001795472635421902, 'learning_rate': 2.6712819822958917e-05, 'epoch': 5.3}\n",
      "{'loss': 0.0001, 'grad_norm': 7.182864646892995e-05, 'learning_rate': 2.632396059140749e-05, 'epoch': 5.35}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00019206797878723592, 'learning_rate': 2.593477985690815e-05, 'epoch': 5.39}\n",
      "{'loss': 0.0018, 'grad_norm': 0.00011775075836339965, 'learning_rate': 2.5545372125864032e-05, 'epoch': 5.43}\n",
      "{'loss': 0.0002, 'grad_norm': 0.006604189518839121, 'learning_rate': 2.515583195980084e-05, 'epoch': 5.48}\n",
      "{'loss': 0.0013, 'grad_norm': 0.00017205358017235994, 'learning_rate': 2.4766253952404024e-05, 'epoch': 5.52}\n",
      "{'loss': 0.0, 'grad_norm': 0.00014775345334783196, 'learning_rate': 2.4376732706548183e-05, 'epoch': 5.57}\n",
      "{'loss': 0.0006, 'grad_norm': 0.001705028349533677, 'learning_rate': 2.3987362811324298e-05, 'epoch': 5.61}\n",
      "{'loss': 0.0006, 'grad_norm': 0.5956684947013855, 'learning_rate': 2.3598238819070202e-05, 'epoch': 5.66}\n",
      "{'loss': 0.0, 'grad_norm': 0.00023457374481949955, 'learning_rate': 2.3209455222410122e-05, 'epoch': 5.7}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004527298442553729, 'learning_rate': 2.2821106431308544e-05, 'epoch': 5.75}\n",
      "{'loss': 0.0, 'grad_norm': 0.0005938918329775333, 'learning_rate': 2.2433286750144293e-05, 'epoch': 5.79}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001255664392374456, 'learning_rate': 2.204609035481018e-05, 'epoch': 5.84}\n",
      "{'loss': 0.0, 'grad_norm': 0.0010908701224252582, 'learning_rate': 2.1659611269843906e-05, 'epoch': 5.88}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001815229916246608, 'learning_rate': 2.1273943345595637e-05, 'epoch': 5.92}\n",
      "{'loss': 0.0, 'grad_norm': 0.0023727763909846544, 'learning_rate': 2.0889180235437976e-05, 'epoch': 5.97}\n",
      "{'loss': 0.0008, 'grad_norm': 5.4168416681932285e-05, 'learning_rate': 2.0505415373023684e-05, 'epoch': 6.01}\n",
      "{'loss': 0.0, 'grad_norm': 0.00702242273837328, 'learning_rate': 2.0122741949596797e-05, 'epoch': 6.06}\n",
      "{'loss': 0.0001, 'grad_norm': 0.004906489979475737, 'learning_rate': 1.9741252891362612e-05, 'epoch': 6.1}\n",
      "{'loss': 0.0, 'grad_norm': 0.0006852435762993991, 'learning_rate': 1.936104083692202e-05, 'epoch': 6.15}\n",
      "{'loss': 0.0, 'grad_norm': 4.8696314479457214e-05, 'learning_rate': 1.8982198114775682e-05, 'epoch': 6.19}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004765612829942256, 'learning_rate': 1.86048167209035e-05, 'epoch': 6.24}\n",
      "{'loss': 0.0, 'grad_norm': 0.0006366568268276751, 'learning_rate': 1.8228988296424877e-05, 'epoch': 6.28}\n",
      "{'loss': 0.0005, 'grad_norm': 0.00018628894758876413, 'learning_rate': 1.7854804105345062e-05, 'epoch': 6.33}\n",
      "{'loss': 0.0001, 'grad_norm': 8.681676990818232e-05, 'learning_rate': 1.7482355012393177e-05, 'epoch': 6.37}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003140113258268684, 'learning_rate': 1.711173146095712e-05, 'epoch': 6.41}\n",
      "{'loss': 0.0013, 'grad_norm': 0.00024145649513229728, 'learning_rate': 1.6743023451120832e-05, 'epoch': 6.46}\n",
      "{'loss': 0.0, 'grad_norm': 9.836271055974066e-05, 'learning_rate': 1.637632051780917e-05, 'epoch': 6.5}\n",
      "{'loss': 0.0, 'grad_norm': 7.879910117480904e-05, 'learning_rate': 1.6011711709045813e-05, 'epoch': 6.55}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002773040614556521, 'learning_rate': 1.5649285564329296e-05, 'epoch': 6.59}\n",
      "{'loss': 0.0, 'grad_norm': 4.556723797577433e-05, 'learning_rate': 1.5289130093132632e-05, 'epoch': 6.64}\n",
      "{'loss': 0.0, 'grad_norm': 5.685495852958411e-05, 'learning_rate': 1.4931332753531574e-05, 'epoch': 6.68}\n",
      "{'loss': 0.0007, 'grad_norm': 0.0013852656120434403, 'learning_rate': 1.4575980430966807e-05, 'epoch': 6.73}\n",
      "{'loss': 0.0, 'grad_norm': 0.00032234753598459065, 'learning_rate': 1.4223159417145179e-05, 'epoch': 6.77}\n",
      "{'loss': 0.0, 'grad_norm': 7.701858703512698e-05, 'learning_rate': 1.387295538908519e-05, 'epoch': 6.82}\n",
      "{'loss': 0.0008, 'grad_norm': 5.952161518507637e-05, 'learning_rate': 1.3525453388311554e-05, 'epoch': 6.86}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001464711967855692, 'learning_rate': 1.318073780020433e-05, 'epoch': 6.9}\n",
      "{'loss': 0.0, 'grad_norm': 5.735196828027256e-05, 'learning_rate': 1.2838892333507154e-05, 'epoch': 6.95}\n",
      "{'loss': 0.0083, 'grad_norm': 0.00027227369719184935, 'learning_rate': 1.2500000000000006e-05, 'epoch': 6.99}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0002234709681943059, 'learning_rate': 1.2164143094340993e-05, 'epoch': 7.04}\n",
      "{'loss': 0.0012, 'grad_norm': 0.00047460763016715646, 'learning_rate': 1.183140317408248e-05, 'epoch': 7.08}\n",
      "{'loss': 0.0, 'grad_norm': 0.0022619778756052256, 'learning_rate': 1.1501861039866094e-05, 'epoch': 7.13}\n",
      "{'loss': 0.0, 'grad_norm': 0.00022734449885319918, 'learning_rate': 1.1175596715801515e-05, 'epoch': 7.17}\n",
      "{'loss': 0.0, 'grad_norm': 0.0006289948360063136, 'learning_rate': 1.0852689430033972e-05, 'epoch': 7.22}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003994193975813687, 'learning_rate': 1.0533217595504858e-05, 'epoch': 7.26}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0020987701136618853, 'learning_rate': 1.0217258790910448e-05, 'epoch': 7.31}\n",
      "{'loss': 0.0, 'grad_norm': 0.0009373293723911047, 'learning_rate': 9.90488974186306e-06, 'epoch': 7.35}\n",
      "{'loss': 0.0, 'grad_norm': 6.17463156231679e-05, 'learning_rate': 9.596186302259563e-06, 'epoch': 7.39}\n",
      "{'loss': 0.0, 'grad_norm': 0.00016837714065331966, 'learning_rate': 9.291223435861318e-06, 'epoch': 7.44}\n",
      "{'loss': 0.0, 'grad_norm': 0.01960211992263794, 'learning_rate': 8.99007519809053e-06, 'epoch': 7.48}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00023873253667261451, 'learning_rate': 8.69281471804698e-06, 'epoch': 7.53}\n",
      "{'loss': 0.0, 'grad_norm': 0.0017910546157509089, 'learning_rate': 8.399514180749795e-06, 'epoch': 7.57}\n",
      "{'loss': 0.0, 'grad_norm': 5.682682240149006e-05, 'learning_rate': 8.110244809608495e-06, 'epoch': 7.62}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003722946858033538, 'learning_rate': 7.825076849127458e-06, 'epoch': 7.66}\n",
      "{'loss': 0.0, 'grad_norm': 0.0008297181921079755, 'learning_rate': 7.5440795478481815e-06, 'epoch': 7.71}\n",
      "{'loss': 0.0, 'grad_norm': 0.004017130937427282, 'learning_rate': 7.26732114153334e-06, 'epoch': 7.75}\n",
      "{'loss': 0.0, 'grad_norm': 0.000907350389752537, 'learning_rate': 6.99486883659684e-06, 'epoch': 7.8}\n",
      "{'loss': 0.0, 'grad_norm': 0.0007327233906835318, 'learning_rate': 6.72678879378377e-06, 'epoch': 7.84}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011008382716681808, 'learning_rate': 6.463146112104332e-06, 'epoch': 7.88}\n",
      "{'loss': 0.0, 'grad_norm': 6.335963553283364e-05, 'learning_rate': 6.204004813025568e-06, 'epoch': 7.93}\n",
      "{'loss': 0.0, 'grad_norm': 0.00037844269536435604, 'learning_rate': 5.949427824924731e-06, 'epoch': 7.97}\n",
      "{'loss': 0.0, 'grad_norm': 0.0014946742448955774, 'learning_rate': 5.699476967808168e-06, 'epoch': 8.02}\n",
      "{'loss': 0.0, 'grad_norm': 8.760030323173851e-05, 'learning_rate': 5.454212938299255e-06, 'epoch': 8.06}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002941512211691588, 'learning_rate': 5.2136952948992346e-06, 'epoch': 8.11}\n",
      "{'loss': 0.0, 'grad_norm': 5.27796073583886e-05, 'learning_rate': 4.977982443524304e-06, 'epoch': 8.15}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002744482771959156, 'learning_rate': 4.747131623322737e-06, 'epoch': 8.2}\n",
      "{'loss': 0.0, 'grad_norm': 9.500033047515899e-05, 'learning_rate': 4.521198892775203e-06, 'epoch': 8.24}\n",
      "{'loss': 0.0, 'grad_norm': 0.002744798082858324, 'learning_rate': 4.3002391160818775e-06, 'epoch': 8.29}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001024712692014873, 'learning_rate': 4.0843059498395065e-06, 'epoch': 8.33}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002304952940903604, 'learning_rate': 3.873451830011795e-06, 'epoch': 8.37}\n",
      "{'loss': 0.0, 'grad_norm': 0.00016222878184635192, 'learning_rate': 3.66772795919611e-06, 'epoch': 8.42}\n",
      "{'loss': 0.0, 'grad_norm': 9.611382847651839e-05, 'learning_rate': 3.4671842941897765e-06, 'epoch': 8.46}\n",
      "{'loss': 0.0, 'grad_norm': 0.00014441722305491567, 'learning_rate': 3.27186953385884e-06, 'epoch': 8.51}\n",
      "{'loss': 0.0, 'grad_norm': 0.004272943362593651, 'learning_rate': 3.081831107312308e-06, 'epoch': 8.55}\n",
      "{'loss': 0.0, 'grad_norm': 7.686247408855706e-05, 'learning_rate': 2.8971151623847587e-06, 'epoch': 8.6}\n",
      "{'loss': 0.0, 'grad_norm': 0.00016167762805707753, 'learning_rate': 2.717766554430043e-06, 'epoch': 8.64}\n",
      "{'loss': 0.0, 'grad_norm': 0.00019478483591228724, 'learning_rate': 2.543828835428899e-06, 'epoch': 8.69}\n",
      "{'loss': 0.0, 'grad_norm': 0.00012713042087852955, 'learning_rate': 2.3753442434129997e-06, 'epoch': 8.73}\n",
      "{'loss': 0.0, 'grad_norm': 0.00040521827759221196, 'learning_rate': 2.212353692208172e-06, 'epoch': 8.78}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002336410980205983, 'learning_rate': 2.0548967614990507e-06, 'epoch': 8.82}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00013279936683829874, 'learning_rate': 1.9030116872178316e-06, 'epoch': 8.86}\n",
      "{'loss': 0.0, 'grad_norm': 0.00023423039237968624, 'learning_rate': 1.7567353522592477e-06, 'epoch': 8.91}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003885050246026367, 'learning_rate': 1.6161032775241503e-06, 'epoch': 8.95}\n",
      "{'loss': 0.0, 'grad_norm': 8.059939136728644e-05, 'learning_rate': 1.4811496132938196e-06, 'epoch': 9.0}\n",
      "{'loss': 0.0, 'grad_norm': 0.00010005093645304441, 'learning_rate': 1.3519071309370996e-06, 'epoch': 9.04}\n",
      "{'loss': 0.0, 'grad_norm': 6.729590677423403e-05, 'learning_rate': 1.2284072149524094e-06, 'epoch': 9.09}\n",
      "{'loss': 0.0, 'grad_norm': 5.1630238885991275e-05, 'learning_rate': 1.1106798553464804e-06, 'epoch': 9.13}\n",
      "{'loss': 0.0, 'grad_norm': 8.281679038191214e-05, 'learning_rate': 9.98753640351785e-07, 'epoch': 9.18}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011972397624049336, 'learning_rate': 8.926557494843085e-07, 'epoch': 9.22}\n",
      "{'loss': 0.0, 'grad_norm': 6.531958206323907e-05, 'learning_rate': 7.924119469434665e-07, 'epoch': 9.27}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002550821518525481, 'learning_rate': 6.980465753556376e-07, 'epoch': 9.31}\n",
      "{'loss': 0.0, 'grad_norm': 8.811727457214147e-05, 'learning_rate': 6.095825498629692e-07, 'epoch': 9.35}\n",
      "{'loss': 0.0, 'grad_norm': 6.56511983834207e-05, 'learning_rate': 5.270413525587909e-07, 'epoch': 9.4}\n",
      "{'loss': 0.0, 'grad_norm': 0.000953804817982018, 'learning_rate': 4.5044302727100806e-07, 'epoch': 9.44}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002061470877379179, 'learning_rate': 3.7980617469479953e-07, 'epoch': 9.49}\n",
      "{'loss': 0.0, 'grad_norm': 0.000281306856777519, 'learning_rate': 3.1514794787571854e-07, 'epoch': 9.53}\n",
      "{'loss': 0.0, 'grad_norm': 5.6165779824368656e-05, 'learning_rate': 2.564840480443503e-07, 'epoch': 9.58}\n",
      "{'loss': 0.0, 'grad_norm': 4.684971645474434e-05, 'learning_rate': 2.0382872080351166e-07, 'epoch': 9.62}\n",
      "{'loss': 0.0, 'grad_norm': 4.039745545014739e-05, 'learning_rate': 1.571947526689349e-07, 'epoch': 9.67}\n",
      "{'loss': 0.0, 'grad_norm': 0.004783155396580696, 'learning_rate': 1.1659346796426551e-07, 'epoch': 9.71}\n",
      "{'loss': 0.0, 'grad_norm': 0.0020641095470637083, 'learning_rate': 8.203472607112295e-08, 'epoch': 9.76}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011164349416503683, 'learning_rate': 5.352691903491303e-08, 'epoch': 9.8}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002960123820230365, 'learning_rate': 3.107696952694139e-08, 'epoch': 9.84}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011015539348591119, 'learning_rate': 1.4690329163363769e-08, 'epoch': 9.89}\n",
      "{'loss': 0.0, 'grad_norm': 2.5985978936660103e-05, 'learning_rate': 4.370977181339386e-09, 'epoch': 9.93}\n",
      "{'loss': 0.0, 'grad_norm': 0.00012205949315102771, 'learning_rate': 1.214194727400253e-10, 'epoch': 9.98}\n",
      "100%|█████████████████████████████████████| 2240/2240 [2:07:21<00:00,  3.32s/it][INFO|trainer.py:2329] 2024-06-13 15:26:59,166 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 7641.848, 'train_samples_per_second': 2.35, 'train_steps_per_second': 0.293, 'train_loss': 0.010164137166964338, 'epoch': 9.98}\n",
      "100%|█████████████████████████████████████| 2240/2240 [2:07:21<00:00,  3.41s/it]\n",
      "[INFO|trainer.py:3410] 2024-06-13 15:26:59,169 >> Saving model checkpoint to /Utilisateurs/umushtaq/models/PE_LTC_paragraph_llama-3-8b-Instruct-bnb-4bit\n",
      "[INFO|configuration_utils.py:733] 2024-06-13 15:26:59,453 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-13 15:26:59,454 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-13 15:27:00,384 >> tokenizer config file saved in /Utilisateurs/umushtaq/models/PE_LTC_paragraph_llama-3-8b-Instruct-bnb-4bit/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-13 15:27:00,384 >> Special tokens file saved in /Utilisateurs/umushtaq/models/PE_LTC_paragraph_llama-3-8b-Instruct-bnb-4bit/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =      9.9777\n",
      "  total_flos               = 275243874GF\n",
      "  train_loss               =      0.0102\n",
      "  train_runtime            =  2:07:21.84\n",
      "  train_samples_per_second =        2.35\n",
      "  train_steps_per_second   =       0.293\n",
      "[INFO|modelcard.py:450] 2024-06-13 15:27:00,590 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 15:27:02,047 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 15:27:02,049 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 15:27:02,050 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 15:27:02,052 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-06-13 15:27:02,305 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13/2024 15:27:02 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/llama-factory-notebook/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-13 15:27:02,411 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-13 15:27:02,413 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13/2024 15:27:02 - INFO - llamafactory.model.utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "06/13/2024 15:27:02 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|quantization_config.py:393] 2024-06-13 15:27:02,829 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:3474] 2024-06-13 15:27:02,836 >> loading weights file model.safetensors from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/model.safetensors\n",
      "[INFO|modeling_utils.py:1519] 2024-06-13 15:27:02,867 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:962] 2024-06-13 15:27:02,872 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4280] 2024-06-13 15:27:05,247 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-06-13 15:27:05,249 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-06-13 15:27:05,356 >> loading configuration file generation_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-06-13 15:27:05,358 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13/2024 15:27:05 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "06/13/2024 15:27:05 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "06/13/2024 15:27:05 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "06/13/2024 15:27:05 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/models/PE_LTC_paragraph_llama-3-8b-Instruct-bnb-4bit\n",
      "06/13/2024 15:27:05 - INFO - llamafactory.model.loader - all params: 8051232768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc81163b44f4240928e230867f44030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/439 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Attack      0.882     0.652     0.750        46\n",
      "     Support      0.977     0.994     0.985       678\n",
      "\n",
      "    accuracy                          0.972       724\n",
      "   macro avg      0.930     0.823     0.868       724\n",
      "weighted avg      0.971     0.972     0.970       724\n",
      "\n",
      "/Utilisateurs/umushtaq\n",
      "\n",
      "Running experiment ['unsloth/llama-3-8b-Instruct-bnb-4bit', 'PE_LTC_paragraph_wo_tags_train.json', 'PE_LTC_paragraph_wo_tags_test.json']\n",
      "/Utilisateurs/umushtaq/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/llama-factory-notebook/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13/2024 15:36:15 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "06/13/2024 15:36:15 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 15:36:15,619 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 15:36:15,619 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 15:36:15,619 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 15:36:15,619 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-06-13 15:36:15,875 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "06/13/2024 15:36:15 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "06/13/2024 15:36:15 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/datasets/PE_LTC_paragraph_wo_tags_train.json...\n",
      "Converting format of dataset: 100%|█| 1796/1796 [00:00<00:00, 15073.45 examples/\n",
      "Running tokenizer on dataset: 100%|█| 1796/1796 [00:01<00:00, 1404.88 examples/s\n",
      "input_ids:\n",
      "[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 11190, 18328, 13, 128009, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 14646, 902, 5727, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 1472, 527, 1101, 2728, 264, 1160, 315, 13840, 315, 5552, 5811, 6956, 304, 279, 1376, 25, 18305, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 5850, 320, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 5850, 61453, 320, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 595, 948, 4718, 3465, 374, 311, 49229, 1855, 6857, 315, 5552, 5811, 6956, 304, 279, 1160, 439, 3060, 330, 8075, 1, 477, 330, 29702, 3343, 1472, 2011, 471, 264, 1160, 315, 12976, 4595, 304, 2768, 4823, 3645, 25, 5324, 23013, 9962, 794, 510, 23013, 1857, 320, 496, 705, 12976, 1857, 320, 496, 705, 61453, 12976, 1857, 320, 496, 7400, 633, 14711, 5810, 374, 279, 14646, 1495, 25, 12540, 4236, 387, 15972, 311, 20874, 477, 311, 47903, 18072, 14711, 8586, 374, 279, 1160, 315, 13840, 315, 5552, 5811, 6956, 304, 420, 14646, 25, 3132, 128009, 128006, 78191, 128007, 271, 5018, 23013, 9962, 794, 3132, 92, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "### You are an expert in Argument Mining. You are given a paragraph which contains argument components enclosed by <AC></AC> tags. You are also given a list of pairs of related argument components in the form: [(target AC (int), source AC (int)), (target AC (int), source AC (int)),..., (target AC (int), source AC (int))]. Your task is to classify each pair of related argument components in the list as either \"Support\" or \"Attack\". You must return a list of relation types in following JSON format: {\"relation_types\": [relation_type (str), relation_type (str),..., relation_type (str)]}\n",
      "\n",
      "### Here is the paragraph text: Should students be taught to compete or to cooperate?\n",
      "###Here is the list of pairs of related argument components in this paragraph: []<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\"relation_types\": []}<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 23013, 9962, 794, 3132, 92, 128009]\n",
      "labels:\n",
      "{\"relation_types\": []}<|eot_id|>\n",
      "/Utilisateurs/umushtaq/.conda/envs/llama-factory-notebook/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-13 15:36:17,952 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-13 15:36:17,953 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "06/13/2024 15:36:17 - INFO - llamafactory.model.utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "[WARNING|quantization_config.py:393] 2024-06-13 15:36:18,726 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:3474] 2024-06-13 15:36:18,744 >> loading weights file model.safetensors from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/model.safetensors\n",
      "[INFO|modeling_utils.py:1519] 2024-06-13 15:36:18,800 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:962] 2024-06-13 15:36:18,804 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4280] 2024-06-13 15:36:22,424 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-06-13 15:36:22,425 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-06-13 15:36:22,535 >> loading configuration file generation_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-06-13 15:36:22,536 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "06/13/2024 15:36:22 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\n",
      "06/13/2024 15:36:22 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "06/13/2024 15:36:22 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "06/13/2024 15:36:22 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "06/13/2024 15:36:22 - INFO - llamafactory.model.utils.misc - Found linear modules: v_proj,q_proj,gate_proj,k_proj,down_proj,o_proj,up_proj\n",
      "06/13/2024 15:36:22 - INFO - llamafactory.model.loader - trainable params: 20971520 || all params: 8051232768 || trainable%: 0.2605\n",
      "[INFO|trainer.py:641] 2024-06-13 15:36:22,979 >> Using auto half precision backend\n",
      "06/13/2024 15:36:23 - INFO - llamafactory.train.utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
      "[INFO|trainer.py:2078] 2024-06-13 15:36:23,192 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-06-13 15:36:23,192 >>   Num examples = 1,796\n",
      "[INFO|trainer.py:2080] 2024-06-13 15:36:23,192 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:2081] 2024-06-13 15:36:23,192 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2084] 2024-06-13 15:36:23,192 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2085] 2024-06-13 15:36:23,192 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2086] 2024-06-13 15:36:23,192 >>   Total optimization steps = 2,240\n",
      "[INFO|trainer.py:2087] 2024-06-13 15:36:23,196 >>   Number of trainable parameters = 20,971,520\n",
      "{'loss': 0.7486, 'grad_norm': 2.1757378578186035, 'learning_rate': 2.0089285714285715e-06, 'epoch': 0.04}\n",
      "{'loss': 0.0467, 'grad_norm': 2.8144209384918213, 'learning_rate': 4.241071428571429e-06, 'epoch': 0.09}\n",
      "{'loss': 0.0216, 'grad_norm': 0.9909223914146423, 'learning_rate': 6.473214285714287e-06, 'epoch': 0.13}\n",
      "{'loss': 0.0301, 'grad_norm': 0.26721107959747314, 'learning_rate': 8.705357142857143e-06, 'epoch': 0.18}\n",
      "{'loss': 0.0407, 'grad_norm': 0.4982137084007263, 'learning_rate': 1.09375e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0155, 'grad_norm': 0.24135901033878326, 'learning_rate': 1.3169642857142858e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0336, 'grad_norm': 0.15257631242275238, 'learning_rate': 1.5401785714285715e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0107, 'grad_norm': 0.17677801847457886, 'learning_rate': 1.7633928571428573e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0126, 'grad_norm': 0.21197573840618134, 'learning_rate': 1.9866071428571427e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0341, 'grad_norm': 0.6978017687797546, 'learning_rate': 2.2098214285714286e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0167, 'grad_norm': 0.6952716112136841, 'learning_rate': 2.4330357142857144e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0221, 'grad_norm': 0.03545217588543892, 'learning_rate': 2.6562500000000002e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0275, 'grad_norm': 0.11030280590057373, 'learning_rate': 2.8794642857142857e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0127, 'grad_norm': 0.17802496254444122, 'learning_rate': 3.102678571428572e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0036, 'grad_norm': 0.004369109869003296, 'learning_rate': 3.325892857142857e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0142, 'grad_norm': 0.6721985340118408, 'learning_rate': 3.5491071428571435e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0228, 'grad_norm': 0.3215861916542053, 'learning_rate': 3.7723214285714286e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0113, 'grad_norm': 0.010583569295704365, 'learning_rate': 3.9955357142857144e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0179, 'grad_norm': 0.06867918372154236, 'learning_rate': 4.21875e-05, 'epoch': 0.85}\n",
      "{'loss': 0.018, 'grad_norm': 0.1717725396156311, 'learning_rate': 4.4419642857142854e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0161, 'grad_norm': 2.1542060375213623, 'learning_rate': 4.665178571428572e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0045, 'grad_norm': 0.20831139385700226, 'learning_rate': 4.888392857142857e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0128, 'grad_norm': 0.31003043055534363, 'learning_rate': 4.9999241131520337e-05, 'epoch': 1.02}\n",
      "{'loss': 0.0074, 'grad_norm': 0.0014231903478503227, 'learning_rate': 4.999317046010329e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0174, 'grad_norm': 0.10067906975746155, 'learning_rate': 4.998103059143599e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0147, 'grad_norm': 0.00153082434553653, 'learning_rate': 4.996491795204623e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0218, 'grad_norm': 0.0020642720628529787, 'learning_rate': 4.9941255947808224e-05, 'epoch': 1.2}\n",
      "{'loss': 0.018, 'grad_norm': 0.27045950293540955, 'learning_rate': 4.991153735294049e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0127, 'grad_norm': 0.017792958766222, 'learning_rate': 4.987576938413504e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0605, 'grad_norm': 2.2919530868530273, 'learning_rate': 4.9833960727078975e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0017, 'grad_norm': 0.015738777816295624, 'learning_rate': 4.9786121534345265e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0179, 'grad_norm': 1.1988248825073242, 'learning_rate': 4.9732263422927315e-05, 'epoch': 1.43}\n",
      "{'loss': 0.021, 'grad_norm': 0.9313099384307861, 'learning_rate': 4.967239947141803e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0379, 'grad_norm': 1.6366008520126343, 'learning_rate': 4.960654421683386e-05, 'epoch': 1.51}\n",
      "{'loss': 0.032, 'grad_norm': 0.5652340054512024, 'learning_rate': 4.9534713651084696e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0197, 'grad_norm': 0.17164167761802673, 'learning_rate': 4.94569252170905e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0128, 'grad_norm': 0.016785137355327606, 'learning_rate': 4.937319780454559e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0026, 'grad_norm': 0.006351370830088854, 'learning_rate': 4.9283551745331534e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0163, 'grad_norm': 0.0025406209751963615, 'learning_rate': 4.9188008808579914e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0336, 'grad_norm': 0.5131105184555054, 'learning_rate': 4.9086592195385974e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0183, 'grad_norm': 0.1369166076183319, 'learning_rate': 4.89793265331747e-05, 'epoch': 1.83}\n",
      "{'loss': 0.0051, 'grad_norm': 0.0015895607648417354, 'learning_rate': 4.886623786972033e-05, 'epoch': 1.87}\n",
      "{'loss': 0.0096, 'grad_norm': 0.0010499178897589445, 'learning_rate': 4.874735366682115e-05, 'epoch': 1.92}\n",
      "{'loss': 0.0312, 'grad_norm': 0.21651214361190796, 'learning_rate': 4.8622702793630756e-05, 'epoch': 1.96}\n",
      "{'loss': 0.0088, 'grad_norm': 0.3176184594631195, 'learning_rate': 4.849231551964771e-05, 'epoch': 2.0}\n",
      "{'loss': 0.0121, 'grad_norm': 0.008831796236336231, 'learning_rate': 4.8356223507364996e-05, 'epoch': 2.05}\n",
      "{'loss': 0.0106, 'grad_norm': 0.4841039478778839, 'learning_rate': 4.821445980458134e-05, 'epoch': 2.09}\n",
      "{'loss': 0.001, 'grad_norm': 0.9104244709014893, 'learning_rate': 4.8067058836376044e-05, 'epoch': 2.14}\n",
      "{'loss': 0.0208, 'grad_norm': 0.00016247013991232961, 'learning_rate': 4.791405639674941e-05, 'epoch': 2.18}\n",
      "{'loss': 0.0205, 'grad_norm': 0.05111517757177353, 'learning_rate': 4.775548963993072e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0026, 'grad_norm': 0.19127695262432098, 'learning_rate': 4.759139707135592e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0005, 'grad_norm': 0.019079511985182762, 'learning_rate': 4.742181853831721e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0131, 'grad_norm': 0.004898557905107737, 'learning_rate': 4.724679522028672e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0096, 'grad_norm': 0.47933608293533325, 'learning_rate': 4.706636961891673e-05, 'epoch': 2.41}\n",
      "{'loss': 0.0022, 'grad_norm': 0.0535467304289341, 'learning_rate': 4.6880585547718845e-05, 'epoch': 2.45}\n",
      "{'loss': 0.0172, 'grad_norm': 0.0005038735107518733, 'learning_rate': 4.668948812142453e-05, 'epoch': 2.49}\n",
      "{'loss': 0.0093, 'grad_norm': 0.007022665813565254, 'learning_rate': 4.649312374502976e-05, 'epoch': 2.54}\n",
      "{'loss': 0.0157, 'grad_norm': 0.0008066107984632254, 'learning_rate': 4.6291540102526235e-05, 'epoch': 2.58}\n",
      "{'loss': 0.0065, 'grad_norm': 0.001610350445844233, 'learning_rate': 4.608478614532215e-05, 'epoch': 2.63}\n",
      "{'loss': 0.0049, 'grad_norm': 0.003067184239625931, 'learning_rate': 4.587291208035503e-05, 'epoch': 2.67}\n",
      "{'loss': 0.0116, 'grad_norm': 0.8631930351257324, 'learning_rate': 4.5655969357899874e-05, 'epoch': 2.72}\n",
      "{'loss': 0.0089, 'grad_norm': 0.46367087960243225, 'learning_rate': 4.543401065907516e-05, 'epoch': 2.76}\n",
      "{'loss': 0.0074, 'grad_norm': 2.356670379638672, 'learning_rate': 4.5207089883050136e-05, 'epoch': 2.81}\n",
      "{'loss': 0.0016, 'grad_norm': 0.0035543206613510847, 'learning_rate': 4.497526213395623e-05, 'epoch': 2.85}\n",
      "{'loss': 0.0007, 'grad_norm': 0.0010229405015707016, 'learning_rate': 4.4738583707505885e-05, 'epoch': 2.9}\n",
      "{'loss': 0.0216, 'grad_norm': 0.0009036623523570597, 'learning_rate': 4.4497112077322044e-05, 'epoch': 2.94}\n",
      "{'loss': 0.0114, 'grad_norm': 0.09079980850219727, 'learning_rate': 4.4250905880981574e-05, 'epoch': 2.98}\n",
      "{'loss': 0.002, 'grad_norm': 0.13071539998054504, 'learning_rate': 4.400002490577604e-05, 'epoch': 3.03}\n",
      "{'loss': 0.0021, 'grad_norm': 0.0006389051559381187, 'learning_rate': 4.374453007419336e-05, 'epoch': 3.07}\n",
      "{'loss': 0.0096, 'grad_norm': 1.9019798040390015, 'learning_rate': 4.3484483429123656e-05, 'epoch': 3.12}\n",
      "{'loss': 0.0046, 'grad_norm': 0.3196853697299957, 'learning_rate': 4.3219948118793214e-05, 'epoch': 3.16}\n",
      "{'loss': 0.0056, 'grad_norm': 0.00424893107265234, 'learning_rate': 4.295098838142985e-05, 'epoch': 3.21}\n",
      "{'loss': 0.0089, 'grad_norm': 0.8079983592033386, 'learning_rate': 4.267766952966369e-05, 'epoch': 3.25}\n",
      "{'loss': 0.0048, 'grad_norm': 0.01439042016863823, 'learning_rate': 4.240005793466709e-05, 'epoch': 3.3}\n",
      "{'loss': 0.0021, 'grad_norm': 0.028737621381878853, 'learning_rate': 4.211822101003734e-05, 'epoch': 3.34}\n",
      "{'loss': 0.0104, 'grad_norm': 0.0002502258575987071, 'learning_rate': 4.183222719542643e-05, 'epoch': 3.39}\n",
      "{'loss': 0.0025, 'grad_norm': 0.23025059700012207, 'learning_rate': 4.154214593992149e-05, 'epoch': 3.43}\n",
      "{'loss': 0.0025, 'grad_norm': 0.0038441503420472145, 'learning_rate': 4.1248047685180215e-05, 'epoch': 3.47}\n",
      "{'loss': 0.0009, 'grad_norm': 0.0024054201785475016, 'learning_rate': 4.095000384832522e-05, 'epoch': 3.52}\n",
      "{'loss': 0.0116, 'grad_norm': 0.19100478291511536, 'learning_rate': 4.064808680460148e-05, 'epoch': 3.56}\n",
      "{'loss': 0.0044, 'grad_norm': 0.007590200286358595, 'learning_rate': 4.034236986980119e-05, 'epoch': 3.61}\n",
      "{'loss': 0.0003, 'grad_norm': 0.012230586260557175, 'learning_rate': 4.0032927282460146e-05, 'epoch': 3.65}\n",
      "{'loss': 0.0066, 'grad_norm': 0.03561391308903694, 'learning_rate': 3.9719834185830116e-05, 'epoch': 3.7}\n",
      "{'loss': 0.0038, 'grad_norm': 0.007932838052511215, 'learning_rate': 3.940316660963147e-05, 'epoch': 3.74}\n",
      "{'loss': 0.0111, 'grad_norm': 0.6471788883209229, 'learning_rate': 3.908300145159055e-05, 'epoch': 3.79}\n",
      "{'loss': 0.0053, 'grad_norm': 0.004639755468815565, 'learning_rate': 3.875941645876631e-05, 'epoch': 3.83}\n",
      "{'loss': 0.0052, 'grad_norm': 0.007265015039592981, 'learning_rate': 3.84324902086706e-05, 'epoch': 3.88}\n",
      "{'loss': 0.0095, 'grad_norm': 0.047882966697216034, 'learning_rate': 3.810230209018694e-05, 'epoch': 3.92}\n",
      "{'loss': 0.0086, 'grad_norm': 0.0072963666170835495, 'learning_rate': 3.7768932284292146e-05, 'epoch': 3.96}\n",
      "{'loss': 0.0013, 'grad_norm': 0.013543493114411831, 'learning_rate': 3.74324617445856e-05, 'epoch': 4.01}\n",
      "{'loss': 0.0012, 'grad_norm': 0.06121395155787468, 'learning_rate': 3.7092972177631e-05, 'epoch': 4.05}\n",
      "{'loss': 0.0008, 'grad_norm': 0.10381816327571869, 'learning_rate': 3.6750546023115216e-05, 'epoch': 4.1}\n",
      "{'loss': 0.0014, 'grad_norm': 0.004521024879068136, 'learning_rate': 3.6405266433829075e-05, 'epoch': 4.14}\n",
      "{'loss': 0.0018, 'grad_norm': 0.006505629047751427, 'learning_rate': 3.6057217255475034e-05, 'epoch': 4.19}\n",
      "{'loss': 0.0024, 'grad_norm': 0.0020687920041382313, 'learning_rate': 3.570648300630657e-05, 'epoch': 4.23}\n",
      "{'loss': 0.006, 'grad_norm': 0.01424127072095871, 'learning_rate': 3.535314885660426e-05, 'epoch': 4.28}\n",
      "{'loss': 0.0014, 'grad_norm': 0.02463914267718792, 'learning_rate': 3.499730060799352e-05, 'epoch': 4.32}\n",
      "{'loss': 0.0072, 'grad_norm': 0.026772750541567802, 'learning_rate': 3.463902467260905e-05, 'epoch': 4.37}\n",
      "{'loss': 0.0012, 'grad_norm': 0.30878597497940063, 'learning_rate': 3.4278408052110946e-05, 'epoch': 4.41}\n",
      "{'loss': 0.0031, 'grad_norm': 0.00513667706400156, 'learning_rate': 3.391553831655782e-05, 'epoch': 4.45}\n",
      "{'loss': 0.0013, 'grad_norm': 1.2770551443099976, 'learning_rate': 3.355050358314172e-05, 'epoch': 4.5}\n",
      "{'loss': 0.0024, 'grad_norm': 0.08721622824668884, 'learning_rate': 3.318339249479026e-05, 'epoch': 4.54}\n",
      "{'loss': 0.0024, 'grad_norm': 0.0007021183264441788, 'learning_rate': 3.281429419864112e-05, 'epoch': 4.59}\n",
      "{'loss': 0.0194, 'grad_norm': 0.00032253071549348533, 'learning_rate': 3.244329832439404e-05, 'epoch': 4.63}\n",
      "{'loss': 0.0038, 'grad_norm': 0.016769936308264732, 'learning_rate': 3.207049496254569e-05, 'epoch': 4.68}\n",
      "{'loss': 0.0068, 'grad_norm': 0.0901808962225914, 'learning_rate': 3.1695974642512585e-05, 'epoch': 4.72}\n",
      "{'loss': 0.0008, 'grad_norm': 0.0027634005527943373, 'learning_rate': 3.131982831064744e-05, 'epoch': 4.77}\n",
      "{'loss': 0.0009, 'grad_norm': 0.0008903598063625395, 'learning_rate': 3.094214730815433e-05, 'epoch': 4.81}\n",
      "{'loss': 0.008, 'grad_norm': 0.0375090166926384, 'learning_rate': 3.056302334890786e-05, 'epoch': 4.86}\n",
      "{'loss': 0.0022, 'grad_norm': 0.002586246933788061, 'learning_rate': 3.0182548497181946e-05, 'epoch': 4.9}\n",
      "{'loss': 0.0029, 'grad_norm': 0.04965070262551308, 'learning_rate': 2.980081514529341e-05, 'epoch': 4.94}\n",
      "{'loss': 0.0003, 'grad_norm': 0.00047989445738494396, 'learning_rate': 2.9417915991166008e-05, 'epoch': 4.99}\n",
      "{'loss': 0.0025, 'grad_norm': 0.005873543675988913, 'learning_rate': 2.903394401582017e-05, 'epoch': 5.03}\n",
      "{'loss': 0.0002, 'grad_norm': 0.00149804528336972, 'learning_rate': 2.8648992460794056e-05, 'epoch': 5.08}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004523227398749441, 'learning_rate': 2.8263154805501297e-05, 'epoch': 5.12}\n",
      "{'loss': 0.0, 'grad_norm': 0.03775851055979729, 'learning_rate': 2.787652474453097e-05, 'epoch': 5.17}\n",
      "{'loss': 0.0012, 'grad_norm': 0.0006073449621908367, 'learning_rate': 2.748919616489542e-05, 'epoch': 5.21}\n",
      "{'loss': 0.007, 'grad_norm': 3.3035974502563477, 'learning_rate': 2.710126312323119e-05, 'epoch': 5.26}\n",
      "{'loss': 0.0005, 'grad_norm': 0.00013278264668770134, 'learning_rate': 2.6712819822958917e-05, 'epoch': 5.3}\n",
      "{'loss': 0.0139, 'grad_norm': 0.00013355301052797586, 'learning_rate': 2.632396059140749e-05, 'epoch': 5.35}\n",
      "{'loss': 0.0006, 'grad_norm': 0.2921489179134369, 'learning_rate': 2.593477985690815e-05, 'epoch': 5.39}\n",
      "{'loss': 0.0034, 'grad_norm': 0.0005157868145033717, 'learning_rate': 2.5545372125864032e-05, 'epoch': 5.43}\n",
      "{'loss': 0.0032, 'grad_norm': 0.8958199620246887, 'learning_rate': 2.515583195980084e-05, 'epoch': 5.48}\n",
      "{'loss': 0.0061, 'grad_norm': 0.0030702787917107344, 'learning_rate': 2.4766253952404024e-05, 'epoch': 5.52}\n",
      "{'loss': 0.0031, 'grad_norm': 0.002073203679174185, 'learning_rate': 2.4376732706548183e-05, 'epoch': 5.57}\n",
      "{'loss': 0.0007, 'grad_norm': 0.02443726174533367, 'learning_rate': 2.3987362811324298e-05, 'epoch': 5.61}\n",
      "{'loss': 0.0027, 'grad_norm': 0.007631442975252867, 'learning_rate': 2.3598238819070202e-05, 'epoch': 5.66}\n",
      "{'loss': 0.0004, 'grad_norm': 0.00038902118103578687, 'learning_rate': 2.3209455222410122e-05, 'epoch': 5.7}\n",
      "{'loss': 0.0006, 'grad_norm': 0.00038520610542036593, 'learning_rate': 2.2821106431308544e-05, 'epoch': 5.75}\n",
      "{'loss': 0.0023, 'grad_norm': 1.2275328636169434, 'learning_rate': 2.2433286750144293e-05, 'epoch': 5.79}\n",
      "{'loss': 0.0, 'grad_norm': 0.00036118197022005916, 'learning_rate': 2.204609035481018e-05, 'epoch': 5.84}\n",
      "{'loss': 0.002, 'grad_norm': 0.03164217621088028, 'learning_rate': 2.1659611269843906e-05, 'epoch': 5.88}\n",
      "{'loss': 0.0167, 'grad_norm': 0.0007967654964886606, 'learning_rate': 2.1273943345595637e-05, 'epoch': 5.92}\n",
      "{'loss': 0.0094, 'grad_norm': 0.21392138302326202, 'learning_rate': 2.0889180235437976e-05, 'epoch': 5.97}\n",
      "{'loss': 0.0043, 'grad_norm': 0.003213705960661173, 'learning_rate': 2.0505415373023684e-05, 'epoch': 6.01}\n",
      "{'loss': 0.0025, 'grad_norm': 0.017538951709866524, 'learning_rate': 2.0122741949596797e-05, 'epoch': 6.06}\n",
      "{'loss': 0.0032, 'grad_norm': 0.6028438806533813, 'learning_rate': 1.9741252891362612e-05, 'epoch': 6.1}\n",
      "{'loss': 0.0004, 'grad_norm': 0.3939867317676544, 'learning_rate': 1.936104083692202e-05, 'epoch': 6.15}\n",
      "{'loss': 0.0008, 'grad_norm': 0.0008922569104470313, 'learning_rate': 1.8982198114775682e-05, 'epoch': 6.19}\n",
      "{'loss': 0.0002, 'grad_norm': 0.001658802735619247, 'learning_rate': 1.86048167209035e-05, 'epoch': 6.24}\n",
      "{'loss': 0.0002, 'grad_norm': 0.04557753726840019, 'learning_rate': 1.8228988296424877e-05, 'epoch': 6.28}\n",
      "{'loss': 0.0044, 'grad_norm': 0.004939831793308258, 'learning_rate': 1.7854804105345062e-05, 'epoch': 6.33}\n",
      "{'loss': 0.0029, 'grad_norm': 0.002049373695626855, 'learning_rate': 1.7482355012393177e-05, 'epoch': 6.37}\n",
      "{'loss': 0.0002, 'grad_norm': 0.001348676742054522, 'learning_rate': 1.711173146095712e-05, 'epoch': 6.41}\n",
      "{'loss': 0.0001, 'grad_norm': 0.01422921847552061, 'learning_rate': 1.6743023451120832e-05, 'epoch': 6.46}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0014420503284782171, 'learning_rate': 1.637632051780917e-05, 'epoch': 6.5}\n",
      "{'loss': 0.0, 'grad_norm': 0.0014432294992730021, 'learning_rate': 1.6011711709045813e-05, 'epoch': 6.55}\n",
      "{'loss': 0.0004, 'grad_norm': 0.005391239188611507, 'learning_rate': 1.5649285564329296e-05, 'epoch': 6.59}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0004843803180847317, 'learning_rate': 1.5289130093132632e-05, 'epoch': 6.64}\n",
      "{'loss': 0.0002, 'grad_norm': 0.00042548333294689655, 'learning_rate': 1.4931332753531574e-05, 'epoch': 6.68}\n",
      "{'loss': 0.0, 'grad_norm': 0.003179361578077078, 'learning_rate': 1.4575980430966807e-05, 'epoch': 6.73}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0003259571094531566, 'learning_rate': 1.4223159417145179e-05, 'epoch': 6.77}\n",
      "{'loss': 0.0, 'grad_norm': 0.000510972342453897, 'learning_rate': 1.387295538908519e-05, 'epoch': 6.82}\n",
      "{'loss': 0.0, 'grad_norm': 0.0005705534713342786, 'learning_rate': 1.3525453388311554e-05, 'epoch': 6.86}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0009837133111432195, 'learning_rate': 1.318073780020433e-05, 'epoch': 6.9}\n",
      "{'loss': 0.0, 'grad_norm': 0.00030703371157869697, 'learning_rate': 1.2838892333507154e-05, 'epoch': 6.95}\n",
      "{'loss': 0.0012, 'grad_norm': 0.0007308187778107822, 'learning_rate': 1.2500000000000006e-05, 'epoch': 6.99}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001688450574874878, 'learning_rate': 1.2164143094340993e-05, 'epoch': 7.04}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0001731671509332955, 'learning_rate': 1.183140317408248e-05, 'epoch': 7.08}\n",
      "{'loss': 0.0, 'grad_norm': 0.004282613284885883, 'learning_rate': 1.1501861039866094e-05, 'epoch': 7.13}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001507232227595523, 'learning_rate': 1.1175596715801515e-05, 'epoch': 7.17}\n",
      "{'loss': 0.0, 'grad_norm': 0.0007133720791898668, 'learning_rate': 1.0852689430033972e-05, 'epoch': 7.22}\n",
      "{'loss': 0.0, 'grad_norm': 0.0172371044754982, 'learning_rate': 1.0533217595504858e-05, 'epoch': 7.26}\n",
      "{'loss': 0.0, 'grad_norm': 0.00045819825027137995, 'learning_rate': 1.0217258790910448e-05, 'epoch': 7.31}\n",
      "{'loss': 0.0001, 'grad_norm': 0.001145422924309969, 'learning_rate': 9.90488974186306e-06, 'epoch': 7.35}\n",
      "{'loss': 0.0, 'grad_norm': 0.00012347145820967853, 'learning_rate': 9.596186302259563e-06, 'epoch': 7.39}\n",
      "{'loss': 0.0, 'grad_norm': 0.00015991194231901318, 'learning_rate': 9.291223435861318e-06, 'epoch': 7.44}\n",
      "{'loss': 0.0, 'grad_norm': 0.0018597396556288004, 'learning_rate': 8.99007519809053e-06, 'epoch': 7.48}\n",
      "{'loss': 0.0, 'grad_norm': 0.00021378835663199425, 'learning_rate': 8.69281471804698e-06, 'epoch': 7.53}\n",
      "{'loss': 0.0, 'grad_norm': 0.00024602882331237197, 'learning_rate': 8.399514180749795e-06, 'epoch': 7.57}\n",
      "{'loss': 0.0, 'grad_norm': 9.866211621556431e-05, 'learning_rate': 8.110244809608495e-06, 'epoch': 7.62}\n",
      "{'loss': 0.0, 'grad_norm': 0.00013627427688334137, 'learning_rate': 7.825076849127458e-06, 'epoch': 7.66}\n",
      "{'loss': 0.0, 'grad_norm': 0.00015613278083037585, 'learning_rate': 7.5440795478481815e-06, 'epoch': 7.71}\n",
      "{'loss': 0.0, 'grad_norm': 0.01223087403923273, 'learning_rate': 7.26732114153334e-06, 'epoch': 7.75}\n",
      "{'loss': 0.0, 'grad_norm': 0.0008728314423933625, 'learning_rate': 6.99486883659684e-06, 'epoch': 7.8}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002146591868950054, 'learning_rate': 6.72678879378377e-06, 'epoch': 7.84}\n",
      "{'loss': 0.0, 'grad_norm': 0.00018451327923685312, 'learning_rate': 6.463146112104332e-06, 'epoch': 7.88}\n",
      "{'loss': 0.0, 'grad_norm': 0.00012763196718879044, 'learning_rate': 6.204004813025568e-06, 'epoch': 7.93}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002441247779643163, 'learning_rate': 5.949427824924731e-06, 'epoch': 7.97}\n",
      "{'loss': 0.0, 'grad_norm': 0.0022114403545856476, 'learning_rate': 5.699476967808168e-06, 'epoch': 8.02}\n",
      "{'loss': 0.0, 'grad_norm': 0.00013024380314163864, 'learning_rate': 5.454212938299255e-06, 'epoch': 8.06}\n",
      "{'loss': 0.0, 'grad_norm': 0.00014281213225331157, 'learning_rate': 5.2136952948992346e-06, 'epoch': 8.11}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001711891236482188, 'learning_rate': 4.977982443524304e-06, 'epoch': 8.15}\n",
      "{'loss': 0.0, 'grad_norm': 0.0019646405708044767, 'learning_rate': 4.747131623322737e-06, 'epoch': 8.2}\n",
      "{'loss': 0.0, 'grad_norm': 0.00015047371562104672, 'learning_rate': 4.521198892775203e-06, 'epoch': 8.24}\n",
      "{'loss': 0.0, 'grad_norm': 0.0014257323928177357, 'learning_rate': 4.3002391160818775e-06, 'epoch': 8.29}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001733606623020023, 'learning_rate': 4.0843059498395065e-06, 'epoch': 8.33}\n",
      "{'loss': 0.0, 'grad_norm': 0.00022825495398137718, 'learning_rate': 3.873451830011795e-06, 'epoch': 8.37}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001492237497586757, 'learning_rate': 3.66772795919611e-06, 'epoch': 8.42}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002778342750389129, 'learning_rate': 3.4671842941897765e-06, 'epoch': 8.46}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001859277399489656, 'learning_rate': 3.27186953385884e-06, 'epoch': 8.51}\n",
      "{'loss': 0.0, 'grad_norm': 0.0040486655198037624, 'learning_rate': 3.081831107312308e-06, 'epoch': 8.55}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011285350774414837, 'learning_rate': 2.8971151623847587e-06, 'epoch': 8.6}\n",
      "{'loss': 0.0, 'grad_norm': 0.00030445752781815827, 'learning_rate': 2.717766554430043e-06, 'epoch': 8.64}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003731494944076985, 'learning_rate': 2.543828835428899e-06, 'epoch': 8.69}\n",
      "{'loss': 0.0, 'grad_norm': 0.00016241824778262526, 'learning_rate': 2.3753442434129997e-06, 'epoch': 8.73}\n",
      "{'loss': 0.0, 'grad_norm': 0.0008665122441016138, 'learning_rate': 2.212353692208172e-06, 'epoch': 8.78}\n",
      "{'loss': 0.0, 'grad_norm': 8.910596079658717e-05, 'learning_rate': 2.0548967614990507e-06, 'epoch': 8.82}\n",
      "{'loss': 0.0, 'grad_norm': 9.396130189998075e-05, 'learning_rate': 1.9030116872178316e-06, 'epoch': 8.86}\n",
      "{'loss': 0.0, 'grad_norm': 0.00023552958737127483, 'learning_rate': 1.7567353522592477e-06, 'epoch': 8.91}\n",
      "{'loss': 0.0, 'grad_norm': 0.002089672489091754, 'learning_rate': 1.6161032775241503e-06, 'epoch': 8.95}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001159180246759206, 'learning_rate': 1.4811496132938196e-06, 'epoch': 9.0}\n",
      "{'loss': 0.0, 'grad_norm': 0.00012950139353051782, 'learning_rate': 1.3519071309370996e-06, 'epoch': 9.04}\n",
      "{'loss': 0.0, 'grad_norm': 0.00013748389028478414, 'learning_rate': 1.2284072149524094e-06, 'epoch': 9.09}\n",
      "{'loss': 0.0, 'grad_norm': 0.00013288923946674913, 'learning_rate': 1.1106798553464804e-06, 'epoch': 9.13}\n",
      "{'loss': 0.0, 'grad_norm': 7.476230530301109e-05, 'learning_rate': 9.98753640351785e-07, 'epoch': 9.18}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001280463329749182, 'learning_rate': 8.926557494843085e-07, 'epoch': 9.22}\n",
      "{'loss': 0.0, 'grad_norm': 0.00010335801198380068, 'learning_rate': 7.924119469434665e-07, 'epoch': 9.27}\n",
      "{'loss': 0.0, 'grad_norm': 0.00013463589129969478, 'learning_rate': 6.980465753556376e-07, 'epoch': 9.31}\n",
      "{'loss': 0.0, 'grad_norm': 0.00012023848830722272, 'learning_rate': 6.095825498629692e-07, 'epoch': 9.35}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001603167474968359, 'learning_rate': 5.270413525587909e-07, 'epoch': 9.4}\n",
      "{'loss': 0.0, 'grad_norm': 0.0013622951228171587, 'learning_rate': 4.5044302727100806e-07, 'epoch': 9.44}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002593093959148973, 'learning_rate': 3.7980617469479953e-07, 'epoch': 9.49}\n",
      "{'loss': 0.0, 'grad_norm': 0.00022656370128970593, 'learning_rate': 3.1514794787571854e-07, 'epoch': 9.53}\n",
      "{'loss': 0.0, 'grad_norm': 0.00014308451500255615, 'learning_rate': 2.564840480443503e-07, 'epoch': 9.58}\n",
      "{'loss': 0.0, 'grad_norm': 0.00010199959797319025, 'learning_rate': 2.0382872080351166e-07, 'epoch': 9.62}\n",
      "{'loss': 0.0, 'grad_norm': 8.529689512215555e-05, 'learning_rate': 1.571947526689349e-07, 'epoch': 9.67}\n",
      "{'loss': 0.0, 'grad_norm': 0.0014335495652630925, 'learning_rate': 1.1659346796426551e-07, 'epoch': 9.71}\n",
      "{'loss': 0.0, 'grad_norm': 0.0008636104175820947, 'learning_rate': 8.203472607112295e-08, 'epoch': 9.76}\n",
      "{'loss': 0.0, 'grad_norm': 0.00011029958113795146, 'learning_rate': 5.352691903491303e-08, 'epoch': 9.8}\n",
      "{'loss': 0.0, 'grad_norm': 0.004445059224963188, 'learning_rate': 3.107696952694139e-08, 'epoch': 9.84}\n",
      "{'loss': 0.0, 'grad_norm': 0.00013553754251915962, 'learning_rate': 1.4690329163363769e-08, 'epoch': 9.89}\n",
      "{'loss': 0.0, 'grad_norm': 8.094578515738249e-05, 'learning_rate': 4.370977181339386e-09, 'epoch': 9.93}\n",
      "{'loss': 0.0, 'grad_norm': 0.00018286144768353552, 'learning_rate': 1.214194727400253e-10, 'epoch': 9.98}\n",
      "100%|█████████████████████████████████████| 2240/2240 [2:00:37<00:00,  3.15s/it][INFO|trainer.py:2329] 2024-06-13 17:37:00,620 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 7237.4241, 'train_samples_per_second': 2.482, 'train_steps_per_second': 0.31, 'train_loss': 0.009492915000587994, 'epoch': 9.98}\n",
      "100%|█████████████████████████████████████| 2240/2240 [2:00:37<00:00,  3.23s/it]\n",
      "[INFO|trainer.py:3410] 2024-06-13 17:37:00,622 >> Saving model checkpoint to /Utilisateurs/umushtaq/models/PE_LTC_paragraph_wo_tags_llama-3-8b-Instruct-bnb-4bit\n",
      "[INFO|configuration_utils.py:733] 2024-06-13 17:37:00,956 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-13 17:37:00,956 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-13 17:37:01,867 >> tokenizer config file saved in /Utilisateurs/umushtaq/models/PE_LTC_paragraph_wo_tags_llama-3-8b-Instruct-bnb-4bit/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-13 17:37:01,868 >> Special tokens file saved in /Utilisateurs/umushtaq/models/PE_LTC_paragraph_wo_tags_llama-3-8b-Instruct-bnb-4bit/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =      9.9777\n",
      "  total_flos               = 252272176GF\n",
      "  train_loss               =      0.0095\n",
      "  train_runtime            =  2:00:37.42\n",
      "  train_samples_per_second =       2.482\n",
      "  train_steps_per_second   =        0.31\n",
      "[INFO|modelcard.py:450] 2024-06-13 17:37:02,098 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 17:37:03,546 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 17:37:03,548 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 17:37:03,549 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 17:37:03,550 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-06-13 17:37:03,801 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13/2024 17:37:03 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/llama-factory-notebook/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-13 17:37:03,921 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-13 17:37:03,923 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13/2024 17:37:03 - INFO - llamafactory.model.utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "06/13/2024 17:37:03 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|quantization_config.py:393] 2024-06-13 17:37:03,930 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:3474] 2024-06-13 17:37:03,964 >> loading weights file model.safetensors from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/model.safetensors\n",
      "[INFO|modeling_utils.py:1519] 2024-06-13 17:37:04,042 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:962] 2024-06-13 17:37:04,049 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4280] 2024-06-13 17:37:06,393 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-06-13 17:37:06,395 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-06-13 17:37:06,505 >> loading configuration file generation_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-06-13 17:37:06,507 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13/2024 17:37:06 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "06/13/2024 17:37:06 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "06/13/2024 17:37:06 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "06/13/2024 17:37:07 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/models/PE_LTC_paragraph_wo_tags_llama-3-8b-Instruct-bnb-4bit\n",
      "06/13/2024 17:37:07 - INFO - llamafactory.model.loader - all params: 8051232768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987a008462f446b095bd2739cffaa84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/439 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Attack      0.880     0.478     0.620        46\n",
      "     Support      0.966     0.996     0.980       678\n",
      "\n",
      "    accuracy                          0.963       724\n",
      "   macro avg      0.923     0.737     0.800       724\n",
      "weighted avg      0.960     0.963     0.957       724\n",
      "\n",
      "/Utilisateurs/umushtaq\n",
      "\n",
      "Running experiment ['unsloth/llama-3-8b-Instruct', 'PE_LTC_paragraph_train.json', 'PE_LTC_paragraph_test.json']\n",
      "/Utilisateurs/umushtaq/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/llama-factory-notebook/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13/2024 17:46:22 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "06/13/2024 17:46:22 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 17:46:22,918 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 17:46:22,918 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 17:46:22,918 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 17:46:22,918 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-06-13 17:46:23,178 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "06/13/2024 17:46:23 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "06/13/2024 17:46:23 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/datasets/PE_LTC_paragraph_train.json...\n",
      "Running tokenizer on dataset: 100%|█| 1796/1796 [00:01<00:00, 1457.39 examples/s\n",
      "input_ids:\n",
      "[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 11190, 18328, 13, 128009, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 14646, 902, 5727, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 1472, 527, 1101, 2728, 264, 1160, 315, 13840, 315, 5552, 5811, 6956, 304, 279, 1376, 25, 18305, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 5850, 320, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 5850, 61453, 320, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 595, 948, 4718, 3465, 374, 311, 49229, 1855, 6857, 315, 5552, 5811, 6956, 304, 279, 1160, 439, 3060, 330, 8075, 1, 477, 330, 29702, 3343, 1472, 2011, 471, 264, 1160, 315, 12976, 4595, 304, 2768, 4823, 3645, 25, 5324, 23013, 9962, 794, 510, 23013, 1857, 320, 496, 705, 12976, 1857, 320, 496, 705, 61453, 12976, 1857, 320, 496, 7400, 633, 14711, 5810, 374, 279, 14646, 1495, 25, 366, 16816, 29, 12540, 4236, 387, 15972, 311, 20874, 477, 311, 47903, 949, 694, 16816, 397, 14711, 8586, 374, 279, 1160, 315, 13840, 315, 5552, 5811, 6956, 304, 420, 14646, 25, 3132, 128009, 128006, 78191, 128007, 271, 5018, 23013, 9962, 794, 3132, 92, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "### You are an expert in Argument Mining. You are given a paragraph which contains argument components enclosed by <AC></AC> tags. You are also given a list of pairs of related argument components in the form: [(target AC (int), source AC (int)), (target AC (int), source AC (int)),..., (target AC (int), source AC (int))]. Your task is to classify each pair of related argument components in the list as either \"Support\" or \"Attack\". You must return a list of relation types in following JSON format: {\"relation_types\": [relation_type (str), relation_type (str),..., relation_type (str)]}\n",
      "\n",
      "### Here is the paragraph text: <topic> Should students be taught to compete or to cooperate? </topic>\n",
      "###Here is the list of pairs of related argument components in this paragraph: []<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\"relation_types\": []}<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 23013, 9962, 794, 3132, 92, 128009]\n",
      "labels:\n",
      "{\"relation_types\": []}<|eot_id|>\n",
      "/Utilisateurs/umushtaq/.conda/envs/llama-factory-notebook/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-13 17:46:24,990 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-13 17:46:24,991 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "06/13/2024 17:46:24 - INFO - llamafactory.model.utils.quantization - Quantizing model to 4 bit.\n",
      "[INFO|modeling_utils.py:3474] 2024-06-13 17:46:25,073 >> loading weights file model.safetensors from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1519] 2024-06-13 17:46:25,078 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:962] 2024-06-13 17:46:25,079 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:10<00:00,  2.60s/it]\n",
      "[INFO|modeling_utils.py:4280] 2024-06-13 17:46:35,779 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-06-13 17:46:35,779 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-06-13 17:46:36,051 >> loading configuration file generation_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-06-13 17:46:36,051 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "06/13/2024 17:46:36 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\n",
      "06/13/2024 17:46:36 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "06/13/2024 17:46:36 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "06/13/2024 17:46:36 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "06/13/2024 17:46:36 - INFO - llamafactory.model.utils.misc - Found linear modules: down_proj,gate_proj,v_proj,up_proj,q_proj,o_proj,k_proj\n",
      "06/13/2024 17:46:36 - INFO - llamafactory.model.loader - trainable params: 20971520 || all params: 8051232768 || trainable%: 0.2605\n",
      "[INFO|trainer.py:641] 2024-06-13 17:46:36,508 >> Using auto half precision backend\n",
      "06/13/2024 17:46:36 - INFO - llamafactory.train.utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
      "[INFO|trainer.py:2078] 2024-06-13 17:46:36,721 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-06-13 17:46:36,721 >>   Num examples = 1,796\n",
      "[INFO|trainer.py:2080] 2024-06-13 17:46:36,721 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:2081] 2024-06-13 17:46:36,721 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2084] 2024-06-13 17:46:36,721 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2085] 2024-06-13 17:46:36,721 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2086] 2024-06-13 17:46:36,721 >>   Total optimization steps = 2,240\n",
      "[INFO|trainer.py:2087] 2024-06-13 17:46:36,725 >>   Number of trainable parameters = 20,971,520\n",
      "{'loss': 0.7248, 'grad_norm': 2.3113908767700195, 'learning_rate': 2.0089285714285715e-06, 'epoch': 0.04}\n",
      "{'loss': 0.043, 'grad_norm': 3.2390623092651367, 'learning_rate': 4.241071428571429e-06, 'epoch': 0.09}\n",
      "{'loss': 0.0204, 'grad_norm': 0.935011088848114, 'learning_rate': 6.473214285714287e-06, 'epoch': 0.13}\n",
      "{'loss': 0.0268, 'grad_norm': 0.2801229655742645, 'learning_rate': 8.705357142857143e-06, 'epoch': 0.18}\n",
      "{'loss': 0.0381, 'grad_norm': 0.7548724412918091, 'learning_rate': 1.09375e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0141, 'grad_norm': 0.027602560818195343, 'learning_rate': 1.3169642857142858e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0284, 'grad_norm': 0.21027912199497223, 'learning_rate': 1.5401785714285715e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0127, 'grad_norm': 0.01955975778400898, 'learning_rate': 1.7633928571428573e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0126, 'grad_norm': 0.26842716336250305, 'learning_rate': 1.9642857142857145e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0239, 'grad_norm': 0.5008429288864136, 'learning_rate': 2.1875e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0222, 'grad_norm': 3.3051671981811523, 'learning_rate': 2.4107142857142858e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0236, 'grad_norm': 0.01304625067859888, 'learning_rate': 2.6339285714285716e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0142, 'grad_norm': 0.006848624907433987, 'learning_rate': 2.857142857142857e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0084, 'grad_norm': 0.0010947097325697541, 'learning_rate': 3.080357142857143e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0044, 'grad_norm': 0.006731922272592783, 'learning_rate': 3.303571428571429e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0077, 'grad_norm': 0.11809570342302322, 'learning_rate': 3.5267857142857145e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0243, 'grad_norm': 1.6228501796722412, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0175, 'grad_norm': 0.004803163465112448, 'learning_rate': 3.9732142857142855e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0171, 'grad_norm': 0.013068422675132751, 'learning_rate': 4.196428571428572e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0253, 'grad_norm': 0.49204424023628235, 'learning_rate': 4.419642857142857e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0157, 'grad_norm': 0.502726674079895, 'learning_rate': 4.642857142857143e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0097, 'grad_norm': 0.01989557407796383, 'learning_rate': 4.866071428571429e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0255, 'grad_norm': 2.565990924835205, 'learning_rate': 4.9999514323288454e-05, 'epoch': 1.02}\n",
      "{'loss': 0.0221, 'grad_norm': 0.13826872408390045, 'learning_rate': 4.999405067699773e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0157, 'grad_norm': 0.03914874419569969, 'learning_rate': 4.998251761970997e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0061, 'grad_norm': 0.0003018932475242764, 'learning_rate': 4.996491795204623e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0199, 'grad_norm': 0.000538039777893573, 'learning_rate': 4.9941255947808224e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0236, 'grad_norm': 0.00601766025647521, 'learning_rate': 4.991153735294049e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0256, 'grad_norm': 0.06996162980794907, 'learning_rate': 4.987576938413504e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0244, 'grad_norm': 3.436851739883423, 'learning_rate': 4.9833960727078975e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0065, 'grad_norm': 0.022313298657536507, 'learning_rate': 4.9786121534345265e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0187, 'grad_norm': 1.48516845703125, 'learning_rate': 4.9732263422927315e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0228, 'grad_norm': 0.4846045672893524, 'learning_rate': 4.967239947141803e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0285, 'grad_norm': 0.4524075984954834, 'learning_rate': 4.960654421683386e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0305, 'grad_norm': 1.609179973602295, 'learning_rate': 4.9534713651084696e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0503, 'grad_norm': 0.07378499954938889, 'learning_rate': 4.94569252170905e-05, 'epoch': 1.6}\n",
      "{'loss': 0.036, 'grad_norm': 0.04365398362278938, 'learning_rate': 4.937319780454559e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0424, 'grad_norm': 4.06055212020874, 'learning_rate': 4.9283551745331534e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0332, 'grad_norm': 0.2327485829591751, 'learning_rate': 4.9188008808579914e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0148, 'grad_norm': 0.8923928737640381, 'learning_rate': 4.9086592195385974e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0087, 'grad_norm': 0.10076367110013962, 'learning_rate': 4.89793265331747e-05, 'epoch': 1.83}\n",
      "{'loss': 0.0159, 'grad_norm': 0.05609801784157753, 'learning_rate': 4.886623786972033e-05, 'epoch': 1.87}\n",
      "{'loss': 0.0065, 'grad_norm': 0.00829281285405159, 'learning_rate': 4.874735366682115e-05, 'epoch': 1.92}\n",
      "{'loss': 0.0183, 'grad_norm': 0.11412230879068375, 'learning_rate': 4.8622702793630756e-05, 'epoch': 1.96}\n",
      "{'loss': 0.0096, 'grad_norm': 0.44152724742889404, 'learning_rate': 4.849231551964771e-05, 'epoch': 2.0}\n",
      "{'loss': 0.0076, 'grad_norm': 1.1631742715835571, 'learning_rate': 4.8356223507364996e-05, 'epoch': 2.05}\n",
      "{'loss': 0.0098, 'grad_norm': 0.16701316833496094, 'learning_rate': 4.821445980458134e-05, 'epoch': 2.09}\n",
      "{'loss': 0.0002, 'grad_norm': 0.012068236246705055, 'learning_rate': 4.8067058836376044e-05, 'epoch': 2.14}\n",
      "{'loss': 0.0022, 'grad_norm': 0.0001843378704506904, 'learning_rate': 4.791405639674941e-05, 'epoch': 2.18}\n",
      "{'loss': 0.0169, 'grad_norm': 0.001992602599784732, 'learning_rate': 4.7771595623029394e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0017, 'grad_norm': 0.0020183385349810123, 'learning_rate': 4.7608053864567926e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0142, 'grad_norm': 6.276535987854004, 'learning_rate': 4.743902209680302e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0176, 'grad_norm': 0.0024567842483520508, 'learning_rate': 4.7264541366433494e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0225, 'grad_norm': 0.895508348941803, 'learning_rate': 4.708465404335277e-05, 'epoch': 2.41}\n",
      "{'loss': 0.0264, 'grad_norm': 0.058338843286037445, 'learning_rate': 4.6899403810360055e-05, 'epoch': 2.45}\n",
      "{'loss': 0.0165, 'grad_norm': 0.004959458019584417, 'learning_rate': 4.670883565255264e-05, 'epoch': 2.49}\n",
      "{'loss': 0.0308, 'grad_norm': 0.20109683275222778, 'learning_rate': 4.6512995846401975e-05, 'epoch': 2.54}\n",
      "{'loss': 0.0293, 'grad_norm': 0.014721033163368702, 'learning_rate': 4.631193194851617e-05, 'epoch': 2.58}\n",
      "{'loss': 0.0154, 'grad_norm': 0.1156778484582901, 'learning_rate': 4.6105692784091636e-05, 'epoch': 2.63}\n",
      "{'loss': 0.0145, 'grad_norm': 1.7259900569915771, 'learning_rate': 4.589432843505659e-05, 'epoch': 2.67}\n",
      "{'loss': 0.0235, 'grad_norm': 0.16184213757514954, 'learning_rate': 4.567789022790953e-05, 'epoch': 2.72}\n",
      "{'loss': 0.0066, 'grad_norm': 0.02808052860200405, 'learning_rate': 4.545643072125538e-05, 'epoch': 2.76}\n",
      "{'loss': 0.0112, 'grad_norm': 1.6445636749267578, 'learning_rate': 4.523000369304243e-05, 'epoch': 2.81}\n",
      "{'loss': 0.0021, 'grad_norm': 0.0171738862991333, 'learning_rate': 4.499866412750324e-05, 'epoch': 2.85}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0032936083152890205, 'learning_rate': 4.476246820180259e-05, 'epoch': 2.9}\n",
      "{'loss': 0.0149, 'grad_norm': 0.0011538078542798758, 'learning_rate': 4.452147327239571e-05, 'epoch': 2.94}\n",
      "{'loss': 0.0151, 'grad_norm': 0.473615437746048, 'learning_rate': 4.4275737861100194e-05, 'epoch': 2.98}\n",
      "{'loss': 0.0127, 'grad_norm': 0.44282159209251404, 'learning_rate': 4.4025321640884905e-05, 'epoch': 3.03}\n",
      "{'loss': 0.0017, 'grad_norm': 0.0014326770324259996, 'learning_rate': 4.3770285421379324e-05, 'epoch': 3.07}\n",
      "{'loss': 0.0145, 'grad_norm': 0.5371965765953064, 'learning_rate': 4.351069113410688e-05, 'epoch': 3.12}\n",
      "{'loss': 0.008, 'grad_norm': 0.4565414786338806, 'learning_rate': 4.324660181744589e-05, 'epoch': 3.16}\n",
      "{'loss': 0.0082, 'grad_norm': 0.1214754581451416, 'learning_rate': 4.297808160132165e-05, 'epoch': 3.21}\n",
      "{'loss': 0.0112, 'grad_norm': 0.5607004165649414, 'learning_rate': 4.270519569163348e-05, 'epoch': 3.25}\n",
      "{'loss': 0.0091, 'grad_norm': 0.009269158355891705, 'learning_rate': 4.242801035442058e-05, 'epoch': 3.3}\n",
      "{'loss': 0.0004, 'grad_norm': 0.012648766860365868, 'learning_rate': 4.214659289977027e-05, 'epoch': 3.34}\n",
      "{'loss': 0.002, 'grad_norm': 0.0002481154224369675, 'learning_rate': 4.186101166547286e-05, 'epoch': 3.39}\n",
      "{'loss': 0.0128, 'grad_norm': 2.1235620975494385, 'learning_rate': 4.157133600042686e-05, 'epoch': 3.43}\n",
      "{'loss': 0.0033, 'grad_norm': 0.0003053987165912986, 'learning_rate': 4.127763624779873e-05, 'epoch': 3.47}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0002046891167992726, 'learning_rate': 4.0979983727941115e-05, 'epoch': 3.52}\n",
      "{'loss': 0.0397, 'grad_norm': 1.8915855884552002, 'learning_rate': 4.067845072107383e-05, 'epoch': 3.56}\n",
      "{'loss': 0.006, 'grad_norm': 0.012440880760550499, 'learning_rate': 4.03731104497318e-05, 'epoch': 3.61}\n",
      "{'loss': 0.0015, 'grad_norm': 0.1387048363685608, 'learning_rate': 4.0064037060984017e-05, 'epoch': 3.65}\n",
      "{'loss': 0.0102, 'grad_norm': 0.003052200423553586, 'learning_rate': 3.9751305608428205e-05, 'epoch': 3.7}\n",
      "{'loss': 0.001, 'grad_norm': 0.05117218568921089, 'learning_rate': 3.943499203396517e-05, 'epoch': 3.74}\n",
      "{'loss': 0.0034, 'grad_norm': 0.1585596352815628, 'learning_rate': 3.911517314935752e-05, 'epoch': 3.79}\n",
      "{'loss': 0.0018, 'grad_norm': 0.004881733562797308, 'learning_rate': 3.8791926617577144e-05, 'epoch': 3.83}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0006127732922323048, 'learning_rate': 3.846533093394601e-05, 'epoch': 3.88}\n",
      "{'loss': 0.0093, 'grad_norm': 0.0006306295981630683, 'learning_rate': 3.8135465407074756e-05, 'epoch': 3.92}\n",
      "{'loss': 0.0062, 'grad_norm': 0.004209973383694887, 'learning_rate': 3.780241013960391e-05, 'epoch': 3.96}\n",
      "{'loss': 0.0015, 'grad_norm': 0.0008658331935293972, 'learning_rate': 3.746624600875216e-05, 'epoch': 4.01}\n",
      "{'loss': 0.0086, 'grad_norm': 0.7676407098770142, 'learning_rate': 3.712705464667667e-05, 'epoch': 4.05}\n",
      "{'loss': 0.0052, 'grad_norm': 0.7187272906303406, 'learning_rate': 3.678491842064995e-05, 'epoch': 4.1}\n",
      "{'loss': 0.0015, 'grad_norm': 0.0016570512671023607, 'learning_rate': 3.6439920413058244e-05, 'epoch': 4.14}\n",
      "{'loss': 0.0014, 'grad_norm': 0.0016192523762583733, 'learning_rate': 3.609214440122635e-05, 'epoch': 4.19}\n",
      "{'loss': 0.0083, 'grad_norm': 0.011735728941857815, 'learning_rate': 3.574167483707356e-05, 'epoch': 4.23}\n",
      "{'loss': 0.0023, 'grad_norm': 0.010345612652599812, 'learning_rate': 3.5388596826605885e-05, 'epoch': 4.28}\n",
      "{'loss': 0.0004, 'grad_norm': 0.013220038264989853, 'learning_rate': 3.503299610924935e-05, 'epoch': 4.32}\n",
      "{'loss': 0.0113, 'grad_norm': 0.008036424405872822, 'learning_rate': 3.4674959037029594e-05, 'epoch': 4.37}\n",
      "{'loss': 0.0007, 'grad_norm': 0.2202543020248413, 'learning_rate': 3.4314572553602576e-05, 'epoch': 4.41}\n",
      "{'loss': 0.0014, 'grad_norm': 0.00032074295450001955, 'learning_rate': 3.3951924173141735e-05, 'epoch': 4.45}\n",
      "{'loss': 0.001, 'grad_norm': 0.8525513410568237, 'learning_rate': 3.358710195908653e-05, 'epoch': 4.5}\n",
      "{'loss': 0.0012, 'grad_norm': 0.013538016006350517, 'learning_rate': 3.32201945027576e-05, 'epoch': 4.54}\n",
      "{'loss': 0.0014, 'grad_norm': 0.0011505982838571072, 'learning_rate': 3.2851290901843807e-05, 'epoch': 4.59}\n",
      "{'loss': 0.0011, 'grad_norm': 0.00013032963033765554, 'learning_rate': 3.248048073876622e-05, 'epoch': 4.63}\n",
      "{'loss': 0.0116, 'grad_norm': 0.006422148551791906, 'learning_rate': 3.210785405892448e-05, 'epoch': 4.68}\n",
      "{'loss': 0.0004, 'grad_norm': 0.01654098555445671, 'learning_rate': 3.173350134883066e-05, 'epoch': 4.72}\n",
      "{'loss': 0.0011, 'grad_norm': 0.01618492789566517, 'learning_rate': 3.1357513514136044e-05, 'epoch': 4.77}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0012966679641976953, 'learning_rate': 3.097998185755618e-05, 'epoch': 4.81}\n",
      "{'loss': 0.0005, 'grad_norm': 0.00033424937282688916, 'learning_rate': 3.0600998056699364e-05, 'epoch': 4.86}\n",
      "{'loss': 0.0024, 'grad_norm': 0.16564404964447021, 'learning_rate': 3.022065414180425e-05, 'epoch': 4.9}\n",
      "{'loss': 0.0028, 'grad_norm': 0.0074531156569719315, 'learning_rate': 2.983904247339173e-05, 'epoch': 4.94}\n",
      "{'loss': 0.0031, 'grad_norm': 0.0006012410740368068, 'learning_rate': 2.9456255719836644e-05, 'epoch': 4.99}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0038976704236119986, 'learning_rate': 2.9072386834864724e-05, 'epoch': 5.03}\n",
      "{'loss': 0.0021, 'grad_norm': 0.0007317446288652718, 'learning_rate': 2.8687529034980244e-05, 'epoch': 5.08}\n",
      "{'loss': 0.0044, 'grad_norm': 0.00010606290015857667, 'learning_rate': 2.8301775776829876e-05, 'epoch': 5.12}\n",
      "{'loss': 0.0007, 'grad_norm': 0.15120598673820496, 'learning_rate': 2.791522073450819e-05, 'epoch': 5.17}\n",
      "{'loss': 0.0008, 'grad_norm': 0.009072530083358288, 'learning_rate': 2.752795777681043e-05, 'epoch': 5.21}\n",
      "{'loss': 0.0001, 'grad_norm': 0.029569804668426514, 'learning_rate': 2.71400809444379e-05, 'epoch': 5.26}\n",
      "{'loss': 0.0003, 'grad_norm': 4.1081719245994464e-05, 'learning_rate': 2.6751684427161683e-05, 'epoch': 5.3}\n",
      "{'loss': 0.0002, 'grad_norm': 3.3987696951953694e-05, 'learning_rate': 2.6362862540950162e-05, 'epoch': 5.35}\n",
      "{'loss': 0.0, 'grad_norm': 8.850000449456275e-05, 'learning_rate': 2.5973709705065834e-05, 'epoch': 5.39}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00025238931993953884, 'learning_rate': 2.5584320419137127e-05, 'epoch': 5.43}\n",
      "{'loss': 0.0035, 'grad_norm': 1.417120099067688, 'learning_rate': 2.519478924021062e-05, 'epoch': 5.48}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0001339253649348393, 'learning_rate': 2.4805210759789382e-05, 'epoch': 5.52}\n",
      "{'loss': 0.003, 'grad_norm': 5.7953417126554996e-05, 'learning_rate': 2.441567958086288e-05, 'epoch': 5.57}\n",
      "{'loss': 0.0057, 'grad_norm': 0.01689297892153263, 'learning_rate': 2.4026290294934175e-05, 'epoch': 5.61}\n",
      "{'loss': 0.0017, 'grad_norm': 0.014565708115696907, 'learning_rate': 2.363713745904984e-05, 'epoch': 5.66}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0010955117177218199, 'learning_rate': 2.3248315572838316e-05, 'epoch': 5.7}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0029888658318668604, 'learning_rate': 2.2859919055562105e-05, 'epoch': 5.75}\n",
      "{'loss': 0.0005, 'grad_norm': 0.04081633314490318, 'learning_rate': 2.2472042223189572e-05, 'epoch': 5.79}\n",
      "{'loss': 0.0016, 'grad_norm': 0.00013647816376760602, 'learning_rate': 2.2084779265491813e-05, 'epoch': 5.84}\n",
      "{'loss': 0.0005, 'grad_norm': 0.22748588025569916, 'learning_rate': 2.169822422317014e-05, 'epoch': 5.88}\n",
      "{'loss': 0.0007, 'grad_norm': 0.0008975097443908453, 'learning_rate': 2.1312470965019762e-05, 'epoch': 5.92}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00271310075186193, 'learning_rate': 2.0927613165135285e-05, 'epoch': 5.97}\n",
      "{'loss': 0.0001, 'grad_norm': 2.9782178899040446e-05, 'learning_rate': 2.0543744280163362e-05, 'epoch': 6.01}\n",
      "{'loss': 0.0, 'grad_norm': 0.0033788331784307957, 'learning_rate': 2.0160957526608276e-05, 'epoch': 6.06}\n",
      "{'loss': 0.0001, 'grad_norm': 0.006174802780151367, 'learning_rate': 1.977934585819576e-05, 'epoch': 6.1}\n",
      "{'loss': 0.0001, 'grad_norm': 0.02661370299756527, 'learning_rate': 1.9399001943300645e-05, 'epoch': 6.15}\n",
      "{'loss': 0.0, 'grad_norm': 2.0181949366815388e-05, 'learning_rate': 1.9020018142443833e-05, 'epoch': 6.19}\n",
      "{'loss': 0.0, 'grad_norm': 9.312444308307022e-05, 'learning_rate': 1.864248648586395e-05, 'epoch': 6.24}\n",
      "{'loss': 0.0, 'grad_norm': 0.005415190476924181, 'learning_rate': 1.826649865116935e-05, 'epoch': 6.28}\n",
      "{'loss': 0.0, 'grad_norm': 9.395729284733534e-05, 'learning_rate': 1.7892145941075526e-05, 'epoch': 6.33}\n",
      "{'loss': 0.0, 'grad_norm': 5.6664350267965347e-05, 'learning_rate': 1.7519519261233786e-05, 'epoch': 6.37}\n",
      "{'loss': 0.0, 'grad_norm': 0.00018288889259565622, 'learning_rate': 1.7148709098156203e-05, 'epoch': 6.41}\n",
      "{'loss': 0.0, 'grad_norm': 0.0021609701216220856, 'learning_rate': 1.6779805497242406e-05, 'epoch': 6.46}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002135521499440074, 'learning_rate': 1.641289804091347e-05, 'epoch': 6.5}\n",
      "{'loss': 0.0, 'grad_norm': 7.391178223770112e-05, 'learning_rate': 1.6048075826858264e-05, 'epoch': 6.55}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003446417977102101, 'learning_rate': 1.5685427446397427e-05, 'epoch': 6.59}\n",
      "{'loss': 0.0, 'grad_norm': 2.259986285935156e-05, 'learning_rate': 1.5325040962970415e-05, 'epoch': 6.64}\n",
      "{'loss': 0.0, 'grad_norm': 6.678478530375287e-05, 'learning_rate': 1.4967003890750656e-05, 'epoch': 6.68}\n",
      "{'loss': 0.0, 'grad_norm': 0.0010863207280635834, 'learning_rate': 1.4611403173394126e-05, 'epoch': 6.73}\n",
      "{'loss': 0.0, 'grad_norm': 0.004189993254840374, 'learning_rate': 1.4258325162926442e-05, 'epoch': 6.77}\n",
      "{'loss': 0.0, 'grad_norm': 7.037031900836155e-05, 'learning_rate': 1.3907855598773653e-05, 'epoch': 6.82}\n",
      "{'loss': 0.0, 'grad_norm': 5.687014709110372e-05, 'learning_rate': 1.3560079586941765e-05, 'epoch': 6.86}\n",
      "{'loss': 0.0028, 'grad_norm': 0.0004846875090152025, 'learning_rate': 1.3215081579350058e-05, 'epoch': 6.9}\n",
      "{'loss': 0.0, 'grad_norm': 0.00014418558566831052, 'learning_rate': 1.2872945353323334e-05, 'epoch': 6.95}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0008093127980828285, 'learning_rate': 1.2533753991247843e-05, 'epoch': 6.99}\n",
      "{'loss': 0.0, 'grad_norm': 0.0008943749126046896, 'learning_rate': 1.2197589860396108e-05, 'epoch': 7.04}\n",
      "{'loss': 0.0, 'grad_norm': 0.00024914403911679983, 'learning_rate': 1.1864534592925253e-05, 'epoch': 7.08}\n",
      "{'loss': 0.0, 'grad_norm': 0.030782341957092285, 'learning_rate': 1.1534669066054e-05, 'epoch': 7.13}\n",
      "{'loss': 0.0, 'grad_norm': 4.865905793849379e-05, 'learning_rate': 1.1208073382422865e-05, 'epoch': 7.17}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0006111674592830241, 'learning_rate': 1.088482685064249e-05, 'epoch': 7.22}\n",
      "{'loss': 0.0, 'grad_norm': 0.0026847212575376034, 'learning_rate': 1.0565007966034843e-05, 'epoch': 7.26}\n",
      "{'loss': 0.0, 'grad_norm': 0.00047767951036803424, 'learning_rate': 1.0248694391571801e-05, 'epoch': 7.31}\n",
      "{'loss': 0.0, 'grad_norm': 0.00042548292549327016, 'learning_rate': 9.935962939015982e-06, 'epoch': 7.35}\n",
      "{'loss': 0.0, 'grad_norm': 0.00012664902897085994, 'learning_rate': 9.626889550268202e-06, 'epoch': 7.39}\n",
      "{'loss': 0.0, 'grad_norm': 0.0006530918763019145, 'learning_rate': 9.32154927892617e-06, 'epoch': 7.44}\n",
      "{'loss': 0.0, 'grad_norm': 0.0011459293309599161, 'learning_rate': 9.02001627205889e-06, 'epoch': 7.48}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002646078064572066, 'learning_rate': 8.722363752201277e-06, 'epoch': 7.53}\n",
      "{'loss': 0.0, 'grad_norm': 0.0005198146682232618, 'learning_rate': 8.428663999573142e-06, 'epoch': 7.57}\n",
      "{'loss': 0.0, 'grad_norm': 2.7279433197691105e-05, 'learning_rate': 8.138988334527143e-06, 'epoch': 7.62}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001478481135563925, 'learning_rate': 7.853407100229731e-06, 'epoch': 7.66}\n",
      "{'loss': 0.0, 'grad_norm': 0.0005368783022277057, 'learning_rate': 7.571989645579419e-06, 'epoch': 7.71}\n",
      "{'loss': 0.0, 'grad_norm': 0.0020019947551190853, 'learning_rate': 7.294804308366524e-06, 'epoch': 7.75}\n",
      "{'loss': 0.0, 'grad_norm': 0.001507870270870626, 'learning_rate': 7.0219183986783596e-06, 'epoch': 7.8}\n",
      "{'loss': 0.0, 'grad_norm': 0.0012733136536553502, 'learning_rate': 6.753398182554116e-06, 'epoch': 7.84}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001075147811206989, 'learning_rate': 6.489308865893121e-06, 'epoch': 7.88}\n",
      "{'loss': 0.0, 'grad_norm': 1.9019122191821225e-05, 'learning_rate': 6.2297145786206795e-06, 'epoch': 7.93}\n",
      "{'loss': 0.0, 'grad_norm': 0.000422983051976189, 'learning_rate': 5.974678359115094e-06, 'epoch': 7.97}\n",
      "{'loss': 0.0, 'grad_norm': 0.0010883057257160544, 'learning_rate': 5.724262138899813e-06, 'epoch': 8.02}\n",
      "{'loss': 0.0, 'grad_norm': 0.00013368237705435604, 'learning_rate': 5.478526727604296e-06, 'epoch': 8.06}\n",
      "{'loss': 0.0, 'grad_norm': 0.0002237894368590787, 'learning_rate': 5.237531798197415e-06, 'epoch': 8.11}\n",
      "{'loss': 0.0, 'grad_norm': 2.9909055228927173e-05, 'learning_rate': 5.00133587249676e-06, 'epoch': 8.15}\n",
      "{'loss': 0.0, 'grad_norm': 0.00019764163880608976, 'learning_rate': 4.769996306957569e-06, 'epoch': 8.2}\n",
      "{'loss': 0.0, 'grad_norm': 3.369249316165224e-05, 'learning_rate': 4.543569278744625e-06, 'epoch': 8.24}\n",
      "{'loss': 0.0, 'grad_norm': 0.0012226476101204753, 'learning_rate': 4.3221097720904716e-06, 'epoch': 8.29}\n",
      "{'loss': 0.0, 'grad_norm': 0.00013434325228445232, 'learning_rate': 4.1056715649434195e-06, 'epoch': 8.33}\n",
      "{'loss': 0.0, 'grad_norm': 0.00016261482960544527, 'learning_rate': 3.894307215908371e-06, 'epoch': 8.37}\n",
      "{'loss': 0.0, 'grad_norm': 7.986862328834832e-05, 'learning_rate': 3.6880680514838283e-06, 'epoch': 8.42}\n",
      "{'loss': 0.0, 'grad_norm': 0.0012066696071997285, 'learning_rate': 3.487004153598028e-06, 'epoch': 8.46}\n",
      "{'loss': 0.0, 'grad_norm': 0.00019083595543634146, 'learning_rate': 3.2911643474473646e-06, 'epoch': 8.51}\n",
      "{'loss': 0.0, 'grad_norm': 0.0036866602022200823, 'learning_rate': 3.1005961896399538e-06, 'epoch': 8.55}\n",
      "{'loss': 0.0, 'grad_norm': 4.083526073372923e-05, 'learning_rate': 2.915345956647239e-06, 'epoch': 8.6}\n",
      "{'loss': 0.0, 'grad_norm': 8.063470158958808e-05, 'learning_rate': 2.7354586335665207e-06, 'epoch': 8.64}\n",
      "{'loss': 0.0, 'grad_norm': 0.003014159854501486, 'learning_rate': 2.560977903196987e-06, 'epoch': 8.69}\n",
      "{'loss': 0.0, 'grad_norm': 0.0003532338887453079, 'learning_rate': 2.39194613543208e-06, 'epoch': 8.73}\n",
      "{'loss': 0.0, 'grad_norm': 0.000279032567050308, 'learning_rate': 2.2284043769706027e-06, 'epoch': 8.78}\n",
      "{'loss': 0.0, 'grad_norm': 9.838060941547155e-05, 'learning_rate': 2.070392341349203e-06, 'epoch': 8.82}\n",
      "{'loss': 0.0, 'grad_norm': 0.00026613249792717397, 'learning_rate': 1.9179483992985277e-06, 'epoch': 8.86}\n",
      "{'loss': 0.0, 'grad_norm': 0.0011304161744192243, 'learning_rate': 1.771109569425547e-06, 'epoch': 8.91}\n",
      "{'loss': 0.0, 'grad_norm': 0.0005335157620720565, 'learning_rate': 1.6299115092241247e-06, 'epoch': 8.95}\n",
      "{'loss': 0.0, 'grad_norm': 7.078394264681265e-05, 'learning_rate': 1.4943885064161889e-06, 'epoch': 9.0}\n",
      "{'loss': 0.0, 'grad_norm': 5.1186405471526086e-05, 'learning_rate': 1.364573470625477e-06, 'epoch': 9.04}\n",
      "{'loss': 0.0, 'grad_norm': 0.00038856343599036336, 'learning_rate': 1.2404979253859722e-06, 'epoch': 9.09}\n",
      "{'loss': 0.0, 'grad_norm': 3.6669214750872925e-05, 'learning_rate': 1.1221920004868991e-06, 'epoch': 9.13}\n",
      "{'loss': 0.0, 'grad_norm': 2.4010761990211904e-05, 'learning_rate': 1.0096844246561794e-06, 'epoch': 9.18}\n",
      "{'loss': 0.0, 'grad_norm': 0.00015375537623185664, 'learning_rate': 9.030025185841173e-07, 'epoch': 9.22}\n",
      "{'loss': 0.0, 'grad_norm': 0.00010332631063647568, 'learning_rate': 8.021721882889993e-07, 'epoch': 9.27}\n",
      "{'loss': 0.0, 'grad_norm': 5.602799865300767e-05, 'learning_rate': 7.072179188262251e-07, 'epoch': 9.31}\n",
      "{'loss': 0.0, 'grad_norm': 8.317639003507793e-05, 'learning_rate': 6.181627683425061e-07, 'epoch': 9.35}\n",
      "{'loss': 0.0, 'grad_norm': 2.701762605283875e-05, 'learning_rate': 5.350283624765417e-07, 'epoch': 9.4}\n",
      "{'loss': 0.0, 'grad_norm': 0.002307281829416752, 'learning_rate': 4.57834889107589e-07, 'epoch': 9.44}\n",
      "{'loss': 0.0, 'grad_norm': 0.00012170615082141012, 'learning_rate': 3.866010934531511e-07, 'epoch': 9.49}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004898869665339589, 'learning_rate': 3.2134427351699083e-07, 'epoch': 9.53}\n",
      "{'loss': 0.0, 'grad_norm': 3.817367905867286e-05, 'learning_rate': 2.620802758885876e-07, 'epoch': 9.58}\n",
      "{'loss': 0.0, 'grad_norm': 9.016099647851661e-05, 'learning_rate': 2.0882349189504936e-07, 'epoch': 9.62}\n",
      "{'loss': 0.0, 'grad_norm': 1.6429265087936074e-05, 'learning_rate': 1.6158685410639086e-07, 'epoch': 9.67}\n",
      "{'loss': 0.0, 'grad_norm': 0.0014599323039874434, 'learning_rate': 1.2038183319507955e-07, 'epoch': 9.71}\n",
      "{'loss': 0.0, 'grad_norm': 0.0009344730642624199, 'learning_rate': 8.521843515054694e-08, 'epoch': 9.76}\n",
      "{'loss': 0.0, 'grad_norm': 5.639539085677825e-05, 'learning_rate': 5.61051988494099e-08, 'epoch': 9.8}\n",
      "{'loss': 0.0, 'grad_norm': 0.0014588248450309038, 'learning_rate': 3.304919398192663e-08, 'epoch': 9.84}\n",
      "{'loss': 0.0, 'grad_norm': 5.3943491366226226e-05, 'learning_rate': 1.605601933523104e-08, 'epoch': 9.89}\n",
      "{'loss': 0.0, 'grad_norm': 1.527847234683577e-05, 'learning_rate': 5.129801433775838e-09, 'epoch': 9.93}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001260726130567491, 'learning_rate': 2.7319353723964656e-10, 'epoch': 9.98}\n",
      "100%|█████████████████████████████████████| 2240/2240 [1:43:36<00:00,  2.69s/it][INFO|trainer.py:2329] 2024-06-13 19:30:13,480 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 6216.7547, 'train_samples_per_second': 2.889, 'train_steps_per_second': 0.36, 'train_loss': 0.009838400572484066, 'epoch': 9.98}\n",
      "100%|█████████████████████████████████████| 2240/2240 [1:43:36<00:00,  2.78s/it]\n",
      "[INFO|trainer.py:3410] 2024-06-13 19:30:13,496 >> Saving model checkpoint to /Utilisateurs/umushtaq/models/PE_LTC_paragraph_llama-3-8b-Instruct\n",
      "[INFO|configuration_utils.py:733] 2024-06-13 19:30:13,916 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-13 19:30:13,917 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-13 19:30:15,832 >> tokenizer config file saved in /Utilisateurs/umushtaq/models/PE_LTC_paragraph_llama-3-8b-Instruct/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-13 19:30:15,839 >> Special tokens file saved in /Utilisateurs/umushtaq/models/PE_LTC_paragraph_llama-3-8b-Instruct/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =      9.9777\n",
      "  total_flos               = 275243874GF\n",
      "  train_loss               =      0.0098\n",
      "  train_runtime            =  1:43:36.75\n",
      "  train_samples_per_second =       2.889\n",
      "  train_steps_per_second   =        0.36\n",
      "[INFO|modelcard.py:450] 2024-06-13 19:30:16,159 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 19:30:17,612 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 19:30:17,614 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 19:30:17,615 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 19:30:17,616 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-06-13 19:30:17,860 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13/2024 19:30:17 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/llama-factory-notebook/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-13 19:30:17,975 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-13 19:30:17,977 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13/2024 19:30:17 - INFO - llamafactory.model.utils.quantization - Quantizing model to 4 bit.\n",
      "06/13/2024 19:30:17 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3474] 2024-06-13 19:30:17,986 >> loading weights file model.safetensors from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1519] 2024-06-13 19:30:17,993 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:962] 2024-06-13 19:30:17,995 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581372cd67f347e79c62cedaca6992e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:4280] 2024-06-13 19:30:23,252 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-06-13 19:30:23,253 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-06-13 19:30:23,365 >> loading configuration file generation_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-06-13 19:30:23,367 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13/2024 19:30:23 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "06/13/2024 19:30:23 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "06/13/2024 19:30:23 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "06/13/2024 19:30:23 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/models/PE_LTC_paragraph_llama-3-8b-Instruct\n",
      "06/13/2024 19:30:23 - INFO - llamafactory.model.loader - all params: 8051232768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bbccfda11b04d35821f0c83eaeaae21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/439 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Attack      0.892     0.717     0.795        46\n",
      "     Support      0.981     0.994     0.988       678\n",
      "\n",
      "    accuracy                          0.977       724\n",
      "   macro avg      0.936     0.856     0.891       724\n",
      "weighted avg      0.975     0.977     0.975       724\n",
      "\n",
      "/Utilisateurs/umushtaq\n",
      "\n",
      "Running experiment ['unsloth/llama-3-8b-Instruct', 'PE_LTC_paragraph_wo_tags_train.json', 'PE_LTC_paragraph_wo_tags_test.json']\n",
      "/Utilisateurs/umushtaq/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/llama-factory-notebook/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13/2024 19:39:18 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "06/13/2024 19:39:18 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 19:39:18,506 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 19:39:18,506 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 19:39:18,506 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-13 19:39:18,506 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-06-13 19:39:18,758 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "06/13/2024 19:39:18 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "06/13/2024 19:39:18 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/datasets/PE_LTC_paragraph_wo_tags_train.json...\n",
      "Running tokenizer on dataset: 100%|█| 1796/1796 [00:01<00:00, 1583.26 examples/s\n",
      "input_ids:\n",
      "[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 11190, 18328, 13, 128009, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 14646, 902, 5727, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 1472, 527, 1101, 2728, 264, 1160, 315, 13840, 315, 5552, 5811, 6956, 304, 279, 1376, 25, 18305, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 5850, 320, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 5850, 61453, 320, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 595, 948, 4718, 3465, 374, 311, 49229, 1855, 6857, 315, 5552, 5811, 6956, 304, 279, 1160, 439, 3060, 330, 8075, 1, 477, 330, 29702, 3343, 1472, 2011, 471, 264, 1160, 315, 12976, 4595, 304, 2768, 4823, 3645, 25, 5324, 23013, 9962, 794, 510, 23013, 1857, 320, 496, 705, 12976, 1857, 320, 496, 705, 61453, 12976, 1857, 320, 496, 7400, 633, 14711, 5810, 374, 279, 14646, 1495, 25, 12540, 4236, 387, 15972, 311, 20874, 477, 311, 47903, 18072, 14711, 8586, 374, 279, 1160, 315, 13840, 315, 5552, 5811, 6956, 304, 420, 14646, 25, 3132, 128009, 128006, 78191, 128007, 271, 5018, 23013, 9962, 794, 3132, 92, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "### You are an expert in Argument Mining. You are given a paragraph which contains argument components enclosed by <AC></AC> tags. You are also given a list of pairs of related argument components in the form: [(target AC (int), source AC (int)), (target AC (int), source AC (int)),..., (target AC (int), source AC (int))]. Your task is to classify each pair of related argument components in the list as either \"Support\" or \"Attack\". You must return a list of relation types in following JSON format: {\"relation_types\": [relation_type (str), relation_type (str),..., relation_type (str)]}\n",
      "\n",
      "### Here is the paragraph text: Should students be taught to compete or to cooperate?\n",
      "###Here is the list of pairs of related argument components in this paragraph: []<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\"relation_types\": []}<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 23013, 9962, 794, 3132, 92, 128009]\n",
      "labels:\n",
      "{\"relation_types\": []}<|eot_id|>\n",
      "/Utilisateurs/umushtaq/.conda/envs/llama-factory-notebook/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-13 19:39:20,734 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-13 19:39:20,735 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "06/13/2024 19:39:20 - INFO - llamafactory.model.utils.quantization - Quantizing model to 4 bit.\n",
      "[INFO|modeling_utils.py:3474] 2024-06-13 19:39:20,819 >> loading weights file model.safetensors from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1519] 2024-06-13 19:39:20,823 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:962] 2024-06-13 19:39:20,823 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:10<00:00,  2.62s/it]\n",
      "[INFO|modeling_utils.py:4280] 2024-06-13 19:39:31,787 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-06-13 19:39:31,788 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-06-13 19:39:32,063 >> loading configuration file generation_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct/snapshots/f77838872cca586fcbafa67efc77fb7d3afe775d/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-06-13 19:39:32,063 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "06/13/2024 19:39:32 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\n",
      "06/13/2024 19:39:32 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "06/13/2024 19:39:32 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "06/13/2024 19:39:32 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "06/13/2024 19:39:32 - INFO - llamafactory.model.utils.misc - Found linear modules: o_proj,up_proj,gate_proj,k_proj,down_proj,v_proj,q_proj\n",
      "06/13/2024 19:39:32 - INFO - llamafactory.model.loader - trainable params: 20971520 || all params: 8051232768 || trainable%: 0.2605\n",
      "[INFO|trainer.py:641] 2024-06-13 19:39:32,528 >> Using auto half precision backend\n",
      "06/13/2024 19:39:32 - INFO - llamafactory.train.utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
      "[INFO|trainer.py:2078] 2024-06-13 19:39:32,740 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-06-13 19:39:32,740 >>   Num examples = 1,796\n",
      "[INFO|trainer.py:2080] 2024-06-13 19:39:32,740 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:2081] 2024-06-13 19:39:32,740 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2084] 2024-06-13 19:39:32,740 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2085] 2024-06-13 19:39:32,740 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2086] 2024-06-13 19:39:32,740 >>   Total optimization steps = 2,240\n",
      "[INFO|trainer.py:2087] 2024-06-13 19:39:32,743 >>   Number of trainable parameters = 20,971,520\n",
      "{'loss': 0.7484, 'grad_norm': 2.144371509552002, 'learning_rate': 2.0089285714285715e-06, 'epoch': 0.04}\n",
      "{'loss': 0.0472, 'grad_norm': 2.8241000175476074, 'learning_rate': 4.241071428571429e-06, 'epoch': 0.09}\n",
      "{'loss': 0.0215, 'grad_norm': 0.9893636703491211, 'learning_rate': 6.473214285714287e-06, 'epoch': 0.13}\n",
      "{'loss': 0.0301, 'grad_norm': 0.24984285235404968, 'learning_rate': 8.705357142857143e-06, 'epoch': 0.18}\n",
      "{'loss': 0.0406, 'grad_norm': 0.4633408486843109, 'learning_rate': 1.09375e-05, 'epoch': 0.22}\n",
      "{'loss': 0.015, 'grad_norm': 0.28496474027633667, 'learning_rate': 1.3169642857142858e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0303, 'grad_norm': 0.13142135739326477, 'learning_rate': 1.5401785714285715e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0097, 'grad_norm': 0.04002426937222481, 'learning_rate': 1.7633928571428573e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0109, 'grad_norm': 0.11566805094480515, 'learning_rate': 1.9866071428571427e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0249, 'grad_norm': 0.4978567659854889, 'learning_rate': 2.2098214285714286e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0165, 'grad_norm': 0.09486262500286102, 'learning_rate': 2.4330357142857144e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0203, 'grad_norm': 0.03615241125226021, 'learning_rate': 2.6562500000000002e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0255, 'grad_norm': 0.03216886892914772, 'learning_rate': 2.8794642857142857e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0115, 'grad_norm': 0.0931415781378746, 'learning_rate': 3.102678571428572e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0032, 'grad_norm': 0.0039942567236721516, 'learning_rate': 3.325892857142857e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0089, 'grad_norm': 0.07758556306362152, 'learning_rate': 3.5491071428571435e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0233, 'grad_norm': 0.22493590414524078, 'learning_rate': 3.7723214285714286e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0196, 'grad_norm': 0.029402589425444603, 'learning_rate': 3.9955357142857144e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0154, 'grad_norm': 0.15051481127738953, 'learning_rate': 4.21875e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0295, 'grad_norm': 0.4775336682796478, 'learning_rate': 4.4419642857142854e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0226, 'grad_norm': 1.4123398065567017, 'learning_rate': 4.665178571428572e-05, 'epoch': 0.94}\n",
      "{'loss': 0.009, 'grad_norm': 0.006357001606374979, 'learning_rate': 4.888392857142857e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0166, 'grad_norm': 2.18325138092041, 'learning_rate': 4.9999241131520337e-05, 'epoch': 1.02}\n",
      "{'loss': 0.023, 'grad_norm': 0.014645658433437347, 'learning_rate': 4.999405067699773e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0289, 'grad_norm': 0.49848613142967224, 'learning_rate': 4.998251761970997e-05, 'epoch': 1.11}\n",
      "{'loss': 0.038, 'grad_norm': 0.1257196068763733, 'learning_rate': 4.996491795204623e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0245, 'grad_norm': 0.004818241111934185, 'learning_rate': 4.9941255947808224e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0132, 'grad_norm': 0.1235145702958107, 'learning_rate': 4.991153735294049e-05, 'epoch': 1.25}\n",
      "{'loss': 0.012, 'grad_norm': 0.15763546526432037, 'learning_rate': 4.987576938413504e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0253, 'grad_norm': 2.127624750137329, 'learning_rate': 4.9833960727078975e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0054, 'grad_norm': 0.0006287721917033195, 'learning_rate': 4.9786121534345265e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0472, 'grad_norm': 0.42694079875946045, 'learning_rate': 4.9732263422927315e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0156, 'grad_norm': 0.3534495234489441, 'learning_rate': 4.967239947141803e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0253, 'grad_norm': 2.8079605102539062, 'learning_rate': 4.960654421683386e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0492, 'grad_norm': 2.932776927947998, 'learning_rate': 4.9534713651084696e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0246, 'grad_norm': 8.967650413513184, 'learning_rate': 4.94569252170905e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0323, 'grad_norm': 0.03552459180355072, 'learning_rate': 4.937319780454559e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0366, 'grad_norm': 1.8657790422439575, 'learning_rate': 4.9283551745331534e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0109, 'grad_norm': 0.001854083500802517, 'learning_rate': 4.9188008808579914e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0469, 'grad_norm': 2.7604947090148926, 'learning_rate': 4.9086592195385974e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0199, 'grad_norm': 0.04319087415933609, 'learning_rate': 4.89793265331747e-05, 'epoch': 1.83}\n",
      "{'loss': 0.0158, 'grad_norm': 0.31167352199554443, 'learning_rate': 4.886623786972033e-05, 'epoch': 1.87}\n",
      "{'loss': 0.0056, 'grad_norm': 0.0015861891442909837, 'learning_rate': 4.874735366682115e-05, 'epoch': 1.92}\n",
      "{'loss': 0.0119, 'grad_norm': 0.24370865523815155, 'learning_rate': 4.8622702793630756e-05, 'epoch': 1.96}\n",
      "{'loss': 0.0101, 'grad_norm': 0.8712748289108276, 'learning_rate': 4.849231551964771e-05, 'epoch': 2.0}\n",
      "{'loss': 0.0079, 'grad_norm': 0.0456051267683506, 'learning_rate': 4.8356223507364996e-05, 'epoch': 2.05}\n",
      "{'loss': 0.0156, 'grad_norm': 0.2764073610305786, 'learning_rate': 4.821445980458134e-05, 'epoch': 2.09}\n",
      "{'loss': 0.0015, 'grad_norm': 0.021107545122504234, 'learning_rate': 4.8067058836376044e-05, 'epoch': 2.14}\n",
      "{'loss': 0.0108, 'grad_norm': 0.047910988330841064, 'learning_rate': 4.791405639674941e-05, 'epoch': 2.18}\n",
      "{'loss': 0.0125, 'grad_norm': 2.461207389831543, 'learning_rate': 4.775548963993072e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0048, 'grad_norm': 0.004474119748920202, 'learning_rate': 4.759139707135592e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0118, 'grad_norm': 0.2464408576488495, 'learning_rate': 4.742181853831721e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0064, 'grad_norm': 0.0065194591879844666, 'learning_rate': 4.724679522028672e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0145, 'grad_norm': 0.5608314275741577, 'learning_rate': 4.706636961891673e-05, 'epoch': 2.41}\n",
      "{'loss': 0.0042, 'grad_norm': 0.07852275669574738, 'learning_rate': 4.6880585547718845e-05, 'epoch': 2.45}\n",
      "{'loss': 0.0128, 'grad_norm': 0.003987419884651899, 'learning_rate': 4.668948812142453e-05, 'epoch': 2.49}\n",
      "{'loss': 0.0069, 'grad_norm': 0.012855550274252892, 'learning_rate': 4.649312374502976e-05, 'epoch': 2.54}\n",
      "{'loss': 0.0232, 'grad_norm': 0.028048448264598846, 'learning_rate': 4.6291540102526235e-05, 'epoch': 2.58}\n",
      "{'loss': 0.0149, 'grad_norm': 0.04188673570752144, 'learning_rate': 4.608478614532215e-05, 'epoch': 2.63}\n",
      "{'loss': 0.0059, 'grad_norm': 0.013209047727286816, 'learning_rate': 4.587291208035503e-05, 'epoch': 2.67}\n",
      "{'loss': 0.0046, 'grad_norm': 0.10312220454216003, 'learning_rate': 4.5655969357899874e-05, 'epoch': 2.72}\n",
      "{'loss': 0.0029, 'grad_norm': 0.0025183563120663166, 'learning_rate': 4.543401065907516e-05, 'epoch': 2.76}\n",
      "{'loss': 0.0191, 'grad_norm': 0.37212619185447693, 'learning_rate': 4.5207089883050136e-05, 'epoch': 2.81}\n",
      "{'loss': 0.0186, 'grad_norm': 0.011536081321537495, 'learning_rate': 4.497526213395623e-05, 'epoch': 2.85}\n",
      "{'loss': 0.0065, 'grad_norm': 0.1460859179496765, 'learning_rate': 4.4738583707505885e-05, 'epoch': 2.9}\n",
      "{'loss': 0.0244, 'grad_norm': 0.004871892277151346, 'learning_rate': 4.4497112077322044e-05, 'epoch': 2.94}\n",
      "{'loss': 0.008, 'grad_norm': 0.7928218245506287, 'learning_rate': 4.4250905880981574e-05, 'epoch': 2.98}\n",
      "{'loss': 0.0036, 'grad_norm': 0.26903921365737915, 'learning_rate': 4.400002490577604e-05, 'epoch': 3.03}\n",
      "{'loss': 0.0061, 'grad_norm': 0.0004079467908013612, 'learning_rate': 4.374453007419336e-05, 'epoch': 3.07}\n",
      "{'loss': 0.0144, 'grad_norm': 3.14686918258667, 'learning_rate': 4.3484483429123656e-05, 'epoch': 3.12}\n",
      "{'loss': 0.0082, 'grad_norm': 0.4690719544887543, 'learning_rate': 4.3219948118793214e-05, 'epoch': 3.16}\n",
      "{'loss': 0.0049, 'grad_norm': 0.002351950854063034, 'learning_rate': 4.295098838142985e-05, 'epoch': 3.21}\n",
      "{'loss': 0.0146, 'grad_norm': 0.2055404782295227, 'learning_rate': 4.267766952966369e-05, 'epoch': 3.25}\n",
      " 33%|████████████▍                         | 735/2240 [31:59<1:05:44,  2.62s/it]"
     ]
    }
   ],
   "source": [
    "for model_instance in model_names: # 3 models\n",
    "\n",
    "    for dataset in dataset_names:\n",
    "\n",
    "        l = [model_instance] + dataset\n",
    "        \n",
    "        with open(\"tmp.pkl\", \"wb\") as fh:\n",
    "            \n",
    "            pickle.dump(l, fh)\n",
    "        \n",
    "        print(f\"\\nRunning experiment {l}\")\n",
    "        \n",
    "        %run ./PE_LTC_finetune_v2.ipynb\n",
    "        %cd /Utilisateurs/umushtaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd041309-0821-4132-ad90-94835c839690",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for model_name in model_names: # 3 models\n",
    "\n",
    "#     for dataset in dataset_names:\n",
    "\n",
    "#         l = [model_name] + dataset\n",
    "        \n",
    "#         with open(\"tmp.pkl\", \"wb\") as fh:\n",
    "            \n",
    "#             pickle.dump(l, fh)\n",
    "        \n",
    "#         print(f\"\\nRunning experiment ...\")\n",
    "        \n",
    "#         %run ./PE_LTC_finetune.ipynb\n",
    "#         %cd /Utilisateurs/umushtaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afc00d2-8785-4576-9efd-b5398ccd9307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39075d8-4f45-429c-8d94-2aa715df7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /Utilisateurs/umushtaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c17d7-eaf4-4ffb-81a4-2d3288cd2bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../models/PE_LTC_llama-3-8b-Instruct-bnb-4bit/classification_report.pickle\", 'rb') as fh:\n",
    "#     x = pickle.load(fh)\n",
    "#models/PE_LTC_llama-3-70b-Instruct-bnb-4bit/classification_report.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f890218-9867-4b70-a989-923d895fc6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"models/PE_LTC_llama-3-70b-Instruct-bnb-4bit/classification_report.pickle\", 'rb') as fh:\n",
    "#     x = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5494fa0d-7375-4fb3-bbb9-990887b9252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43405c28-5ca7-44b9-b04c-1e42b34d34e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1506/3832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3551641f-dd30-4115-a179-45234ddec847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
