Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit acc
09/09/2024 16:21:11 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/09/2024 16:21:11 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16
09/09/2024 16:21:12 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 16:21:12 - INFO - llamafactory.data.loader - Loading dataset /nfs/scratch/umushtaq/coling_2025/cdcp/datasets/CDCP_acc_train.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 49229, 1855, 5811, 3777, 304, 279, 1495, 439, 3060, 330, 34210, 498, 330, 35890, 498, 330, 16690, 498, 330, 1985, 65556, 1, 477, 330, 970, 3343, 1472, 2011, 471, 264, 1160, 315, 5811, 3777, 4595, 11, 26549, 315, 3160, 220, 18, 11, 304, 2768, 4823, 3645, 25, 5324, 8739, 9962, 794, 4482, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 8, 93546, 1405, 1855, 2449, 330, 8739, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 34210, 498, 330, 35890, 498, 330, 16690, 498, 330, 1985, 65556, 1, 477, 330, 970, 3343, 4815, 14711, 5810, 374, 279, 1495, 25, 366, 1741, 16, 29, 1423, 323, 2254, 5590, 5718, 7170, 1304, 1670, 59358, 1790, 810, 4461, 4005, 1741, 16, 1822, 1741, 17, 29, 1789, 3187, 11, 994, 264, 1732, 889, 19755, 69944, 264, 11897, 374, 3309, 311, 2586, 311, 5590, 389, 264, 990, 1938, 11, 814, 1253, 387, 9770, 311, 5268, 1990, 264, 1670, 19971, 323, 872, 2683, 4005, 1741, 17, 1822, 1741, 18, 29, 358, 33147, 279, 356, 11960, 33, 311, 1505, 12659, 430, 21736, 38952, 45348, 520, 84783, 3115, 28743, 11, 81374, 11, 323, 43658, 11, 477, 40240, 449, 220, 11739, 17, 72, 4005, 1741, 18, 29, 128009, 128006, 78191, 128007, 271, 5018, 8739, 9962, 794, 4482, 970, 498, 330, 970, 498, 330, 35890, 93546, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to classify each argument component in the text as either "fact", "policy", "reference", "testimony" or "value". You must return a list of argument component types, strictly of length 3, in following JSON format: {"component_types": ["component_type (str)", "component_type (str)", "component_type (str)"]} where each element "component_type (str)" is replaced by either "fact", "policy", "reference", "testimony" or "value". 

### Here is the text: <AC1>State and local court rules sometimes make default judgments much more likely.</AC1><AC2> For example, when a person who allegedly owes a debt is told to come to court on a work day, they may be forced to choose between a default judgment and their job.</AC2><AC3> I urge the CFPB to find practices that involve scheduling hearings at inconvenient times unfair, deceptive, and abusive, or inconsistent with 1692i.</AC3><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"component_types": ["value", "value", "policy"]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 8739, 9962, 794, 4482, 970, 498, 330, 970, 498, 330, 35890, 93546, 128009]
labels:
{"component_types": ["value", "value", "policy"]}<|eot_id|>
09/09/2024 16:21:13 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 16:21:13 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 16:21:33 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/09/2024 16:21:33 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 16:21:33 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/09/2024 16:21:33 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/09/2024 16:21:33 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,v_proj,q_proj,o_proj,gate_proj,k_proj,up_proj
09/09/2024 16:21:34 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/09/2024 16:21:35 - WARNING - llamafactory.train.callbacks - Previous trainer log in this folder will be deleted.
09/09/2024 16:21:37 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.4528, 'grad_norm': 1.3997589349746704, 'learning_rate': 1.25e-05, 'epoch': 0.14}
{'loss': 0.2011, 'grad_norm': 0.7202951312065125, 'learning_rate': 2.6388888888888892e-05, 'epoch': 0.28}
{'loss': 0.1622, 'grad_norm': 0.5170820951461792, 'learning_rate': 4.027777777777778e-05, 'epoch': 0.41}
{'loss': 0.1455, 'grad_norm': 0.6996171474456787, 'learning_rate': 4.998942375205502e-05, 'epoch': 0.55}
{'loss': 0.1291, 'grad_norm': 1.5045768022537231, 'learning_rate': 4.9801650402038555e-05, 'epoch': 0.69}
{'loss': 0.1314, 'grad_norm': 0.8050339221954346, 'learning_rate': 4.938088021881233e-05, 'epoch': 0.83}
{'loss': 0.1201, 'grad_norm': 0.6460157632827759, 'learning_rate': 4.873106608932585e-05, 'epoch': 0.97}
{'loss': 0.0704, 'grad_norm': 0.8564806580543518, 'learning_rate': 4.785831263290449e-05, 'epoch': 1.1}
{'loss': 0.0769, 'grad_norm': 1.7794405221939087, 'learning_rate': 4.6770818851962305e-05, 'epoch': 1.24}
{'loss': 0.0942, 'grad_norm': 0.5160842537879944, 'learning_rate': 4.54788011072248e-05, 'epoch': 1.38}
{'loss': 0.0881, 'grad_norm': 0.8846837878227234, 'learning_rate': 4.3994397141063734e-05, 'epoch': 1.52}
{'loss': 0.0869, 'grad_norm': 0.8303022980690002, 'learning_rate': 4.233155205058811e-05, 'epoch': 1.66}
{'loss': 0.0645, 'grad_norm': 0.5816922187805176, 'learning_rate': 4.05058872817065e-05, 'epoch': 1.79}
{'loss': 0.091, 'grad_norm': 0.552050769329071, 'learning_rate': 3.8534553874884244e-05, 'epoch': 1.93}
{'loss': 0.0555, 'grad_norm': 0.2712700366973877, 'learning_rate': 3.643607134126452e-05, 'epoch': 2.07}
{'loss': 0.031, 'grad_norm': 0.6672087907791138, 'learning_rate': 3.423015368281711e-05, 'epoch': 2.21}
{'loss': 0.0277, 'grad_norm': 0.6393688917160034, 'learning_rate': 3.193752419095239e-05, 'epoch': 2.34}
{'loss': 0.0343, 'grad_norm': 1.2502342462539673, 'learning_rate': 2.957972076345817e-05, 'epoch': 2.48}
{'loss': 0.0273, 'grad_norm': 0.49882057309150696, 'learning_rate': 2.717889356869146e-05, 'epoch': 2.62}
{'loss': 0.0499, 'grad_norm': 0.5962091088294983, 'learning_rate': 2.475759695785054e-05, 'epoch': 2.76}
{'loss': 0.0379, 'grad_norm': 0.18527407944202423, 'learning_rate': 2.2338577580188296e-05, 'epoch': 2.9}
{'loss': 0.0322, 'grad_norm': 1.2369494438171387, 'learning_rate': 1.9944560691699057e-05, 'epoch': 3.03}
{'loss': 0.0105, 'grad_norm': 0.16166874766349792, 'learning_rate': 1.7598036664782508e-05, 'epoch': 3.17}
{'loss': 0.005, 'grad_norm': 0.034289754927158356, 'learning_rate': 1.532104970449999e-05, 'epoch': 3.31}
{'loss': 0.0074, 'grad_norm': 0.22977153956890106, 'learning_rate': 1.313499075630899e-05, 'epoch': 3.45}
{'loss': 0.0069, 'grad_norm': 0.3335634768009186, 'learning_rate': 1.1060396550785182e-05, 'epoch': 3.59}
{'loss': 0.0092, 'grad_norm': 0.18871721625328064, 'learning_rate': 9.116756673187878e-06, 'epoch': 3.72}
{'loss': 0.0054, 'grad_norm': 0.010563638992607594, 'learning_rate': 7.3223304703363135e-06, 'epoch': 3.86}
{'loss': 0.0081, 'grad_norm': 0.021538345143198967, 'learning_rate': 5.693975514848271e-06, 'epoch': 4.0}
{'loss': 0.0048, 'grad_norm': 0.01051558181643486, 'learning_rate': 4.24698923821803e-06, 'epoch': 4.14}
{'loss': 0.0004, 'grad_norm': 0.008251120336353779, 'learning_rate': 2.9949652204972254e-06, 'epoch': 4.28}
{'loss': 0.0043, 'grad_norm': 0.11000096797943115, 'learning_rate': 1.9496654866520414e-06, 'epoch': 4.41}
{'loss': 0.001, 'grad_norm': 0.03345932438969612, 'learning_rate': 1.1209100092969244e-06, 'epoch': 4.55}
{'loss': 0.0011, 'grad_norm': 0.04820537567138672, 'learning_rate': 5.164844558612131e-07, 'epoch': 4.69}
{'loss': 0.0012, 'grad_norm': 0.009317612275481224, 'learning_rate': 1.4206704684953943e-07, 'epoch': 4.83}
{'loss': 0.0008, 'grad_norm': 0.011186467483639717, 'learning_rate': 1.1752123193459197e-09, 'epoch': 4.97}
{'train_runtime': 1733.9381, 'train_samples_per_second': 1.672, 'train_steps_per_second': 0.208, 'train_loss': 0.06322533228538102, 'epoch': 4.97}
***** train metrics *****
  epoch                    =     4.9655
  total_flos               = 55401222GF
  train_loss               =     0.0632
  train_runtime            = 0:28:53.93
  train_samples_per_second =      1.672
  train_steps_per_second   =      0.208
09/09/2024 16:50:56 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 16:50:56 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 16:50:56 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 16:50:56 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/09/2024 16:51:12 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 16:51:14 - INFO - llamafactory.model.adapter - Loaded adapter(s): /nfs/scratch/umushtaq/coling_2025/cdcp/finetuned_models/CDCP_acc_Meta-Llama-3.1-8B-Instruct-bnb-4bit
09/09/2024 16:51:14 - INFO - llamafactory.model.loader - all params: 8,051,232,768
              precision    recall  f1-score   support

        fact      0.598     0.811     0.688       132
      policy      0.916     0.922     0.919       153
   reference      1.000     1.000     1.000         1
   testimony      0.922     0.869     0.895       244
       value      0.896     0.835     0.864       496

    accuracy                          0.853      1026
   macro avg      0.866     0.887     0.873      1026
weighted avg      0.867     0.853     0.857      1026

Successfully ran CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit acc 
 
  *************** 

Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit ari
09/09/2024 16:56:14 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/09/2024 16:56:14 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16
09/09/2024 16:56:15 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 16:56:15 - INFO - llamafactory.data.loader - Loading dataset /nfs/scratch/umushtaq/coling_2025/cdcp/datasets/CDCP_ari_train.json...
09/09/2024 16:56:18 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 16:56:18 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 16:56:18 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 16:56:18 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/09/2024 16:56:31 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
Error encountered with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit ari. Skipping to the next pair. 
 
  ************* 

Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit arc
09/09/2024 16:57:40 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/09/2024 16:57:40 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16
09/09/2024 16:57:42 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 16:57:42 - INFO - llamafactory.data.loader - Loading dataset /nfs/scratch/umushtaq/coling_2025/cdcp/datasets/CDCP_arc_train.json...
09/09/2024 16:57:44 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 16:57:44 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 16:57:44 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 16:57:44 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/09/2024 16:57:59 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit ari
09/09/2024 17:01:28 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/09/2024 17:01:28 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16
09/09/2024 17:01:29 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 17:01:29 - INFO - llamafactory.data.loader - Loading dataset /nfs/scratch/umushtaq/coling_2025/cdcp/datasets/CDCP_ari_train.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 10765, 5811, 4398, 1990, 5811, 6956, 304, 279, 1495, 13, 1472, 2011, 471, 264, 1160, 315, 13840, 304, 279, 2768, 4823, 3645, 25, 5324, 1638, 9202, 95321, 794, 4416, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 26090, 61453, 510, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 8, 5163, 633, 14711, 5810, 374, 279, 1495, 25, 366, 1741, 15, 29, 1423, 323, 2254, 5590, 5718, 7170, 1304, 1670, 59358, 1790, 810, 4461, 4005, 1741, 15, 1822, 1741, 16, 29, 1789, 3187, 11, 994, 264, 1732, 889, 19755, 69944, 264, 11897, 374, 3309, 311, 2586, 311, 5590, 389, 264, 990, 1938, 11, 814, 1253, 387, 9770, 311, 5268, 1990, 264, 1670, 19971, 323, 872, 2683, 4005, 1741, 16, 1822, 1741, 17, 29, 358, 33147, 279, 356, 11960, 33, 311, 1505, 12659, 430, 21736, 38952, 45348, 520, 84783, 3115, 28743, 11, 81374, 11, 323, 43658, 11, 477, 40240, 449, 220, 11739, 17, 72, 4005, 1741, 17, 29, 128009, 128006, 78191, 128007, 271, 5018, 1638, 9202, 95321, 794, 4416, 15, 11, 220, 16, 1145, 510, 17, 11, 220, 15, 5163, 92, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to identify argument relations between argument components in the text. You must return a list of pairs in the following JSON format: {"list_argument_relations": [[source AC (int), target AC (int)],..., [source AC (int), target AC (int)]]}

### Here is the text: <AC0>State and local court rules sometimes make default judgments much more likely.</AC0><AC1> For example, when a person who allegedly owes a debt is told to come to court on a work day, they may be forced to choose between a default judgment and their job.</AC1><AC2> I urge the CFPB to find practices that involve scheduling hearings at inconvenient times unfair, deceptive, and abusive, or inconsistent with 1692i.</AC2><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"list_argument_relations": [[0, 1], [2, 0]]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 1638, 9202, 95321, 794, 4416, 15, 11, 220, 16, 1145, 510, 17, 11, 220, 15, 5163, 92, 128009]
labels:
{"list_argument_relations": [[0, 1], [2, 0]]}<|eot_id|>
09/09/2024 17:01:31 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 17:01:31 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 17:01:51 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/09/2024 17:01:51 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 17:01:51 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/09/2024 17:01:51 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/09/2024 17:01:51 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,gate_proj,q_proj,down_proj,o_proj,k_proj,up_proj
09/09/2024 17:01:51 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/09/2024 17:01:54 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.7803, 'grad_norm': 1.0735604763031006, 'learning_rate': 1.388888888888889e-05, 'epoch': 0.14}
{'loss': 0.3652, 'grad_norm': 0.6414270401000977, 'learning_rate': 2.777777777777778e-05, 'epoch': 0.28}
{'loss': 0.3144, 'grad_norm': 0.518515944480896, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.41}
{'loss': 0.311, 'grad_norm': 0.6451265215873718, 'learning_rate': 4.998119881260576e-05, 'epoch': 0.55}
{'loss': 0.2741, 'grad_norm': 0.9537454843521118, 'learning_rate': 4.977001008412113e-05, 'epoch': 0.69}
{'loss': 0.2538, 'grad_norm': 0.8892354965209961, 'learning_rate': 4.9326121764495596e-05, 'epoch': 0.83}
{'loss': 0.2448, 'grad_norm': 1.4204826354980469, 'learning_rate': 4.8653703921893766e-05, 'epoch': 0.97}
{'loss': 0.2241, 'grad_norm': 1.4557172060012817, 'learning_rate': 4.775907352415367e-05, 'epoch': 1.1}
{'loss': 0.2226, 'grad_norm': 1.1256192922592163, 'learning_rate': 4.665063509461097e-05, 'epoch': 1.24}
{'loss': 0.209, 'grad_norm': 0.8587766885757446, 'learning_rate': 4.533880175657419e-05, 'epoch': 1.38}
{'loss': 0.213, 'grad_norm': 1.1827936172485352, 'learning_rate': 4.3835897408191516e-05, 'epoch': 1.52}
{'loss': 0.1693, 'grad_norm': 0.7358537912368774, 'learning_rate': 4.215604094671835e-05, 'epoch': 1.66}
{'loss': 0.1968, 'grad_norm': 0.764636218547821, 'learning_rate': 4.0315013629830076e-05, 'epoch': 1.79}
{'loss': 0.1837, 'grad_norm': 0.7800518274307251, 'learning_rate': 3.8330110820042285e-05, 'epoch': 1.93}
{'loss': 0.1737, 'grad_norm': 1.4299943447113037, 'learning_rate': 3.621997950501156e-05, 'epoch': 2.07}
{'loss': 0.1156, 'grad_norm': 1.329530954360962, 'learning_rate': 3.400444312011776e-05, 'epoch': 2.21}
{'loss': 0.0864, 'grad_norm': 1.60312819480896, 'learning_rate': 3.170431531901594e-05, 'epoch': 2.34}
{'loss': 0.1076, 'grad_norm': 1.6343026161193848, 'learning_rate': 2.9341204441673266e-05, 'epoch': 2.48}
{'loss': 0.1275, 'grad_norm': 1.3222333192825317, 'learning_rate': 2.6937310516798275e-05, 'epoch': 2.62}
{'loss': 0.1167, 'grad_norm': 0.6892049908638, 'learning_rate': 2.4515216705704395e-05, 'epoch': 2.76}
{'loss': 0.0997, 'grad_norm': 0.9824263453483582, 'learning_rate': 2.2097677146869242e-05, 'epoch': 2.9}
{'loss': 0.0958, 'grad_norm': 0.926001250743866, 'learning_rate': 1.970740319426474e-05, 'epoch': 3.03}
{'loss': 0.053, 'grad_norm': 1.7916696071624756, 'learning_rate': 1.7366850057622175e-05, 'epoch': 3.17}
{'loss': 0.0563, 'grad_norm': 2.0999162197113037, 'learning_rate': 1.509800584902108e-05, 'epoch': 3.31}
{'loss': 0.0412, 'grad_norm': 0.7480193972587585, 'learning_rate': 1.2922185017584037e-05, 'epoch': 3.45}
{'loss': 0.0458, 'grad_norm': 0.7688692808151245, 'learning_rate': 1.085982811283654e-05, 'epoch': 3.59}
{'loss': 0.0284, 'grad_norm': 1.2666606903076172, 'learning_rate': 8.930309757836517e-06, 'epoch': 3.72}
{'loss': 0.0441, 'grad_norm': 0.7259955406188965, 'learning_rate': 7.1517566360525284e-06, 'epoch': 3.86}
{'loss': 0.0371, 'grad_norm': 0.7327691912651062, 'learning_rate': 5.5408772018959995e-06, 'epoch': 4.0}
{'loss': 0.0218, 'grad_norm': 0.33387377858161926, 'learning_rate': 4.112804714676594e-06, 'epoch': 4.14}
{'loss': 0.0098, 'grad_norm': 0.3461725413799286, 'learning_rate': 2.8809550705835548e-06, 'epoch': 4.28}
{'loss': 0.0113, 'grad_norm': 1.2537726163864136, 'learning_rate': 1.8569007682777417e-06, 'epoch': 4.41}
{'loss': 0.0154, 'grad_norm': 0.8658413887023926, 'learning_rate': 1.0502621921127776e-06, 'epoch': 4.55}
{'loss': 0.0071, 'grad_norm': 0.3286372125148773, 'learning_rate': 4.6861723431538276e-07, 'epoch': 4.69}
{'loss': 0.0117, 'grad_norm': 0.18836911022663116, 'learning_rate': 1.1743010517085428e-07, 'epoch': 4.83}
{'loss': 0.0141, 'grad_norm': 0.5091426968574524, 'learning_rate': 0.0, 'epoch': 4.97}
{'train_runtime': 1534.3693, 'train_samples_per_second': 1.89, 'train_steps_per_second': 0.235, 'train_loss': 0.14673041419850455, 'epoch': 4.97}
***** train metrics *****
  epoch                    =     4.9655
  total_flos               = 44833837GF
  train_loss               =     0.1467
  train_runtime            = 0:25:34.36
  train_samples_per_second =       1.89
  train_steps_per_second   =      0.235
09/09/2024 17:27:53 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 17:27:54 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 17:27:54 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 17:27:54 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/09/2024 17:28:11 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 17:28:13 - INFO - llamafactory.model.adapter - Loaded adapter(s): /nfs/scratch/umushtaq/coling_2025/cdcp/finetuned_models/CDCP_ari_Meta-Llama-3.1-8B-Instruct-bnb-4bit
09/09/2024 17:28:13 - INFO - llamafactory.model.loader - all params: 8,051,232,768
              precision    recall  f1-score   support

       N-Rel      0.981     0.987     0.984     10004
         Rel      0.511     0.420     0.461       324

    accuracy                          0.969     10328
   macro avg      0.746     0.703     0.723     10328
weighted avg      0.967     0.969     0.968     10328

Successfully ran CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit ari 
 
  *************** 

Running CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit arc
09/09/2024 17:32:06 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/09/2024 17:32:06 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16
09/09/2024 17:32:08 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 17:32:08 - INFO - llamafactory.data.loader - Loading dataset /nfs/scratch/umushtaq/coling_2025/cdcp/datasets/CDCP_arc_train.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 1472, 527, 1101, 2728, 264, 1160, 315, 13840, 315, 5552, 5811, 6956, 304, 279, 1376, 25, 18305, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 5850, 320, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 5850, 61453, 320, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 595, 948, 4718, 3465, 374, 311, 49229, 1855, 6857, 315, 5552, 5811, 6956, 304, 279, 1160, 439, 3060, 330, 20489, 1, 477, 330, 68, 28580, 3343, 1472, 2011, 471, 264, 1160, 315, 5811, 12976, 4595, 11, 26549, 315, 3160, 220, 17, 11, 304, 2768, 4823, 3645, 25, 5324, 23013, 9962, 794, 4482, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 8, 93546, 1405, 1855, 2449, 330, 23013, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 20489, 1, 477, 330, 68, 28580, 3343, 4815, 14711, 5810, 374, 279, 1495, 25, 366, 1741, 16, 29, 1423, 323, 2254, 5590, 5718, 7170, 1304, 1670, 59358, 1790, 810, 4461, 4005, 1741, 16, 1822, 1741, 17, 29, 1789, 3187, 11, 994, 264, 1732, 889, 19755, 69944, 264, 11897, 374, 3309, 311, 2586, 311, 5590, 389, 264, 990, 1938, 11, 814, 1253, 387, 9770, 311, 5268, 1990, 264, 1670, 19971, 323, 872, 2683, 4005, 1741, 17, 1822, 1741, 18, 29, 358, 33147, 279, 356, 11960, 33, 311, 1505, 12659, 430, 21736, 38952, 45348, 520, 84783, 3115, 28743, 11, 81374, 11, 323, 43658, 11, 477, 40240, 449, 220, 11739, 17, 72, 4005, 1741, 18, 397, 14711, 5810, 374, 279, 1160, 315, 13840, 315, 5552, 5811, 6956, 304, 420, 14646, 25, 4416, 15, 11, 220, 16, 1145, 510, 17, 11, 220, 15, 5163, 128009, 128006, 78191, 128007, 271, 5018, 23013, 9962, 794, 4482, 20489, 498, 330, 20489, 93546, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. You are also given a list of pairs of related argument components in the form: [(target AC (int), source AC (int)), (target AC (int), source AC (int)),..., (target AC (int), source AC (int))]. Your task is to classify each pair of related argument components in the list as either "reason" or "evidence". You must return a list of argument relation types, strictly of length 2, in following JSON format: {"relation_types": ["component_type (str)", "component_type (str)"]} where each element "relation_type (str)" is replaced by either "reason" or "evidence". 

### Here is the text: <AC1>State and local court rules sometimes make default judgments much more likely.</AC1><AC2> For example, when a person who allegedly owes a debt is told to come to court on a work day, they may be forced to choose between a default judgment and their job.</AC2><AC3> I urge the CFPB to find practices that involve scheduling hearings at inconvenient times unfair, deceptive, and abusive, or inconsistent with 1692i.</AC3>
### Here is the list of pairs of related argument components in this paragraph: [[0, 1], [2, 0]]<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"relation_types": ["reason", "reason"]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 23013, 9962, 794, 4482, 20489, 498, 330, 20489, 93546, 128009]
labels:
{"relation_types": ["reason", "reason"]}<|eot_id|>
09/09/2024 17:32:09 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 17:32:09 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 17:32:27 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/09/2024 17:32:27 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 17:32:27 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/09/2024 17:32:27 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/09/2024 17:32:27 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,up_proj,v_proj,o_proj,q_proj,k_proj,gate_proj
09/09/2024 17:32:28 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/09/2024 17:32:30 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.6167, 'grad_norm': 0.0602872297167778, 'learning_rate': 1.1111111111111112e-05, 'epoch': 0.14}
{'loss': 0.034, 'grad_norm': 0.24583642184734344, 'learning_rate': 2.5e-05, 'epoch': 0.28}
{'loss': 0.0137, 'grad_norm': 0.015072855167090893, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.41}
{'loss': 0.0114, 'grad_norm': 0.030162272974848747, 'learning_rate': 4.9995299261212536e-05, 'epoch': 0.55}
{'loss': 0.0325, 'grad_norm': 0.15331164002418518, 'learning_rate': 4.983095894354858e-05, 'epoch': 0.69}
{'loss': 0.0187, 'grad_norm': 0.09634000808000565, 'learning_rate': 4.94333464562659e-05, 'epoch': 0.83}
{'loss': 0.0241, 'grad_norm': 0.06279981136322021, 'learning_rate': 4.880619713346039e-05, 'epoch': 0.97}
{'loss': 0.0184, 'grad_norm': 1.7344964742660522, 'learning_rate': 4.7955402672006854e-05, 'epoch': 1.1}
{'loss': 0.0132, 'grad_norm': 0.12885020673274994, 'learning_rate': 4.6888955782552274e-05, 'epoch': 1.24}
{'loss': 0.012, 'grad_norm': 0.031063703820109367, 'learning_rate': 4.561687510272767e-05, 'epoch': 1.38}
{'loss': 0.0291, 'grad_norm': 0.09116984158754349, 'learning_rate': 4.415111107797445e-05, 'epoch': 1.52}
{'loss': 0.0109, 'grad_norm': 0.042053233832120895, 'learning_rate': 4.2505433694179216e-05, 'epoch': 1.66}
{'loss': 0.0074, 'grad_norm': 0.33995646238327026, 'learning_rate': 4.069530311680247e-05, 'epoch': 1.79}
{'loss': 0.0032, 'grad_norm': 0.0002861966786440462, 'learning_rate': 3.873772445177015e-05, 'epoch': 1.93}
{'loss': 0.0034, 'grad_norm': 0.00017895092605613172, 'learning_rate': 3.665108799256348e-05, 'epoch': 2.07}
{'loss': 0.0304, 'grad_norm': 0.34021231532096863, 'learning_rate': 3.445499645429107e-05, 'epoch': 2.21}
{'loss': 0.003, 'grad_norm': 0.0016262341523543, 'learning_rate': 3.217008081777726e-05, 'epoch': 2.34}
{'loss': 0.0197, 'grad_norm': 0.1969086229801178, 'learning_rate': 2.9817806513702244e-05, 'epoch': 2.48}
{'loss': 0.0012, 'grad_norm': 0.009071371518075466, 'learning_rate': 2.742027176757948e-05, 'epoch': 2.62}
{'loss': 0.011, 'grad_norm': 0.2617630660533905, 'learning_rate': 2.5e-05, 'epoch': 2.76}
{'loss': 0.0075, 'grad_norm': 0.09403780102729797, 'learning_rate': 2.2579728232420525e-05, 'epoch': 2.9}
{'loss': 0.0117, 'grad_norm': 0.14639873802661896, 'learning_rate': 2.0182193486297755e-05, 'epoch': 3.03}
{'loss': 0.0045, 'grad_norm': 0.015193162485957146, 'learning_rate': 1.7829919182222752e-05, 'epoch': 3.17}
{'loss': 0.0014, 'grad_norm': 0.008626030758023262, 'learning_rate': 1.554500354570894e-05, 'epoch': 3.31}
{'loss': 0.0008, 'grad_norm': 0.01137516275048256, 'learning_rate': 1.3348912007436537e-05, 'epoch': 3.45}
{'loss': 0.0025, 'grad_norm': 0.009202866815030575, 'learning_rate': 1.126227554822985e-05, 'epoch': 3.59}
{'loss': 0.0002, 'grad_norm': 0.0024448418989777565, 'learning_rate': 9.304696883197542e-06, 'epoch': 3.72}
{'loss': 0.0071, 'grad_norm': 0.008065409027040005, 'learning_rate': 7.494566305820788e-06, 'epoch': 3.86}
{'loss': 0.0025, 'grad_norm': 0.24935542047023773, 'learning_rate': 5.848888922025553e-06, 'epoch': 4.0}
{'loss': 0.0015, 'grad_norm': 0.01195203885436058, 'learning_rate': 4.383124897272331e-06, 'epoch': 4.14}
{'loss': 0.0005, 'grad_norm': 0.008009125478565693, 'learning_rate': 3.111044217447731e-06, 'epoch': 4.28}
{'loss': 0.0005, 'grad_norm': 0.00440540537238121, 'learning_rate': 2.044597327993153e-06, 'epoch': 4.41}
{'loss': 0.0004, 'grad_norm': 0.014145094901323318, 'learning_rate': 1.1938028665396173e-06, 'epoch': 4.55}
{'loss': 0.0008, 'grad_norm': 0.004925817251205444, 'learning_rate': 5.666535437341108e-07, 'epoch': 4.69}
{'loss': 0.0006, 'grad_norm': 0.05764344707131386, 'learning_rate': 1.6904105645142444e-07, 'epoch': 4.83}
{'loss': 0.0015, 'grad_norm': 0.2576037645339966, 'learning_rate': 4.700738787466463e-09, 'epoch': 4.97}
{'train_runtime': 1746.2721, 'train_samples_per_second': 1.661, 'train_steps_per_second': 0.206, 'train_loss': 0.02660752696812981, 'epoch': 4.97}
***** train metrics *****
  epoch                    =     4.9655
  total_flos               = 56202608GF
  train_loss               =     0.0266
  train_runtime            = 0:29:06.27
  train_samples_per_second =      1.661
  train_steps_per_second   =      0.206
09/09/2024 18:02:02 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 18:02:02 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 18:02:02 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 18:02:02 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/09/2024 18:02:16 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 18:02:17 - INFO - llamafactory.model.adapter - Loaded adapter(s): /nfs/scratch/umushtaq/coling_2025/cdcp/finetuned_models/CDCP_arc_Meta-Llama-3.1-8B-Instruct-bnb-4bit
09/09/2024 18:02:18 - INFO - llamafactory.model.loader - all params: 8,051,232,768
              precision    recall  f1-score   support

    evidence      1.000     0.462     0.632        26
      reason      0.955     1.000     0.977       298

    accuracy                          0.957       324
   macro avg      0.978     0.731     0.804       324
weighted avg      0.959     0.957     0.949       324

Successfully ran CDCP_finetune.py with arguments: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit arc 
 
  *************** 

