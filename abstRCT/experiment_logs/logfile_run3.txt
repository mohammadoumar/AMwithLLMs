Running abstRCT_finetune.py with arguments: unsloth/llama-3-8b-Instruct-bnb-4bit acc
Error encountered with arguments: unsloth/llama-3-8b-Instruct-bnb-4bit acc. Skipping to the next pair. 
 
  ************* 

Running abstRCT_finetune.py with arguments: unsloth/llama-3-8b-Instruct-bnb-4bit aric
Error encountered with arguments: unsloth/llama-3-8b-Instruct-bnb-4bit aric. Skipping to the next pair. 
 
  ************* 

Running abstRCT_finetune.py with arguments: unsloth/llama-3-8b-Instruct acc
Error encountered with arguments: unsloth/llama-3-8b-Instruct acc. Skipping to the next pair. 
 
  ************* 

Running abstRCT_finetune.py with arguments: unsloth/llama-3-8b-Instruct aric
Error encountered with arguments: unsloth/llama-3-8b-Instruct aric. Skipping to the next pair. 
 
  ************* 

Running abstRCT_finetune.py with arguments: unsloth/llama-3-70b-Instruct-bnb-4bit acc
Error encountered with arguments: unsloth/llama-3-70b-Instruct-bnb-4bit acc. Skipping to the next pair. 
 
  ************* 

Running abstRCT_finetune.py with arguments: unsloth/llama-3-70b-Instruct-bnb-4bit aric
Error encountered with arguments: unsloth/llama-3-70b-Instruct-bnb-4bit aric. Skipping to the next pair. 
 
  ************* 

Running abstRCT_finetune.py with arguments: unsloth/llama-3-8b-Instruct-bnb-4bit acc
09/09/2024 01:17:02 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:26199
09/09/2024 01:17:12 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/09/2024 01:17:12 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/09/2024 01:17:12 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/09/2024 01:17:12 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/09/2024 01:17:12 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/09/2024 01:17:12 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/09/2024 01:17:13 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 01:17:13 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 01:17:13 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_acc_train_neo.json...
09/09/2024 01:17:17 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_acc_train_neo.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 49229, 1855, 5811, 3777, 304, 279, 1495, 439, 3060, 330, 46644, 1, 477, 330, 42562, 1082, 3343, 1472, 2011, 471, 264, 1160, 315, 5811, 3777, 4595, 11, 26549, 315, 3160, 220, 23, 11, 304, 2768, 4823, 3645, 25, 5324, 8739, 9962, 794, 4482, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 8, 93546, 1405, 1855, 2449, 330, 8739, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 46644, 1, 477, 330, 42562, 1082, 3343, 4815, 14711, 5810, 374, 279, 8278, 1495, 25, 220, 11579, 43035, 15419, 449, 293, 950, 332, 66796, 11, 264, 2536, 3751, 71916, 7294, 438, 26252, 11, 574, 7863, 449, 2211, 55681, 11, 3060, 34933, 477, 6593, 11, 304, 6978, 449, 83920, 22891, 423, 17, 47447, 9572, 13, 763, 459, 1825, 11, 47341, 11, 92520, 1992, 9269, 11, 6978, 1051, 47341, 311, 6514, 449, 220, 1135, 14060, 293, 950, 332, 66796, 320, 77, 284, 220, 14052, 8, 3131, 7446, 477, 311, 2211, 55681, 320, 77, 284, 220, 14052, 705, 3060, 477, 14946, 72783, 477, 45719, 26127, 315, 342, 24332, 33830, 65802, 349, 1475, 220, 1591, 2919, 13, 26150, 41265, 37442, 1051, 3115, 311, 6514, 8060, 323, 16945, 8624, 33824, 323, 20237, 13, 82935, 1392, 5343, 3477, 315, 66303, 68370, 2315, 11, 47447, 15696, 11, 18516, 86805, 77854, 2508, 5856, 5178, 2704, 11, 6784, 11, 3260, 4282, 292, 8670, 11, 323, 4367, 315, 2324, 14847, 13, 578, 23369, 8250, 315, 15419, 574, 220, 2137, 5672, 369, 293, 950, 332, 66796, 88186, 6978, 323, 220, 2983, 5672, 369, 2211, 496, 660, 6978, 26, 6514, 8060, 10222, 304, 220, 4331, 4, 323, 220, 2983, 4, 323, 8624, 33824, 304, 220, 3391, 4, 323, 220, 1644, 13689, 15947, 16134, 1741, 16, 29, 31969, 6372, 46603, 2211, 55681, 369, 2225, 37442, 320, 47, 366, 477, 284, 220, 15, 13, 6726, 705, 449, 31397, 42338, 320, 65, 950, 332, 66796, 25, 936, 55681, 8, 315, 220, 16, 13, 4370, 320, 2721, 4, 12410, 10074, 510, 11487, 1145, 220, 16, 13, 972, 311, 220, 17, 13, 410, 8, 369, 892, 311, 6514, 8060, 323, 220, 16, 13, 21, 320, 2721, 4, 21351, 11, 220, 16, 13, 777, 311, 220, 17, 13, 868, 8, 369, 892, 311, 8624, 33824, 13, 694, 1741, 16, 1822, 1741, 17, 29, 5659, 279, 220, 16, 4771, 20237, 6492, 11, 279, 31397, 11595, 369, 19463, 315, 4648, 574, 220, 16, 13, 1682, 320, 2721, 4, 21351, 11, 220, 15, 13, 4161, 311, 220, 16, 13, 5332, 570, 694, 1741, 17, 1822, 1741, 18, 29, 14636, 3117, 11, 449, 264, 23369, 1833, 5352, 315, 220, 4218, 5672, 11, 23369, 20237, 706, 539, 1027, 8813, 304, 3060, 1912, 13, 694, 1741, 18, 1822, 1741, 19, 29, 29240, 505, 26954, 304, 3892, 4367, 315, 2324, 7482, 1051, 12207, 2204, 320, 47, 366, 477, 284, 220, 15, 13, 1721, 8, 1990, 6514, 5315, 48582, 505, 4038, 220, 16, 311, 220, 21, 11, 323, 682, 46603, 293, 950, 332, 66796, 13, 694, 1741, 19, 1822, 1741, 20, 29, 28993, 11, 279, 7294, 438, 26252, 574, 1664, 66441, 7863, 449, 2211, 55681, 26, 694, 1741, 20, 1822, 1741, 21, 29, 449, 293, 950, 332, 66796, 11, 4106, 18698, 288, 10222, 2753, 3629, 323, 17659, 8541, 29668, 323, 342, 1910, 66274, 561, 689, 810, 3629, 13, 694, 1741, 21, 1822, 1741, 22, 29, 10541, 264, 47040, 315, 220, 1135, 14060, 315, 293, 950, 332, 66796, 3131, 7446, 574, 539, 439, 7524, 439, 2211, 55681, 11, 694, 1741, 22, 1822, 1741, 23, 29, 279, 37849, 4367, 315, 2324, 20124, 323, 279, 3428, 39775, 315, 2536, 71, 494, 25180, 31959, 4455, 3493, 8125, 311, 15806, 293, 950, 332, 66796, 11, 439, 264, 3254, 37471, 8479, 11, 520, 5190, 35130, 13, 694, 1741, 23, 29, 128009, 128006, 78191, 128007, 271, 5018, 8739, 9962, 794, 4482, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 46644, 93546, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to classify each argument component in the text as either "Claim" or "Premise". You must return a list of argument component types, strictly of length 8, in following JSON format: {"component_types": ["component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)"]} where each element "component_type (str)" is replaced by either "Claim" or "Premise". 

### Here is the abstract text:  Single-agent therapy with bicalutamide, a nonsteroidal antiandrogen, was compared with castration, either surgical or medical, in patients with untreated Stage D2 prostate cancer. In an open, randomized, multicenter trial, patients were randomized to treatment with 50 mg bicalutamide (n = 243) once daily or to castration (n = 243), either orchiectomy or depot injection of goserelin acetate every 28 days. Primary efficacy endpoints were times to treatment failure and objective disease progression and survival. Assessments included review of measurable metastases, prostate dimensions, Eastern Cooperative Oncology Group performance status, pain, analgesic requirements, and quality of life responses. The median duration of therapy was 39 weeks for bicalutamide-treated patients and 42 weeks for castrated patients; treatment failure occurred in 53% and 42% and disease progression in 43% and 33%, respectively.<AC1> Treatment effects favored castration for both endpoints (P < or = 0.002), with hazard ratios (bicalutamide:castration) of 1.54 (95% confidence interval [CI], 1.18 to 2.00) for time to treatment failure and 1.6 (95% CI, 1.19 to 2.15) for time to disease progression. </AC1><AC2> From the 1-year survival analysis, the hazard ratio for probability of death was 1.29 (95% CI, 0.96 to 1.72). </AC2><AC3> Thus far, with a median follow-up of 86 weeks, median survival has not been reached in either group. </AC3><AC4> Changes from baseline in several quality of life variables were significantly different (P < or = 0.01) between treatment groups periodically from months 1 to 6, and all favored bicalutamide. </AC4><AC5> Overall, the antiandrogen was well tolerated compared with castration; </AC5><AC6> with bicalutamide, hot flushes occurred less often and breast tenderness and gynecomastia more often. </AC6><AC7> Although a dosage of 50 mg of bicalutamide once daily was not as effective as castration, </AC7><AC8> the favorable quality of life outcomes and the low incidence of nonhormonal adverse events provide reasons to evaluate bicalutamide, as a single therapeutic agent, at higher doses. </AC8><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"component_types": ["Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Claim"]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 8739, 9962, 794, 4482, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 46644, 93546, 128009]
labels:
{"component_types": ["Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Claim"]}<|eot_id|>
09/09/2024 01:17:23 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 01:17:23 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 01:17:23 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 01:17:23 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 01:17:29 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/09/2024 01:17:29 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 01:17:29 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/09/2024 01:17:29 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/09/2024 01:17:29 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,k_proj,up_proj,gate_proj,q_proj,down_proj,o_proj
09/09/2024 01:17:29 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/09/2024 01:17:29 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 01:17:29 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/09/2024 01:17:29 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/09/2024 01:17:29 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,q_proj,up_proj,gate_proj,k_proj,o_proj,down_proj
09/09/2024 01:17:30 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/09/2024 01:17:30 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/09/2024 01:17:30 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/09/2024 01:17:30 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.2569, 'grad_norm': 0.5198798179626465, 'learning_rate': 4.0909090909090915e-05, 'epoch': 0.45}
{'loss': 0.0674, 'grad_norm': 0.4866596460342407, 'learning_rate': 4.919871753490891e-05, 'epoch': 0.91}
{'loss': 0.0399, 'grad_norm': 1.0112252235412598, 'learning_rate': 4.6031338320779534e-05, 'epoch': 1.36}
{'loss': 0.0251, 'grad_norm': 0.3840089440345764, 'learning_rate': 4.0763816677113064e-05, 'epoch': 1.82}
{'loss': 0.0253, 'grad_norm': 0.17548058927059174, 'learning_rate': 3.392215553979679e-05, 'epoch': 2.27}
{'loss': 0.0171, 'grad_norm': 0.23669496178627014, 'learning_rate': 2.6189547895593562e-05, 'epoch': 2.73}
{'loss': 0.0143, 'grad_norm': 0.08439535647630692, 'learning_rate': 1.8338154657749128e-05, 'epoch': 3.18}
{'loss': 0.01, 'grad_norm': 0.04177205637097359, 'learning_rate': 1.1151998403347244e-05, 'epoch': 3.64}
{'loss': 0.0063, 'grad_norm': 0.040028173476457596, 'learning_rate': 5.348672631430318e-06, 'epoch': 4.09}
{'loss': 0.004, 'grad_norm': 0.08494775742292404, 'learning_rate': 1.5076844803522922e-06, 'epoch': 4.55}
{'loss': 0.0047, 'grad_norm': 0.08834942430257797, 'learning_rate': 1.2586440420372936e-08, 'epoch': 5.0}
{'train_runtime': 684.8741, 'train_samples_per_second': 2.555, 'train_steps_per_second': 0.161, 'train_loss': 0.04281553507528522, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  total_flos               = 53976762GF
  train_loss               =     0.0428
  train_runtime            = 0:11:24.87
  train_samples_per_second =      2.555
  train_steps_per_second   =      0.161
09/09/2024 01:29:05 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 01:29:05 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 01:29:05 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 01:29:05 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/09/2024 01:29:08 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 01:29:08 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/finetuned_models_run3/abstRCT_acc_llama-3-8b-Instruct-bnb-4bit
09/09/2024 01:29:08 - INFO - llamafactory.model.loader - all params: 8,051,232,768
              precision    recall  f1-score   support

       Claim      0.930     0.911     0.921       248
     Premise      0.951     0.962     0.956       443

    accuracy                          0.944       691
   macro avg      0.940     0.936     0.938       691
weighted avg      0.943     0.944     0.943       691

              precision    recall  f1-score   support

       Claim      0.895     0.890     0.892       191
     Premise      0.951     0.953     0.952       424

    accuracy                          0.933       615
   macro avg      0.923     0.921     0.922       615
weighted avg      0.933     0.933     0.933       615

              precision    recall  f1-score   support

       Claim      0.962     0.943     0.952       212
     Premise      0.970     0.980     0.975       397

    accuracy                          0.967       609
   macro avg      0.966     0.962     0.964       609
weighted avg      0.967     0.967     0.967       609

Successfully ran abstRCT_finetune.py with arguments: unsloth/llama-3-8b-Instruct-bnb-4bit acc 
 
  *************** 

Running abstRCT_finetune.py with arguments: unsloth/llama-3-8b-Instruct-bnb-4bit aric
09/09/2024 01:42:11 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:26870
09/09/2024 01:42:21 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/09/2024 01:42:21 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/09/2024 01:42:21 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/09/2024 01:42:21 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/09/2024 01:42:21 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/09/2024 01:42:21 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/09/2024 01:42:22 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 01:42:22 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_aric_train_neo.json...
09/09/2024 01:42:22 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 01:42:28 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_aric_train_neo.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 94612, 8278, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 10765, 5811, 4398, 1990, 5811, 6956, 304, 279, 8278, 1495, 323, 49229, 872, 12976, 955, 439, 3060, 330, 24249, 1, 477, 330, 21208, 3343, 1472, 2011, 471, 264, 1160, 315, 24657, 2641, 304, 279, 2768, 4823, 3645, 25, 5324, 1638, 9202, 48084, 9962, 794, 4416, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 705, 12976, 1857, 320, 496, 26090, 61453, 510, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 705, 12976, 1857, 320, 496, 8, 5163, 92, 1405, 1855, 2449, 330, 23013, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 24249, 1, 477, 330, 21208, 11690, 14711, 5810, 374, 279, 8278, 1495, 25, 220, 11579, 43035, 15419, 449, 293, 950, 332, 66796, 11, 264, 2536, 3751, 71916, 7294, 438, 26252, 11, 574, 7863, 449, 2211, 55681, 11, 3060, 34933, 477, 6593, 11, 304, 6978, 449, 83920, 22891, 423, 17, 47447, 9572, 13, 763, 459, 1825, 11, 47341, 11, 92520, 1992, 9269, 11, 6978, 1051, 47341, 311, 6514, 449, 220, 1135, 14060, 293, 950, 332, 66796, 320, 77, 284, 220, 14052, 8, 3131, 7446, 477, 311, 2211, 55681, 320, 77, 284, 220, 14052, 705, 3060, 477, 14946, 72783, 477, 45719, 26127, 315, 342, 24332, 33830, 65802, 349, 1475, 220, 1591, 2919, 13, 26150, 41265, 37442, 1051, 3115, 311, 6514, 8060, 323, 16945, 8624, 33824, 323, 20237, 13, 82935, 1392, 5343, 3477, 315, 66303, 68370, 2315, 11, 47447, 15696, 11, 18516, 86805, 77854, 2508, 5856, 5178, 2704, 11, 6784, 11, 3260, 4282, 292, 8670, 11, 323, 4367, 315, 2324, 14847, 13, 578, 23369, 8250, 315, 15419, 574, 220, 2137, 5672, 369, 293, 950, 332, 66796, 88186, 6978, 323, 220, 2983, 5672, 369, 2211, 496, 660, 6978, 26, 6514, 8060, 10222, 304, 220, 4331, 4, 323, 220, 2983, 4, 323, 8624, 33824, 304, 220, 3391, 4, 323, 220, 1644, 13689, 15947, 16134, 1741, 16, 29, 31969, 6372, 46603, 2211, 55681, 369, 2225, 37442, 320, 47, 366, 477, 284, 220, 15, 13, 6726, 705, 449, 31397, 42338, 320, 65, 950, 332, 66796, 25, 936, 55681, 8, 315, 220, 16, 13, 4370, 320, 2721, 4, 12410, 10074, 510, 11487, 1145, 220, 16, 13, 972, 311, 220, 17, 13, 410, 8, 369, 892, 311, 6514, 8060, 323, 220, 16, 13, 21, 320, 2721, 4, 21351, 11, 220, 16, 13, 777, 311, 220, 17, 13, 868, 8, 369, 892, 311, 8624, 33824, 13, 694, 1741, 16, 1822, 1741, 17, 29, 5659, 279, 220, 16, 4771, 20237, 6492, 11, 279, 31397, 11595, 369, 19463, 315, 4648, 574, 220, 16, 13, 1682, 320, 2721, 4, 21351, 11, 220, 15, 13, 4161, 311, 220, 16, 13, 5332, 570, 694, 1741, 17, 1822, 1741, 18, 29, 14636, 3117, 11, 449, 264, 23369, 1833, 5352, 315, 220, 4218, 5672, 11, 23369, 20237, 706, 539, 1027, 8813, 304, 3060, 1912, 13, 694, 1741, 18, 1822, 1741, 19, 29, 29240, 505, 26954, 304, 3892, 4367, 315, 2324, 7482, 1051, 12207, 2204, 320, 47, 366, 477, 284, 220, 15, 13, 1721, 8, 1990, 6514, 5315, 48582, 505, 4038, 220, 16, 311, 220, 21, 11, 323, 682, 46603, 293, 950, 332, 66796, 13, 694, 1741, 19, 1822, 1741, 20, 29, 28993, 11, 279, 7294, 438, 26252, 574, 1664, 66441, 7863, 449, 2211, 55681, 26, 694, 1741, 20, 1822, 1741, 21, 29, 449, 293, 950, 332, 66796, 11, 4106, 18698, 288, 10222, 2753, 3629, 323, 17659, 8541, 29668, 323, 342, 1910, 66274, 561, 689, 810, 3629, 13, 694, 1741, 21, 1822, 1741, 22, 29, 10541, 264, 47040, 315, 220, 1135, 14060, 315, 293, 950, 332, 66796, 3131, 7446, 574, 539, 439, 7524, 439, 2211, 55681, 11, 694, 1741, 22, 1822, 1741, 23, 29, 279, 37849, 4367, 315, 2324, 20124, 323, 279, 3428, 39775, 315, 2536, 71, 494, 25180, 31959, 4455, 3493, 8125, 311, 15806, 293, 950, 332, 66796, 11, 439, 264, 3254, 37471, 8479, 11, 520, 5190, 35130, 13, 694, 1741, 23, 29, 128009, 128006, 78191, 128007, 271, 5018, 1638, 9202, 48084, 9962, 794, 4416, 16, 11, 220, 23, 11, 330, 21208, 8073, 510, 17, 11, 220, 23, 11, 330, 24249, 8073, 510, 18, 11, 220, 17, 11, 330, 21208, 8073, 510, 19, 11, 220, 23, 11, 330, 24249, 8073, 510, 20, 11, 220, 23, 11, 330, 24249, 8073, 510, 21, 11, 220, 23, 11, 330, 24249, 8073, 510, 22, 11, 220, 23, 11, 330, 21208, 1365, 14316, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a biomedical abstract text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to identify argument relations between argument components in the abstract text and classify their relation type as either "support" or "attack". You must return a list of triplets in the following JSON format: {"list_argument_relation_types": [[source AC (int), target AC (int), relation_type (str)],..., [source AC (int), target AC (int), relation_type (str)]]} where each element "relation_type (str)" is replaced by either "support" or "attack".

### Here is the abstract text:  Single-agent therapy with bicalutamide, a nonsteroidal antiandrogen, was compared with castration, either surgical or medical, in patients with untreated Stage D2 prostate cancer. In an open, randomized, multicenter trial, patients were randomized to treatment with 50 mg bicalutamide (n = 243) once daily or to castration (n = 243), either orchiectomy or depot injection of goserelin acetate every 28 days. Primary efficacy endpoints were times to treatment failure and objective disease progression and survival. Assessments included review of measurable metastases, prostate dimensions, Eastern Cooperative Oncology Group performance status, pain, analgesic requirements, and quality of life responses. The median duration of therapy was 39 weeks for bicalutamide-treated patients and 42 weeks for castrated patients; treatment failure occurred in 53% and 42% and disease progression in 43% and 33%, respectively.<AC1> Treatment effects favored castration for both endpoints (P < or = 0.002), with hazard ratios (bicalutamide:castration) of 1.54 (95% confidence interval [CI], 1.18 to 2.00) for time to treatment failure and 1.6 (95% CI, 1.19 to 2.15) for time to disease progression. </AC1><AC2> From the 1-year survival analysis, the hazard ratio for probability of death was 1.29 (95% CI, 0.96 to 1.72). </AC2><AC3> Thus far, with a median follow-up of 86 weeks, median survival has not been reached in either group. </AC3><AC4> Changes from baseline in several quality of life variables were significantly different (P < or = 0.01) between treatment groups periodically from months 1 to 6, and all favored bicalutamide. </AC4><AC5> Overall, the antiandrogen was well tolerated compared with castration; </AC5><AC6> with bicalutamide, hot flushes occurred less often and breast tenderness and gynecomastia more often. </AC6><AC7> Although a dosage of 50 mg of bicalutamide once daily was not as effective as castration, </AC7><AC8> the favorable quality of life outcomes and the low incidence of nonhormonal adverse events provide reasons to evaluate bicalutamide, as a single therapeutic agent, at higher doses. </AC8><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"list_argument_relation_types": [[1, 8, "attack"], [2, 8, "support"], [3, 2, "attack"], [4, 8, "support"], [5, 8, "support"], [6, 8, "support"], [7, 8, "attack"]]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 1638, 9202, 48084, 9962, 794, 4416, 16, 11, 220, 23, 11, 330, 21208, 8073, 510, 17, 11, 220, 23, 11, 330, 24249, 8073, 510, 18, 11, 220, 17, 11, 330, 21208, 8073, 510, 19, 11, 220, 23, 11, 330, 24249, 8073, 510, 20, 11, 220, 23, 11, 330, 24249, 8073, 510, 21, 11, 220, 23, 11, 330, 24249, 8073, 510, 22, 11, 220, 23, 11, 330, 21208, 1365, 14316, 128009]
labels:
{"list_argument_relation_types": [[1, 8, "attack"], [2, 8, "support"], [3, 2, "attack"], [4, 8, "support"], [5, 8, "support"], [6, 8, "support"], [7, 8, "attack"]]}<|eot_id|>
09/09/2024 01:42:28 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 01:42:28 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 01:42:28 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 01:42:28 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 01:42:34 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/09/2024 01:42:34 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 01:42:34 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/09/2024 01:42:34 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/09/2024 01:42:34 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,v_proj,q_proj,o_proj,up_proj,down_proj,k_proj
09/09/2024 01:42:34 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/09/2024 01:42:34 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 01:42:34 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/09/2024 01:42:34 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/09/2024 01:42:34 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,k_proj,v_proj,o_proj,down_proj,q_proj,gate_proj
09/09/2024 01:42:35 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/09/2024 01:42:35 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/09/2024 01:42:35 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/09/2024 01:42:35 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.3688, 'grad_norm': 0.3474155068397522, 'learning_rate': 4.0909090909090915e-05, 'epoch': 0.45}
{'loss': 0.1787, 'grad_norm': 0.3548623323440552, 'learning_rate': 4.919871753490891e-05, 'epoch': 0.91}
{'loss': 0.1213, 'grad_norm': 0.3670797049999237, 'learning_rate': 4.6031338320779534e-05, 'epoch': 1.36}
{'loss': 0.1175, 'grad_norm': 0.3594273328781128, 'learning_rate': 4.0763816677113064e-05, 'epoch': 1.82}
{'loss': 0.0871, 'grad_norm': 0.2720778286457062, 'learning_rate': 3.392215553979679e-05, 'epoch': 2.27}
{'loss': 0.0679, 'grad_norm': 0.3677988350391388, 'learning_rate': 2.6189547895593562e-05, 'epoch': 2.73}
{'loss': 0.0568, 'grad_norm': 0.28214821219444275, 'learning_rate': 1.8338154657749128e-05, 'epoch': 3.18}
{'loss': 0.0354, 'grad_norm': 0.22112207114696503, 'learning_rate': 1.1151998403347244e-05, 'epoch': 3.64}
{'loss': 0.0354, 'grad_norm': 0.43844032287597656, 'learning_rate': 5.348672631430318e-06, 'epoch': 4.09}
{'loss': 0.0166, 'grad_norm': 0.2099185436964035, 'learning_rate': 1.5076844803522922e-06, 'epoch': 4.55}
{'loss': 0.0176, 'grad_norm': 0.22958020865917206, 'learning_rate': 1.2586440420372936e-08, 'epoch': 5.0}
{'train_runtime': 685.3584, 'train_samples_per_second': 2.553, 'train_steps_per_second': 0.16, 'train_loss': 0.1002774416045709, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  total_flos               = 54095185GF
  train_loss               =     0.1003
  train_runtime            = 0:11:25.35
  train_samples_per_second =      2.553
  train_steps_per_second   =       0.16
09/09/2024 01:54:09 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 01:54:09 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 01:54:09 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 01:54:09 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/09/2024 01:54:12 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 01:54:12 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/finetuned_models_run3/abstRCT_aric_llama-3-8b-Instruct-bnb-4bit
09/09/2024 01:54:12 - INFO - llamafactory.model.loader - all params: 8,051,232,768
              precision    recall  f1-score   support

        None      0.982     0.975     0.979      6435
      attack      0.545     0.610     0.576        59
     support      0.659     0.725     0.690       360

    accuracy                          0.959      6854
   macro avg      0.729     0.770     0.748      6854
weighted avg      0.961     0.959     0.960      6854

              precision    recall  f1-score   support

        None      0.984     0.970     0.977      5219
      attack      0.433     0.464     0.448        28
     support      0.636     0.782     0.702       317

    accuracy                          0.957      5564
   macro avg      0.685     0.739     0.709      5564
weighted avg      0.962     0.957     0.959      5564

              precision    recall  f1-score   support

        None      0.986     0.969     0.977      5176
      attack      0.438     0.583     0.500        24
     support      0.614     0.784     0.688       296

    accuracy                          0.957      5496
   macro avg      0.679     0.779     0.722      5496
weighted avg      0.964     0.957     0.960      5496

Successfully ran abstRCT_finetune.py with arguments: unsloth/llama-3-8b-Instruct-bnb-4bit aric 
 
  *************** 

Running abstRCT_finetune.py with arguments: unsloth/llama-3-8b-Instruct acc
09/09/2024 02:14:17 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:22456
09/09/2024 02:14:27 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/09/2024 02:14:27 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/09/2024 02:14:27 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/09/2024 02:14:27 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/09/2024 02:14:27 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/09/2024 02:14:27 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/09/2024 02:14:27 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 02:14:27 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 02:14:27 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_acc_train_neo.json...
09/09/2024 02:14:33 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_acc_train_neo.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 49229, 1855, 5811, 3777, 304, 279, 1495, 439, 3060, 330, 46644, 1, 477, 330, 42562, 1082, 3343, 1472, 2011, 471, 264, 1160, 315, 5811, 3777, 4595, 11, 26549, 315, 3160, 220, 23, 11, 304, 2768, 4823, 3645, 25, 5324, 8739, 9962, 794, 4482, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 8, 93546, 1405, 1855, 2449, 330, 8739, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 46644, 1, 477, 330, 42562, 1082, 3343, 4815, 14711, 5810, 374, 279, 8278, 1495, 25, 220, 11579, 43035, 15419, 449, 293, 950, 332, 66796, 11, 264, 2536, 3751, 71916, 7294, 438, 26252, 11, 574, 7863, 449, 2211, 55681, 11, 3060, 34933, 477, 6593, 11, 304, 6978, 449, 83920, 22891, 423, 17, 47447, 9572, 13, 763, 459, 1825, 11, 47341, 11, 92520, 1992, 9269, 11, 6978, 1051, 47341, 311, 6514, 449, 220, 1135, 14060, 293, 950, 332, 66796, 320, 77, 284, 220, 14052, 8, 3131, 7446, 477, 311, 2211, 55681, 320, 77, 284, 220, 14052, 705, 3060, 477, 14946, 72783, 477, 45719, 26127, 315, 342, 24332, 33830, 65802, 349, 1475, 220, 1591, 2919, 13, 26150, 41265, 37442, 1051, 3115, 311, 6514, 8060, 323, 16945, 8624, 33824, 323, 20237, 13, 82935, 1392, 5343, 3477, 315, 66303, 68370, 2315, 11, 47447, 15696, 11, 18516, 86805, 77854, 2508, 5856, 5178, 2704, 11, 6784, 11, 3260, 4282, 292, 8670, 11, 323, 4367, 315, 2324, 14847, 13, 578, 23369, 8250, 315, 15419, 574, 220, 2137, 5672, 369, 293, 950, 332, 66796, 88186, 6978, 323, 220, 2983, 5672, 369, 2211, 496, 660, 6978, 26, 6514, 8060, 10222, 304, 220, 4331, 4, 323, 220, 2983, 4, 323, 8624, 33824, 304, 220, 3391, 4, 323, 220, 1644, 13689, 15947, 16134, 1741, 16, 29, 31969, 6372, 46603, 2211, 55681, 369, 2225, 37442, 320, 47, 366, 477, 284, 220, 15, 13, 6726, 705, 449, 31397, 42338, 320, 65, 950, 332, 66796, 25, 936, 55681, 8, 315, 220, 16, 13, 4370, 320, 2721, 4, 12410, 10074, 510, 11487, 1145, 220, 16, 13, 972, 311, 220, 17, 13, 410, 8, 369, 892, 311, 6514, 8060, 323, 220, 16, 13, 21, 320, 2721, 4, 21351, 11, 220, 16, 13, 777, 311, 220, 17, 13, 868, 8, 369, 892, 311, 8624, 33824, 13, 694, 1741, 16, 1822, 1741, 17, 29, 5659, 279, 220, 16, 4771, 20237, 6492, 11, 279, 31397, 11595, 369, 19463, 315, 4648, 574, 220, 16, 13, 1682, 320, 2721, 4, 21351, 11, 220, 15, 13, 4161, 311, 220, 16, 13, 5332, 570, 694, 1741, 17, 1822, 1741, 18, 29, 14636, 3117, 11, 449, 264, 23369, 1833, 5352, 315, 220, 4218, 5672, 11, 23369, 20237, 706, 539, 1027, 8813, 304, 3060, 1912, 13, 694, 1741, 18, 1822, 1741, 19, 29, 29240, 505, 26954, 304, 3892, 4367, 315, 2324, 7482, 1051, 12207, 2204, 320, 47, 366, 477, 284, 220, 15, 13, 1721, 8, 1990, 6514, 5315, 48582, 505, 4038, 220, 16, 311, 220, 21, 11, 323, 682, 46603, 293, 950, 332, 66796, 13, 694, 1741, 19, 1822, 1741, 20, 29, 28993, 11, 279, 7294, 438, 26252, 574, 1664, 66441, 7863, 449, 2211, 55681, 26, 694, 1741, 20, 1822, 1741, 21, 29, 449, 293, 950, 332, 66796, 11, 4106, 18698, 288, 10222, 2753, 3629, 323, 17659, 8541, 29668, 323, 342, 1910, 66274, 561, 689, 810, 3629, 13, 694, 1741, 21, 1822, 1741, 22, 29, 10541, 264, 47040, 315, 220, 1135, 14060, 315, 293, 950, 332, 66796, 3131, 7446, 574, 539, 439, 7524, 439, 2211, 55681, 11, 694, 1741, 22, 1822, 1741, 23, 29, 279, 37849, 4367, 315, 2324, 20124, 323, 279, 3428, 39775, 315, 2536, 71, 494, 25180, 31959, 4455, 3493, 8125, 311, 15806, 293, 950, 332, 66796, 11, 439, 264, 3254, 37471, 8479, 11, 520, 5190, 35130, 13, 694, 1741, 23, 29, 128009, 128006, 78191, 128007, 271, 5018, 8739, 9962, 794, 4482, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 46644, 93546, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to classify each argument component in the text as either "Claim" or "Premise". You must return a list of argument component types, strictly of length 8, in following JSON format: {"component_types": ["component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)"]} where each element "component_type (str)" is replaced by either "Claim" or "Premise". 

### Here is the abstract text:  Single-agent therapy with bicalutamide, a nonsteroidal antiandrogen, was compared with castration, either surgical or medical, in patients with untreated Stage D2 prostate cancer. In an open, randomized, multicenter trial, patients were randomized to treatment with 50 mg bicalutamide (n = 243) once daily or to castration (n = 243), either orchiectomy or depot injection of goserelin acetate every 28 days. Primary efficacy endpoints were times to treatment failure and objective disease progression and survival. Assessments included review of measurable metastases, prostate dimensions, Eastern Cooperative Oncology Group performance status, pain, analgesic requirements, and quality of life responses. The median duration of therapy was 39 weeks for bicalutamide-treated patients and 42 weeks for castrated patients; treatment failure occurred in 53% and 42% and disease progression in 43% and 33%, respectively.<AC1> Treatment effects favored castration for both endpoints (P < or = 0.002), with hazard ratios (bicalutamide:castration) of 1.54 (95% confidence interval [CI], 1.18 to 2.00) for time to treatment failure and 1.6 (95% CI, 1.19 to 2.15) for time to disease progression. </AC1><AC2> From the 1-year survival analysis, the hazard ratio for probability of death was 1.29 (95% CI, 0.96 to 1.72). </AC2><AC3> Thus far, with a median follow-up of 86 weeks, median survival has not been reached in either group. </AC3><AC4> Changes from baseline in several quality of life variables were significantly different (P < or = 0.01) between treatment groups periodically from months 1 to 6, and all favored bicalutamide. </AC4><AC5> Overall, the antiandrogen was well tolerated compared with castration; </AC5><AC6> with bicalutamide, hot flushes occurred less often and breast tenderness and gynecomastia more often. </AC6><AC7> Although a dosage of 50 mg of bicalutamide once daily was not as effective as castration, </AC7><AC8> the favorable quality of life outcomes and the low incidence of nonhormonal adverse events provide reasons to evaluate bicalutamide, as a single therapeutic agent, at higher doses. </AC8><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"component_types": ["Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Claim"]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 8739, 9962, 794, 4482, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 46644, 93546, 128009]
labels:
{"component_types": ["Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Claim"]}<|eot_id|>
09/09/2024 02:14:34 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/09/2024 02:14:34 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/09/2024 02:15:06 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/09/2024 02:15:06 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 02:15:06 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/09/2024 02:15:06 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/09/2024 02:15:06 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,q_proj,k_proj,down_proj,v_proj,up_proj,gate_proj
09/09/2024 02:15:06 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/09/2024 02:15:06 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 02:15:06 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/09/2024 02:15:06 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/09/2024 02:15:06 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,k_proj,gate_proj,v_proj,q_proj,o_proj,down_proj
09/09/2024 02:15:06 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/09/2024 02:15:06 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/09/2024 02:15:07 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/09/2024 02:15:07 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.2572, 'grad_norm': 0.5341882109642029, 'learning_rate': 4.0909090909090915e-05, 'epoch': 0.45}
{'loss': 0.0663, 'grad_norm': 0.37465858459472656, 'learning_rate': 4.919871753490891e-05, 'epoch': 0.91}
{'loss': 0.0371, 'grad_norm': 0.9447254538536072, 'learning_rate': 4.6031338320779534e-05, 'epoch': 1.36}
{'loss': 0.0252, 'grad_norm': 0.35517066717147827, 'learning_rate': 4.0763816677113064e-05, 'epoch': 1.82}
{'loss': 0.0261, 'grad_norm': 0.23546360433101654, 'learning_rate': 3.392215553979679e-05, 'epoch': 2.27}
{'loss': 0.0163, 'grad_norm': 0.13429053127765656, 'learning_rate': 2.6189547895593562e-05, 'epoch': 2.73}
{'loss': 0.0161, 'grad_norm': 0.17691729962825775, 'learning_rate': 1.8338154657749128e-05, 'epoch': 3.18}
{'loss': 0.0104, 'grad_norm': 0.20331327617168427, 'learning_rate': 1.1151998403347244e-05, 'epoch': 3.64}
{'loss': 0.0064, 'grad_norm': 0.03097614459693432, 'learning_rate': 5.348672631430318e-06, 'epoch': 4.09}
{'loss': 0.0038, 'grad_norm': 0.07927624881267548, 'learning_rate': 1.5076844803522922e-06, 'epoch': 4.55}
{'loss': 0.0041, 'grad_norm': 0.07859472185373306, 'learning_rate': 1.2586440420372936e-08, 'epoch': 5.0}
{'train_runtime': 601.2468, 'train_samples_per_second': 2.911, 'train_steps_per_second': 0.183, 'train_loss': 0.04264527213844386, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  total_flos               = 53976762GF
  train_loss               =     0.0426
  train_runtime            = 0:10:01.24
  train_samples_per_second =      2.911
  train_steps_per_second   =      0.183
09/09/2024 02:25:14 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 02:25:15 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/09/2024 02:25:15 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/09/2024 02:25:20 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 02:25:21 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/finetuned_models_run3/abstRCT_acc_llama-3-8b-Instruct
09/09/2024 02:25:21 - INFO - llamafactory.model.loader - all params: 8,051,232,768
              precision    recall  f1-score   support

       Claim      0.930     0.911     0.921       248
     Premise      0.951     0.962     0.956       443

    accuracy                          0.944       691
   macro avg      0.940     0.936     0.938       691
weighted avg      0.943     0.944     0.943       691

              precision    recall  f1-score   support

       Claim      0.920     0.901     0.910       191
     Premise      0.956     0.965     0.960       424

    accuracy                          0.945       615
   macro avg      0.938     0.933     0.935       615
weighted avg      0.944     0.945     0.945       615

              precision    recall  f1-score   support

       Claim      0.943     0.943     0.943       212
     Premise      0.970     0.970     0.970       397

    accuracy                          0.961       609
   macro avg      0.957     0.957     0.957       609
weighted avg      0.961     0.961     0.961       609

Successfully ran abstRCT_finetune.py with arguments: unsloth/llama-3-8b-Instruct acc 
 
  *************** 

Running abstRCT_finetune.py with arguments: unsloth/llama-3-8b-Instruct aric
09/09/2024 02:37:55 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:29578
09/09/2024 02:38:05 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/09/2024 02:38:05 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/09/2024 02:38:05 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/09/2024 02:38:05 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/09/2024 02:38:05 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/09/2024 02:38:05 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/09/2024 02:38:05 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 02:38:05 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_aric_train_neo.json...
09/09/2024 02:38:05 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 02:38:06 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_aric_train_neo.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 94612, 8278, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 10765, 5811, 4398, 1990, 5811, 6956, 304, 279, 8278, 1495, 323, 49229, 872, 12976, 955, 439, 3060, 330, 24249, 1, 477, 330, 21208, 3343, 1472, 2011, 471, 264, 1160, 315, 24657, 2641, 304, 279, 2768, 4823, 3645, 25, 5324, 1638, 9202, 48084, 9962, 794, 4416, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 705, 12976, 1857, 320, 496, 26090, 61453, 510, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 705, 12976, 1857, 320, 496, 8, 5163, 92, 1405, 1855, 2449, 330, 23013, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 24249, 1, 477, 330, 21208, 11690, 14711, 5810, 374, 279, 8278, 1495, 25, 220, 11579, 43035, 15419, 449, 293, 950, 332, 66796, 11, 264, 2536, 3751, 71916, 7294, 438, 26252, 11, 574, 7863, 449, 2211, 55681, 11, 3060, 34933, 477, 6593, 11, 304, 6978, 449, 83920, 22891, 423, 17, 47447, 9572, 13, 763, 459, 1825, 11, 47341, 11, 92520, 1992, 9269, 11, 6978, 1051, 47341, 311, 6514, 449, 220, 1135, 14060, 293, 950, 332, 66796, 320, 77, 284, 220, 14052, 8, 3131, 7446, 477, 311, 2211, 55681, 320, 77, 284, 220, 14052, 705, 3060, 477, 14946, 72783, 477, 45719, 26127, 315, 342, 24332, 33830, 65802, 349, 1475, 220, 1591, 2919, 13, 26150, 41265, 37442, 1051, 3115, 311, 6514, 8060, 323, 16945, 8624, 33824, 323, 20237, 13, 82935, 1392, 5343, 3477, 315, 66303, 68370, 2315, 11, 47447, 15696, 11, 18516, 86805, 77854, 2508, 5856, 5178, 2704, 11, 6784, 11, 3260, 4282, 292, 8670, 11, 323, 4367, 315, 2324, 14847, 13, 578, 23369, 8250, 315, 15419, 574, 220, 2137, 5672, 369, 293, 950, 332, 66796, 88186, 6978, 323, 220, 2983, 5672, 369, 2211, 496, 660, 6978, 26, 6514, 8060, 10222, 304, 220, 4331, 4, 323, 220, 2983, 4, 323, 8624, 33824, 304, 220, 3391, 4, 323, 220, 1644, 13689, 15947, 16134, 1741, 16, 29, 31969, 6372, 46603, 2211, 55681, 369, 2225, 37442, 320, 47, 366, 477, 284, 220, 15, 13, 6726, 705, 449, 31397, 42338, 320, 65, 950, 332, 66796, 25, 936, 55681, 8, 315, 220, 16, 13, 4370, 320, 2721, 4, 12410, 10074, 510, 11487, 1145, 220, 16, 13, 972, 311, 220, 17, 13, 410, 8, 369, 892, 311, 6514, 8060, 323, 220, 16, 13, 21, 320, 2721, 4, 21351, 11, 220, 16, 13, 777, 311, 220, 17, 13, 868, 8, 369, 892, 311, 8624, 33824, 13, 694, 1741, 16, 1822, 1741, 17, 29, 5659, 279, 220, 16, 4771, 20237, 6492, 11, 279, 31397, 11595, 369, 19463, 315, 4648, 574, 220, 16, 13, 1682, 320, 2721, 4, 21351, 11, 220, 15, 13, 4161, 311, 220, 16, 13, 5332, 570, 694, 1741, 17, 1822, 1741, 18, 29, 14636, 3117, 11, 449, 264, 23369, 1833, 5352, 315, 220, 4218, 5672, 11, 23369, 20237, 706, 539, 1027, 8813, 304, 3060, 1912, 13, 694, 1741, 18, 1822, 1741, 19, 29, 29240, 505, 26954, 304, 3892, 4367, 315, 2324, 7482, 1051, 12207, 2204, 320, 47, 366, 477, 284, 220, 15, 13, 1721, 8, 1990, 6514, 5315, 48582, 505, 4038, 220, 16, 311, 220, 21, 11, 323, 682, 46603, 293, 950, 332, 66796, 13, 694, 1741, 19, 1822, 1741, 20, 29, 28993, 11, 279, 7294, 438, 26252, 574, 1664, 66441, 7863, 449, 2211, 55681, 26, 694, 1741, 20, 1822, 1741, 21, 29, 449, 293, 950, 332, 66796, 11, 4106, 18698, 288, 10222, 2753, 3629, 323, 17659, 8541, 29668, 323, 342, 1910, 66274, 561, 689, 810, 3629, 13, 694, 1741, 21, 1822, 1741, 22, 29, 10541, 264, 47040, 315, 220, 1135, 14060, 315, 293, 950, 332, 66796, 3131, 7446, 574, 539, 439, 7524, 439, 2211, 55681, 11, 694, 1741, 22, 1822, 1741, 23, 29, 279, 37849, 4367, 315, 2324, 20124, 323, 279, 3428, 39775, 315, 2536, 71, 494, 25180, 31959, 4455, 3493, 8125, 311, 15806, 293, 950, 332, 66796, 11, 439, 264, 3254, 37471, 8479, 11, 520, 5190, 35130, 13, 694, 1741, 23, 29, 128009, 128006, 78191, 128007, 271, 5018, 1638, 9202, 48084, 9962, 794, 4416, 16, 11, 220, 23, 11, 330, 21208, 8073, 510, 17, 11, 220, 23, 11, 330, 24249, 8073, 510, 18, 11, 220, 17, 11, 330, 21208, 8073, 510, 19, 11, 220, 23, 11, 330, 24249, 8073, 510, 20, 11, 220, 23, 11, 330, 24249, 8073, 510, 21, 11, 220, 23, 11, 330, 24249, 8073, 510, 22, 11, 220, 23, 11, 330, 21208, 1365, 14316, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a biomedical abstract text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to identify argument relations between argument components in the abstract text and classify their relation type as either "support" or "attack". You must return a list of triplets in the following JSON format: {"list_argument_relation_types": [[source AC (int), target AC (int), relation_type (str)],..., [source AC (int), target AC (int), relation_type (str)]]} where each element "relation_type (str)" is replaced by either "support" or "attack".

### Here is the abstract text:  Single-agent therapy with bicalutamide, a nonsteroidal antiandrogen, was compared with castration, either surgical or medical, in patients with untreated Stage D2 prostate cancer. In an open, randomized, multicenter trial, patients were randomized to treatment with 50 mg bicalutamide (n = 243) once daily or to castration (n = 243), either orchiectomy or depot injection of goserelin acetate every 28 days. Primary efficacy endpoints were times to treatment failure and objective disease progression and survival. Assessments included review of measurable metastases, prostate dimensions, Eastern Cooperative Oncology Group performance status, pain, analgesic requirements, and quality of life responses. The median duration of therapy was 39 weeks for bicalutamide-treated patients and 42 weeks for castrated patients; treatment failure occurred in 53% and 42% and disease progression in 43% and 33%, respectively.<AC1> Treatment effects favored castration for both endpoints (P < or = 0.002), with hazard ratios (bicalutamide:castration) of 1.54 (95% confidence interval [CI], 1.18 to 2.00) for time to treatment failure and 1.6 (95% CI, 1.19 to 2.15) for time to disease progression. </AC1><AC2> From the 1-year survival analysis, the hazard ratio for probability of death was 1.29 (95% CI, 0.96 to 1.72). </AC2><AC3> Thus far, with a median follow-up of 86 weeks, median survival has not been reached in either group. </AC3><AC4> Changes from baseline in several quality of life variables were significantly different (P < or = 0.01) between treatment groups periodically from months 1 to 6, and all favored bicalutamide. </AC4><AC5> Overall, the antiandrogen was well tolerated compared with castration; </AC5><AC6> with bicalutamide, hot flushes occurred less often and breast tenderness and gynecomastia more often. </AC6><AC7> Although a dosage of 50 mg of bicalutamide once daily was not as effective as castration, </AC7><AC8> the favorable quality of life outcomes and the low incidence of nonhormonal adverse events provide reasons to evaluate bicalutamide, as a single therapeutic agent, at higher doses. </AC8><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"list_argument_relation_types": [[1, 8, "attack"], [2, 8, "support"], [3, 2, "attack"], [4, 8, "support"], [5, 8, "support"], [6, 8, "support"], [7, 8, "attack"]]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 1638, 9202, 48084, 9962, 794, 4416, 16, 11, 220, 23, 11, 330, 21208, 8073, 510, 17, 11, 220, 23, 11, 330, 24249, 8073, 510, 18, 11, 220, 17, 11, 330, 21208, 8073, 510, 19, 11, 220, 23, 11, 330, 24249, 8073, 510, 20, 11, 220, 23, 11, 330, 24249, 8073, 510, 21, 11, 220, 23, 11, 330, 24249, 8073, 510, 22, 11, 220, 23, 11, 330, 21208, 1365, 14316, 128009]
labels:
{"list_argument_relation_types": [[1, 8, "attack"], [2, 8, "support"], [3, 2, "attack"], [4, 8, "support"], [5, 8, "support"], [6, 8, "support"], [7, 8, "attack"]]}<|eot_id|>
09/09/2024 02:38:12 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/09/2024 02:38:12 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/09/2024 02:38:32 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/09/2024 02:38:32 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 02:38:32 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/09/2024 02:38:32 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/09/2024 02:38:32 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,k_proj,v_proj,q_proj,gate_proj,o_proj,down_proj
09/09/2024 02:38:33 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/09/2024 02:38:33 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 02:38:33 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/09/2024 02:38:33 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/09/2024 02:38:33 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,q_proj,k_proj,gate_proj,v_proj,up_proj,o_proj
09/09/2024 02:38:33 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/09/2024 02:38:33 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
09/09/2024 02:38:33 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/09/2024 02:38:34 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.3335, 'grad_norm': 0.3674006462097168, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.45}
{'loss': 0.17, 'grad_norm': 0.35987845063209534, 'learning_rate': 4.898732434036244e-05, 'epoch': 0.91}
{'loss': 0.114, 'grad_norm': 0.30732378363609314, 'learning_rate': 4.559191453574582e-05, 'epoch': 1.36}
{'loss': 0.1111, 'grad_norm': 0.4072730541229248, 'learning_rate': 4.014024217844167e-05, 'epoch': 1.82}
{'loss': 0.087, 'grad_norm': 0.3171519935131073, 'learning_rate': 3.3176699082935545e-05, 'epoch': 2.27}
{'loss': 0.0653, 'grad_norm': 0.33152440190315247, 'learning_rate': 2.5396649095870202e-05, 'epoch': 2.73}
{'loss': 0.0525, 'grad_norm': 0.27777281403541565, 'learning_rate': 1.7576990616793137e-05, 'epoch': 3.18}
{'loss': 0.0329, 'grad_norm': 0.22111812233924866, 'learning_rate': 1.049857726072005e-05, 'epoch': 3.64}
{'loss': 0.0298, 'grad_norm': 0.3576277494430542, 'learning_rate': 4.868243561723535e-06, 'epoch': 4.09}
{'loss': 0.0142, 'grad_norm': 0.14781787991523743, 'learning_rate': 1.248222056476367e-06, 'epoch': 4.55}
{'loss': 0.0154, 'grad_norm': 0.23200595378875732, 'learning_rate': 0.0, 'epoch': 5.0}
{'train_runtime': 601.2614, 'train_samples_per_second': 2.911, 'train_steps_per_second': 0.183, 'train_loss': 0.0932389959692955, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  total_flos               = 54095185GF
  train_loss               =     0.0932
  train_runtime            = 0:10:01.26
  train_samples_per_second =      2.911
  train_steps_per_second   =      0.183
09/09/2024 02:48:42 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 02:48:42 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
09/09/2024 02:48:42 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/09/2024 02:48:48 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 02:48:49 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/finetuned_models_run3/abstRCT_aric_llama-3-8b-Instruct
09/09/2024 02:48:49 - INFO - llamafactory.model.loader - all params: 8,051,232,768
              precision    recall  f1-score   support

        None      0.982     0.976     0.979      6435
      attack      0.660     0.593     0.625        59
     support      0.647     0.728     0.685       360

    accuracy                          0.960      6854
   macro avg      0.763     0.766     0.763      6854
weighted avg      0.962     0.960     0.961      6854

              precision    recall  f1-score   support

        None      0.986     0.971     0.979      5219
      attack      0.429     0.429     0.429        28
     support      0.651     0.808     0.721       317

    accuracy                          0.959      5564
   macro avg      0.689     0.736     0.709      5564
weighted avg      0.964     0.959     0.961      5564

              precision    recall  f1-score   support

        None      0.986     0.970     0.978      5176
      attack      0.560     0.583     0.571        24
     support      0.612     0.787     0.688       296

    accuracy                          0.958      5496
   macro avg      0.719     0.780     0.746      5496
weighted avg      0.964     0.958     0.961      5496

Successfully ran abstRCT_finetune.py with arguments: unsloth/llama-3-8b-Instruct aric 
 
  *************** 

Running abstRCT_finetune.py with arguments: unsloth/llama-3-70b-Instruct-bnb-4bit acc
09/09/2024 03:08:36 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:26319
09/09/2024 03:08:46 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/09/2024 03:08:46 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/09/2024 03:08:46 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/09/2024 03:08:46 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/09/2024 03:08:46 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/09/2024 03:08:46 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/09/2024 03:08:47 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 03:08:47 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_acc_train_neo.json...
09/09/2024 03:08:47 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 03:08:48 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_acc_train_neo.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 49229, 1855, 5811, 3777, 304, 279, 1495, 439, 3060, 330, 46644, 1, 477, 330, 42562, 1082, 3343, 1472, 2011, 471, 264, 1160, 315, 5811, 3777, 4595, 11, 26549, 315, 3160, 220, 23, 11, 304, 2768, 4823, 3645, 25, 5324, 8739, 9962, 794, 4482, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 11844, 330, 8739, 1857, 320, 496, 8, 93546, 1405, 1855, 2449, 330, 8739, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 46644, 1, 477, 330, 42562, 1082, 3343, 4815, 14711, 5810, 374, 279, 8278, 1495, 25, 220, 11579, 43035, 15419, 449, 293, 950, 332, 66796, 11, 264, 2536, 3751, 71916, 7294, 438, 26252, 11, 574, 7863, 449, 2211, 55681, 11, 3060, 34933, 477, 6593, 11, 304, 6978, 449, 83920, 22891, 423, 17, 47447, 9572, 13, 763, 459, 1825, 11, 47341, 11, 92520, 1992, 9269, 11, 6978, 1051, 47341, 311, 6514, 449, 220, 1135, 14060, 293, 950, 332, 66796, 320, 77, 284, 220, 14052, 8, 3131, 7446, 477, 311, 2211, 55681, 320, 77, 284, 220, 14052, 705, 3060, 477, 14946, 72783, 477, 45719, 26127, 315, 342, 24332, 33830, 65802, 349, 1475, 220, 1591, 2919, 13, 26150, 41265, 37442, 1051, 3115, 311, 6514, 8060, 323, 16945, 8624, 33824, 323, 20237, 13, 82935, 1392, 5343, 3477, 315, 66303, 68370, 2315, 11, 47447, 15696, 11, 18516, 86805, 77854, 2508, 5856, 5178, 2704, 11, 6784, 11, 3260, 4282, 292, 8670, 11, 323, 4367, 315, 2324, 14847, 13, 578, 23369, 8250, 315, 15419, 574, 220, 2137, 5672, 369, 293, 950, 332, 66796, 88186, 6978, 323, 220, 2983, 5672, 369, 2211, 496, 660, 6978, 26, 6514, 8060, 10222, 304, 220, 4331, 4, 323, 220, 2983, 4, 323, 8624, 33824, 304, 220, 3391, 4, 323, 220, 1644, 13689, 15947, 16134, 1741, 16, 29, 31969, 6372, 46603, 2211, 55681, 369, 2225, 37442, 320, 47, 366, 477, 284, 220, 15, 13, 6726, 705, 449, 31397, 42338, 320, 65, 950, 332, 66796, 25, 936, 55681, 8, 315, 220, 16, 13, 4370, 320, 2721, 4, 12410, 10074, 510, 11487, 1145, 220, 16, 13, 972, 311, 220, 17, 13, 410, 8, 369, 892, 311, 6514, 8060, 323, 220, 16, 13, 21, 320, 2721, 4, 21351, 11, 220, 16, 13, 777, 311, 220, 17, 13, 868, 8, 369, 892, 311, 8624, 33824, 13, 694, 1741, 16, 1822, 1741, 17, 29, 5659, 279, 220, 16, 4771, 20237, 6492, 11, 279, 31397, 11595, 369, 19463, 315, 4648, 574, 220, 16, 13, 1682, 320, 2721, 4, 21351, 11, 220, 15, 13, 4161, 311, 220, 16, 13, 5332, 570, 694, 1741, 17, 1822, 1741, 18, 29, 14636, 3117, 11, 449, 264, 23369, 1833, 5352, 315, 220, 4218, 5672, 11, 23369, 20237, 706, 539, 1027, 8813, 304, 3060, 1912, 13, 694, 1741, 18, 1822, 1741, 19, 29, 29240, 505, 26954, 304, 3892, 4367, 315, 2324, 7482, 1051, 12207, 2204, 320, 47, 366, 477, 284, 220, 15, 13, 1721, 8, 1990, 6514, 5315, 48582, 505, 4038, 220, 16, 311, 220, 21, 11, 323, 682, 46603, 293, 950, 332, 66796, 13, 694, 1741, 19, 1822, 1741, 20, 29, 28993, 11, 279, 7294, 438, 26252, 574, 1664, 66441, 7863, 449, 2211, 55681, 26, 694, 1741, 20, 1822, 1741, 21, 29, 449, 293, 950, 332, 66796, 11, 4106, 18698, 288, 10222, 2753, 3629, 323, 17659, 8541, 29668, 323, 342, 1910, 66274, 561, 689, 810, 3629, 13, 694, 1741, 21, 1822, 1741, 22, 29, 10541, 264, 47040, 315, 220, 1135, 14060, 315, 293, 950, 332, 66796, 3131, 7446, 574, 539, 439, 7524, 439, 2211, 55681, 11, 694, 1741, 22, 1822, 1741, 23, 29, 279, 37849, 4367, 315, 2324, 20124, 323, 279, 3428, 39775, 315, 2536, 71, 494, 25180, 31959, 4455, 3493, 8125, 311, 15806, 293, 950, 332, 66796, 11, 439, 264, 3254, 37471, 8479, 11, 520, 5190, 35130, 13, 694, 1741, 23, 29, 128009, 128006, 78191, 128007, 271, 5018, 8739, 9962, 794, 4482, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 46644, 93546, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to classify each argument component in the text as either "Claim" or "Premise". You must return a list of argument component types, strictly of length 8, in following JSON format: {"component_types": ["component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)", "component_type (str)"]} where each element "component_type (str)" is replaced by either "Claim" or "Premise". 

### Here is the abstract text:  Single-agent therapy with bicalutamide, a nonsteroidal antiandrogen, was compared with castration, either surgical or medical, in patients with untreated Stage D2 prostate cancer. In an open, randomized, multicenter trial, patients were randomized to treatment with 50 mg bicalutamide (n = 243) once daily or to castration (n = 243), either orchiectomy or depot injection of goserelin acetate every 28 days. Primary efficacy endpoints were times to treatment failure and objective disease progression and survival. Assessments included review of measurable metastases, prostate dimensions, Eastern Cooperative Oncology Group performance status, pain, analgesic requirements, and quality of life responses. The median duration of therapy was 39 weeks for bicalutamide-treated patients and 42 weeks for castrated patients; treatment failure occurred in 53% and 42% and disease progression in 43% and 33%, respectively.<AC1> Treatment effects favored castration for both endpoints (P < or = 0.002), with hazard ratios (bicalutamide:castration) of 1.54 (95% confidence interval [CI], 1.18 to 2.00) for time to treatment failure and 1.6 (95% CI, 1.19 to 2.15) for time to disease progression. </AC1><AC2> From the 1-year survival analysis, the hazard ratio for probability of death was 1.29 (95% CI, 0.96 to 1.72). </AC2><AC3> Thus far, with a median follow-up of 86 weeks, median survival has not been reached in either group. </AC3><AC4> Changes from baseline in several quality of life variables were significantly different (P < or = 0.01) between treatment groups periodically from months 1 to 6, and all favored bicalutamide. </AC4><AC5> Overall, the antiandrogen was well tolerated compared with castration; </AC5><AC6> with bicalutamide, hot flushes occurred less often and breast tenderness and gynecomastia more often. </AC6><AC7> Although a dosage of 50 mg of bicalutamide once daily was not as effective as castration, </AC7><AC8> the favorable quality of life outcomes and the low incidence of nonhormonal adverse events provide reasons to evaluate bicalutamide, as a single therapeutic agent, at higher doses. </AC8><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"component_types": ["Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Claim"]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 8739, 9962, 794, 4482, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 42562, 1082, 498, 330, 46644, 93546, 128009]
labels:
{"component_types": ["Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Premise", "Claim"]}<|eot_id|>
09/09/2024 03:08:49 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 03:08:49 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 03:08:49 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 03:08:49 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 03:14:41 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/09/2024 03:14:41 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 03:14:41 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/09/2024 03:14:41 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/09/2024 03:14:41 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,up_proj,q_proj,o_proj,k_proj,down_proj,gate_proj
09/09/2024 03:14:42 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/09/2024 03:14:42 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 03:14:42 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/09/2024 03:14:42 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/09/2024 03:14:42 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,k_proj,v_proj,down_proj,q_proj,gate_proj,up_proj
09/09/2024 03:14:44 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
09/09/2024 03:14:44 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
09/09/2024 03:14:44 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/09/2024 03:14:44 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.2429, 'grad_norm': 0.5610575079917908, 'learning_rate': 3.6363636363636364e-05, 'epoch': 0.45}
{'loss': 0.0635, 'grad_norm': 0.4928364157676697, 'learning_rate': 4.938574467213518e-05, 'epoch': 0.91}
{'loss': 0.0303, 'grad_norm': 0.1925496608018875, 'learning_rate': 4.644958533087443e-05, 'epoch': 1.36}
{'loss': 0.025, 'grad_norm': 0.16944916546344757, 'learning_rate': 4.137151834863213e-05, 'epoch': 1.82}
{'loss': 0.0241, 'grad_norm': 0.31421321630477905, 'learning_rate': 3.465862814232822e-05, 'epoch': 2.27}
{'loss': 0.0162, 'grad_norm': 0.339468389749527, 'learning_rate': 2.698124892141971e-05, 'epoch': 2.73}
{'loss': 0.0128, 'grad_norm': 0.11473339051008224, 'learning_rate': 1.9106026612264316e-05, 'epoch': 3.18}
{'loss': 0.0078, 'grad_norm': 0.052175138145685196, 'learning_rate': 1.181936330973744e-05, 'epoch': 3.64}
{'loss': 0.0078, 'grad_norm': 0.16979682445526123, 'learning_rate': 5.848888922025553e-06, 'epoch': 4.09}
{'loss': 0.0033, 'grad_norm': 0.12427391856908798, 'learning_rate': 1.790801674598186e-06, 'epoch': 4.55}
{'loss': 0.0031, 'grad_norm': 0.09788823872804642, 'learning_rate': 5.033308820289184e-08, 'epoch': 5.0}
{'train_runtime': 4815.688, 'train_samples_per_second': 0.363, 'train_steps_per_second': 0.023, 'train_loss': 0.039705626124685464, 'epoch': 5.0}
***** train metrics *****
  epoch                    =         5.0
  total_flos               = 499227956GF
  train_loss               =      0.0397
  train_runtime            =  1:20:15.68
  train_samples_per_second =       0.363
  train_steps_per_second   =       0.023
09/09/2024 04:35:14 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 04:35:14 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 04:35:14 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 04:35:14 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/09/2024 04:39:17 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 04:39:22 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/finetuned_models_run3/abstRCT_acc_llama-3-70b-Instruct-bnb-4bit
09/09/2024 04:39:22 - INFO - llamafactory.model.loader - all params: 70,657,253,376
              precision    recall  f1-score   support

       Claim      0.940     0.887     0.913       248
     Premise      0.939     0.968     0.953       443

    accuracy                          0.939       691
   macro avg      0.939     0.928     0.933       691
weighted avg      0.939     0.939     0.939       691

              precision    recall  f1-score   support

       Claim      0.909     0.890     0.899       191
     Premise      0.951     0.960     0.955       424

    accuracy                          0.938       615
   macro avg      0.930     0.925     0.927       615
weighted avg      0.938     0.938     0.938       615

              precision    recall  f1-score   support

       Claim      0.933     0.925     0.929       212
     Premise      0.960     0.965     0.962       397

    accuracy                          0.951       609
   macro avg      0.947     0.945     0.946       609
weighted avg      0.951     0.951     0.951       609

Successfully ran abstRCT_finetune.py with arguments: unsloth/llama-3-70b-Instruct-bnb-4bit acc 
 
  *************** 

Running abstRCT_finetune.py with arguments: unsloth/llama-3-70b-Instruct-bnb-4bit aric
09/09/2024 05:14:18 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:29231
09/09/2024 05:14:28 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/09/2024 05:14:28 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/09/2024 05:14:28 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/09/2024 05:14:28 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
09/09/2024 05:14:28 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/09/2024 05:14:28 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
09/09/2024 05:14:29 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 05:14:29 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 05:14:29 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_aric_train_neo.json...
09/09/2024 05:14:35 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/datasets/abstRCT_aric_train_neo.json...
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 94612, 8278, 1495, 902, 5727, 49926, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 10765, 5811, 4398, 1990, 5811, 6956, 304, 279, 8278, 1495, 323, 49229, 872, 12976, 955, 439, 3060, 330, 24249, 1, 477, 330, 21208, 3343, 1472, 2011, 471, 264, 1160, 315, 24657, 2641, 304, 279, 2768, 4823, 3645, 25, 5324, 1638, 9202, 48084, 9962, 794, 4416, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 705, 12976, 1857, 320, 496, 26090, 61453, 510, 2484, 10807, 320, 396, 705, 2218, 10807, 320, 396, 705, 12976, 1857, 320, 496, 8, 5163, 92, 1405, 1855, 2449, 330, 23013, 1857, 320, 496, 10143, 374, 12860, 555, 3060, 330, 24249, 1, 477, 330, 21208, 11690, 14711, 5810, 374, 279, 8278, 1495, 25, 220, 11579, 43035, 15419, 449, 293, 950, 332, 66796, 11, 264, 2536, 3751, 71916, 7294, 438, 26252, 11, 574, 7863, 449, 2211, 55681, 11, 3060, 34933, 477, 6593, 11, 304, 6978, 449, 83920, 22891, 423, 17, 47447, 9572, 13, 763, 459, 1825, 11, 47341, 11, 92520, 1992, 9269, 11, 6978, 1051, 47341, 311, 6514, 449, 220, 1135, 14060, 293, 950, 332, 66796, 320, 77, 284, 220, 14052, 8, 3131, 7446, 477, 311, 2211, 55681, 320, 77, 284, 220, 14052, 705, 3060, 477, 14946, 72783, 477, 45719, 26127, 315, 342, 24332, 33830, 65802, 349, 1475, 220, 1591, 2919, 13, 26150, 41265, 37442, 1051, 3115, 311, 6514, 8060, 323, 16945, 8624, 33824, 323, 20237, 13, 82935, 1392, 5343, 3477, 315, 66303, 68370, 2315, 11, 47447, 15696, 11, 18516, 86805, 77854, 2508, 5856, 5178, 2704, 11, 6784, 11, 3260, 4282, 292, 8670, 11, 323, 4367, 315, 2324, 14847, 13, 578, 23369, 8250, 315, 15419, 574, 220, 2137, 5672, 369, 293, 950, 332, 66796, 88186, 6978, 323, 220, 2983, 5672, 369, 2211, 496, 660, 6978, 26, 6514, 8060, 10222, 304, 220, 4331, 4, 323, 220, 2983, 4, 323, 8624, 33824, 304, 220, 3391, 4, 323, 220, 1644, 13689, 15947, 16134, 1741, 16, 29, 31969, 6372, 46603, 2211, 55681, 369, 2225, 37442, 320, 47, 366, 477, 284, 220, 15, 13, 6726, 705, 449, 31397, 42338, 320, 65, 950, 332, 66796, 25, 936, 55681, 8, 315, 220, 16, 13, 4370, 320, 2721, 4, 12410, 10074, 510, 11487, 1145, 220, 16, 13, 972, 311, 220, 17, 13, 410, 8, 369, 892, 311, 6514, 8060, 323, 220, 16, 13, 21, 320, 2721, 4, 21351, 11, 220, 16, 13, 777, 311, 220, 17, 13, 868, 8, 369, 892, 311, 8624, 33824, 13, 694, 1741, 16, 1822, 1741, 17, 29, 5659, 279, 220, 16, 4771, 20237, 6492, 11, 279, 31397, 11595, 369, 19463, 315, 4648, 574, 220, 16, 13, 1682, 320, 2721, 4, 21351, 11, 220, 15, 13, 4161, 311, 220, 16, 13, 5332, 570, 694, 1741, 17, 1822, 1741, 18, 29, 14636, 3117, 11, 449, 264, 23369, 1833, 5352, 315, 220, 4218, 5672, 11, 23369, 20237, 706, 539, 1027, 8813, 304, 3060, 1912, 13, 694, 1741, 18, 1822, 1741, 19, 29, 29240, 505, 26954, 304, 3892, 4367, 315, 2324, 7482, 1051, 12207, 2204, 320, 47, 366, 477, 284, 220, 15, 13, 1721, 8, 1990, 6514, 5315, 48582, 505, 4038, 220, 16, 311, 220, 21, 11, 323, 682, 46603, 293, 950, 332, 66796, 13, 694, 1741, 19, 1822, 1741, 20, 29, 28993, 11, 279, 7294, 438, 26252, 574, 1664, 66441, 7863, 449, 2211, 55681, 26, 694, 1741, 20, 1822, 1741, 21, 29, 449, 293, 950, 332, 66796, 11, 4106, 18698, 288, 10222, 2753, 3629, 323, 17659, 8541, 29668, 323, 342, 1910, 66274, 561, 689, 810, 3629, 13, 694, 1741, 21, 1822, 1741, 22, 29, 10541, 264, 47040, 315, 220, 1135, 14060, 315, 293, 950, 332, 66796, 3131, 7446, 574, 539, 439, 7524, 439, 2211, 55681, 11, 694, 1741, 22, 1822, 1741, 23, 29, 279, 37849, 4367, 315, 2324, 20124, 323, 279, 3428, 39775, 315, 2536, 71, 494, 25180, 31959, 4455, 3493, 8125, 311, 15806, 293, 950, 332, 66796, 11, 439, 264, 3254, 37471, 8479, 11, 520, 5190, 35130, 13, 694, 1741, 23, 29, 128009, 128006, 78191, 128007, 271, 5018, 1638, 9202, 48084, 9962, 794, 4416, 16, 11, 220, 23, 11, 330, 21208, 8073, 510, 17, 11, 220, 23, 11, 330, 24249, 8073, 510, 18, 11, 220, 17, 11, 330, 21208, 8073, 510, 19, 11, 220, 23, 11, 330, 24249, 8073, 510, 20, 11, 220, 23, 11, 330, 24249, 8073, 510, 21, 11, 220, 23, 11, 330, 24249, 8073, 510, 22, 11, 220, 23, 11, 330, 21208, 1365, 14316, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

### You are an expert in Argument Mining. You are given a biomedical abstract text which contains numbered argument components enclosed by <AC></AC> tags. Your task is to identify argument relations between argument components in the abstract text and classify their relation type as either "support" or "attack". You must return a list of triplets in the following JSON format: {"list_argument_relation_types": [[source AC (int), target AC (int), relation_type (str)],..., [source AC (int), target AC (int), relation_type (str)]]} where each element "relation_type (str)" is replaced by either "support" or "attack".

### Here is the abstract text:  Single-agent therapy with bicalutamide, a nonsteroidal antiandrogen, was compared with castration, either surgical or medical, in patients with untreated Stage D2 prostate cancer. In an open, randomized, multicenter trial, patients were randomized to treatment with 50 mg bicalutamide (n = 243) once daily or to castration (n = 243), either orchiectomy or depot injection of goserelin acetate every 28 days. Primary efficacy endpoints were times to treatment failure and objective disease progression and survival. Assessments included review of measurable metastases, prostate dimensions, Eastern Cooperative Oncology Group performance status, pain, analgesic requirements, and quality of life responses. The median duration of therapy was 39 weeks for bicalutamide-treated patients and 42 weeks for castrated patients; treatment failure occurred in 53% and 42% and disease progression in 43% and 33%, respectively.<AC1> Treatment effects favored castration for both endpoints (P < or = 0.002), with hazard ratios (bicalutamide:castration) of 1.54 (95% confidence interval [CI], 1.18 to 2.00) for time to treatment failure and 1.6 (95% CI, 1.19 to 2.15) for time to disease progression. </AC1><AC2> From the 1-year survival analysis, the hazard ratio for probability of death was 1.29 (95% CI, 0.96 to 1.72). </AC2><AC3> Thus far, with a median follow-up of 86 weeks, median survival has not been reached in either group. </AC3><AC4> Changes from baseline in several quality of life variables were significantly different (P < or = 0.01) between treatment groups periodically from months 1 to 6, and all favored bicalutamide. </AC4><AC5> Overall, the antiandrogen was well tolerated compared with castration; </AC5><AC6> with bicalutamide, hot flushes occurred less often and breast tenderness and gynecomastia more often. </AC6><AC7> Although a dosage of 50 mg of bicalutamide once daily was not as effective as castration, </AC7><AC8> the favorable quality of life outcomes and the low incidence of nonhormonal adverse events provide reasons to evaluate bicalutamide, as a single therapeutic agent, at higher doses. </AC8><|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"list_argument_relation_types": [[1, 8, "attack"], [2, 8, "support"], [3, 2, "attack"], [4, 8, "support"], [5, 8, "support"], [6, 8, "support"], [7, 8, "attack"]]}<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 1638, 9202, 48084, 9962, 794, 4416, 16, 11, 220, 23, 11, 330, 21208, 8073, 510, 17, 11, 220, 23, 11, 330, 24249, 8073, 510, 18, 11, 220, 17, 11, 330, 21208, 8073, 510, 19, 11, 220, 23, 11, 330, 24249, 8073, 510, 20, 11, 220, 23, 11, 330, 24249, 8073, 510, 21, 11, 220, 23, 11, 330, 24249, 8073, 510, 22, 11, 220, 23, 11, 330, 21208, 1365, 14316, 128009]
labels:
{"list_argument_relation_types": [[1, 8, "attack"], [2, 8, "support"], [3, 2, "attack"], [4, 8, "support"], [5, 8, "support"], [6, 8, "support"], [7, 8, "attack"]]}<|eot_id|>
09/09/2024 05:14:35 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 05:14:35 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 05:14:35 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 05:14:35 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 05:20:28 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/09/2024 05:20:28 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 05:20:28 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/09/2024 05:20:28 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/09/2024 05:20:28 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,k_proj,gate_proj,o_proj,up_proj,q_proj,v_proj
09/09/2024 05:20:28 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
09/09/2024 05:20:28 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 05:20:28 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
09/09/2024 05:20:28 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
09/09/2024 05:20:28 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,up_proj,down_proj,v_proj,o_proj,q_proj,gate_proj
09/09/2024 05:20:30 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
09/09/2024 05:20:31 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
09/09/2024 05:20:31 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
09/09/2024 05:20:31 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.
{'loss': 0.6383, 'grad_norm': 0.7971480488777161, 'learning_rate': 3.181818181818182e-05, 'epoch': 0.45}
{'loss': 0.1699, 'grad_norm': 0.18876416981220245, 'learning_rate': 4.9548217431567665e-05, 'epoch': 0.91}
{'loss': 0.1188, 'grad_norm': 0.3093681037425995, 'learning_rate': 4.684623442674463e-05, 'epoch': 1.36}
{'loss': 0.1118, 'grad_norm': 0.3027638792991638, 'learning_rate': 4.1962735288928305e-05, 'epoch': 1.82}
{'loss': 0.0965, 'grad_norm': 0.23206943273544312, 'learning_rate': 3.5385375325047166e-05, 'epoch': 2.27}
{'loss': 0.0807, 'grad_norm': 0.38450372219085693, 'learning_rate': 2.7770954997525277e-05, 'epoch': 2.73}
{'loss': 0.0709, 'grad_norm': 0.19129939377307892, 'learning_rate': 1.9879833298370238e-05, 'epoch': 3.18}
{'loss': 0.0571, 'grad_norm': 0.1455601304769516, 'learning_rate': 1.2500000000000006e-05, 'epoch': 3.64}
{'loss': 0.0557, 'grad_norm': 0.670572817325592, 'learning_rate': 6.368388758106133e-06, 'epoch': 4.09}
{'loss': 0.0374, 'grad_norm': 0.3135869801044464, 'learning_rate': 2.09728856419826e-06, 'epoch': 4.55}
{'loss': 0.0357, 'grad_norm': 0.2687162756919861, 'learning_rate': 1.1320193567288529e-07, 'epoch': 5.0}
{'train_runtime': 4824.7824, 'train_samples_per_second': 0.363, 'train_steps_per_second': 0.023, 'train_loss': 0.13389612165364354, 'epoch': 5.0}
***** train metrics *****
  epoch                    =         5.0
  total_flos               = 500323264GF
  train_loss               =      0.1339
  train_runtime            =  1:20:24.78
  train_samples_per_second =       0.363
  train_steps_per_second   =       0.023
09/09/2024 06:41:11 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
09/09/2024 06:41:11 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.
09/09/2024 06:41:11 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.
09/09/2024 06:41:11 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
09/09/2024 06:46:03 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
09/09/2024 06:46:08 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/am_work/coling_2025/abstRCT/finetuned_models_run3/abstRCT_aric_llama-3-70b-Instruct-bnb-4bit
09/09/2024 06:46:08 - INFO - llamafactory.model.loader - all params: 70,657,253,376
              precision    recall  f1-score   support

        None      0.983     0.978     0.980      6435
      attack      0.635     0.678     0.656        59
     support      0.670     0.728     0.698       360

    accuracy                          0.962      6854
   macro avg      0.763     0.794     0.778      6854
weighted avg      0.964     0.962     0.963      6854

              precision    recall  f1-score   support

        None      0.984     0.974     0.979      5219
      attack      0.560     0.500     0.528        28
     support      0.662     0.773     0.713       317

    accuracy                          0.960      5564
   macro avg      0.735     0.749     0.740      5564
weighted avg      0.963     0.960     0.962      5564

              precision    recall  f1-score   support

        None      0.983     0.970     0.977      5176
      attack      0.424     0.583     0.491        24
     support      0.612     0.740     0.670       296

    accuracy                          0.956      5496
   macro avg      0.673     0.764     0.713      5496
weighted avg      0.961     0.956     0.958      5496

Successfully ran abstRCT_finetune.py with arguments: unsloth/llama-3-70b-Instruct-bnb-4bit aric 
 
  *************** 

