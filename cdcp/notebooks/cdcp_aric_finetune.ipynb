{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oHFCsV0z-Jw"
   },
   "source": [
    "# Finetune CDCP dataset for ACC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr7rB3szzhtx"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "giM74oK1rRIH",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell only once to install LLaMA-Factory\n",
    "\n",
    "# %cd ..\n",
    "# %rm -rf LLaMA-Factory\n",
    "# !git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "# %cd LLaMA-Factory\n",
    "# %ls\n",
    "# !pip install -e .[torch,bitsandbytes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip uninstall -y pydantic\n",
    "# !pip install pydantic==1.10.9 # \n",
    "\n",
    "# !pip uninstall -y gradio\n",
    "# !pip install gradio==3.48.0\n",
    "\n",
    "# !pip uninstall -y bitsandbytes\n",
    "# !pip install --upgrade bitsandbytes\n",
    "\n",
    "# !pip install tqdm\n",
    "# !pip install ipywidgets\n",
    "# !pip install scikit-learn\n",
    "\n",
    "# Restart kernel afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import subprocess\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from llamafactory.chat import ChatModel\n",
    "from llamafactory.extras.misc import torch_gc\n",
    "from sklearn.metrics import classification_report\n",
    "from utils.post_processing import post_process_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:    \n",
    "    assert torch.cuda.is_available() is True\n",
    "    \n",
    "except AssertionError:\n",
    "    \n",
    "    print(\"Please set up a GPU before using LLaMA Factory...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdcp_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq/am_work/coling_2025/cdcp'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdcp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"unsloth/llama-3-8b-Instruct-bnb-4bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"aric\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.path.join(cdcp_dir, \"finetuned_models\", f\"\"\"CDCP_{TASK}_{BASE_MODEL.split(\"/\")[1]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq/am_work/coling_2025/cdcp/finetuned_models/CDCP_aric_llama-3-8b-Instruct-bnb-4bit'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "TeYs5Lz-QJYk"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "# *** TRAIN DATASET NAME *** #\n",
    "\n",
    "train_dataset_name = f\"\"\"CDCP_{TASK}_{TAGS}_train.json\"\"\"\n",
    "test_dataset_name = f\"\"\"CDCP_{TASK}_{TAGS}_test.json\"\"\"\n",
    "\n",
    "#train_dataset_name = f\"\"\"PE_{TASK}_{CONTEXT}_train.json\"\"\"\n",
    "train_dataset_file = os.path.join(DATASET_DIR, train_dataset_name)\n",
    "\n",
    "# *** TEST DATASET NAME *** #\n",
    "\n",
    "#test_dataset_name = f\"\"\"PE_{TASK}_{CONTEXT}_test.json\"\"\"\n",
    "test_dataset_file = os.path.join(DATASET_DIR, test_dataset_name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_dataset_file, test_dataset_file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "rgR3UFhB0Ifq"
   },
   "source": [
    "## Fine-tune Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if not os.path.exists(os.path.join(os.getcwd(), \"ft_arg_files\")):\n",
    "    os.mkdir(os.path.join(os.getcwd(), \"ft_arg_files\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "# *** TRAIN FILE ***\n",
    "\n",
    "# model_name = f\"\"\"{train_dataset_name.split(\".\")[0].split(\"train\")[0]}{BASE_MODEL.split(\"/\")[1]}\"\"\"\n",
    "\n",
    "train_file = os.path.join(os.getcwd(), \"ft_arg_files\", f\"\"\"{train_dataset_name.split(\".\")[0].split(\"train\")[0]}{BASE_MODEL.split(\"/\")[1]}.json\"\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "dataset_info_line =  {\n",
    "  \"file_name\": f\"{train_dataset_file}\",\n",
    "  \"columns\": {\n",
    "    \"prompt\": \"instruction\",\n",
    "    \"query\": \"input\",\n",
    "    \"response\": \"output\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "dataset_info_line"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(os.path.join(LLAMA_FACTORY_DIR, \"data/dataset_info.json\"), \"r\") as jsonFile:\n",
    "    data = json.load(jsonFile)\n",
    "\n",
    "data[\"persuasive_essays\"] = dataset_info_line\n",
    "\n",
    "with open(os.path.join(LLAMA_FACTORY_DIR, \"data/dataset_info.json\"), \"w\") as jsonFile:\n",
    "    json.dump(data, jsonFile)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Training Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "CS0Qk5OR0i4Q",
    "tags": []
   },
   "source": [
    "args = dict(\n",
    "  stage=\"sft\",                           # do supervised fine-tuning\n",
    "  do_train=True,\n",
    "  model_name_or_path=BASE_MODEL,         # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  dataset=\"cdcp\",           # use alpaca and identity datasets\n",
    "  template=\"llama3\",                     # use llama3 prompt template\n",
    "  finetuning_type=\"lora\",                # use LoRA adapters to save memory\n",
    "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
    "  output_dir=OUTPUT_DIR,                 # the path to save LoRA adapters\n",
    "  overwrite_output_dir=True,             # overrides existing output contents\n",
    "  per_device_train_batch_size=2,         # the batch size\n",
    "  gradient_accumulation_steps=4,         # the gradient accumulation steps\n",
    "  lr_scheduler_type=\"cosine\",            # use cosine learning rate scheduler\n",
    "  logging_steps=10,                      # log every 10 steps\n",
    "  warmup_ratio=0.1,                      # use warmup scheduler\n",
    "  save_steps=3000,                       # save checkpoint every 1000 steps\n",
    "  learning_rate=5e-5,                    # the learning rate\n",
    "  num_train_epochs=NB_EPOCHS,            # the epochs of training\n",
    "  max_samples=2000,                       # use 500 examples in each dataset\n",
    "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
    "  quantization_bit=4,                    # use 4-bit QLoRA\n",
    "  loraplus_lr_ratio=16.0,                # use LoRA+ algorithm with lambda=16.0\n",
    "  fp16=True,                             # use float16 mixed precision training\n",
    "  report_to=\"none\"                       # discards wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "json.dump(args, open(train_file, \"w\", encoding=\"utf-8\"), indent=2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p = subprocess.Popen([\"llamafactory-cli\", \"train\", train_file], cwd=LLAMA_FACTORY_DIR)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p.wait()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "PVNaC-xS5N40"
   },
   "source": [
    "## Inference on the fine-tuned model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "os.listdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "oh8H9A_25SF9",
    "tags": []
   },
   "source": [
    "args = dict(\n",
    "  model_name_or_path=BASE_MODEL, # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  adapter_name_or_path=OUTPUT_DIR,            # load the saved LoRA adapters\n",
    "  template=\"llama3\",                     # same to the one in training\n",
    "  finetuning_type=\"lora\",                  # same to the one in training\n",
    "  quantization_bit=4,                    # load 4-bit quantized model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "source": [
    "model = ChatModel(args)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "with open(test_dataset_file, \"r+\") as fh:\n",
    "    test_dataset = json.load(fh)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "test_prompts = []\n",
    "test_grounds = []\n",
    "\n",
    "for sample in test_dataset:\n",
    "    test_prompts.append(\"\\nUser:\" + sample[\"instruction\"] + sample[\"input\"])\n",
    "    test_grounds.append(sample[\"output\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "test_predictions = []\n",
    "\n",
    "for prompt in tqdm(test_prompts):\n",
    "\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = \"\"\n",
    "    \n",
    "    for new_text in model.stream_chat(messages):\n",
    "        #print(new_text, end=\"\", flush=True)\n",
    "        response += new_text\n",
    "        #print()\n",
    "    test_predictions.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    torch_gc()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "with open(os.path.join(OUTPUT_DIR, f\"\"\"CDCP_{TASK}_results_{NB_EPOCHS}.pickle\"\"\"), 'wb') as fh:\n",
    "    results_d = {\"ground_truths\": test_grounds,\n",
    "                 \"predictions\": test_predictions    \n",
    "        \n",
    "    }\n",
    "    pickle.dump(results_d, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdcp_dataset = load_dataset(\"DFKI-SLT/cdcp\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'propositions', 'relations'],\n",
       "        num_rows: 580\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'propositions', 'relations'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdcp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '01418',\n",
       " 'text': 'For further reference on debt tolling: __URL__ So, again, it seems as though the SOL matters very little in these cases. Perhaps the bigger question is whether or not eliminating debt tolling should be considered as a new rule.',\n",
       " 'propositions': {'start': [0, 46, 120],\n",
       "  'end': [46, 120, 227],\n",
       "  'label': [2, 4, 4],\n",
       "  'url': ['http://collectionagencydebt.blogspot.com/2012/08/tolling-debt-and-statute-of-limitations.html',\n",
       "   '',\n",
       "   '']},\n",
       " 'relations': {'head': [], 'tail': [], 'label': []}}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdcp_dataset['test'][149]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_acs_l = []\n",
    "\n",
    "for sample in cdcp_dataset['test']:\n",
    "    nr_acs_l.append(len(sample['propositions']['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nr_acs_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_DIR, f\"\"\"CDCP_{TASK}_results_{NB_EPOCHS}.pickle\"\"\"), \"rb\") as fh:\n",
    "        \n",
    "        results = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounds = results[\"ground_truths\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounds = [json.loads(x)[\"relation_types\"] for x in grounds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = results[\"predictions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [x[\"content\"] for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [json.loads(x)[\"relation_types\"] for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grounds), len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "15\n",
      "17\n",
      "18\n",
      "20\n",
      "21\n",
      "25\n",
      "26\n",
      "28\n",
      "29\n",
      "32\n",
      "34\n",
      "35\n",
      "37\n",
      "38\n",
      "39\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "50\n",
      "51\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "64\n",
      "66\n",
      "67\n",
      "73\n",
      "74\n",
      "76\n",
      "77\n",
      "78\n",
      "80\n",
      "84\n",
      "85\n",
      "87\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "96\n",
      "98\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "110\n",
      "114\n",
      "116\n",
      "117\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "128\n",
      "130\n",
      "132\n",
      "133\n",
      "136\n",
      "139\n",
      "142\n",
      "144\n",
      "148\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "for i,(x,y) in enumerate(zip(grounds, preds)):\n",
    "    \n",
    "    if len(x) != len(y):\n",
    "            \n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounds_tmp = grounds[:]\n",
    "preds_tmp = preds[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_relations(nr_acs_l, grounds_arg, preds_arg):\n",
    "\n",
    "    \n",
    "    #grounds_tmp = copy.copy(grounds)\n",
    "    #preds_tmp = copy.copy(preds)\n",
    "    \n",
    "    final_grounds = []\n",
    "    final_preds = []\n",
    "\n",
    "    for idx, nr_acs in enumerate(nr_acs_l):\n",
    "        \n",
    "        current_grounds = grounds_arg[idx]\n",
    "        current_grounds_st = [[x[0], x[1]] for x in current_grounds]\n",
    "        current_preds = preds_arg[idx]\n",
    "        current_preds_st = [[x[0], x[1]] for x in current_preds]       \n",
    "\n",
    "        \n",
    "        for i in range(nr_acs):\n",
    "            for j in range(nr_acs):\n",
    "                \n",
    "                if i != j:\n",
    "                    \n",
    "                    st = [i, j]\n",
    "                    \n",
    "                    if st not in current_grounds_st:\n",
    "                        current_grounds.append([st[0], st[1], \"None\"])\n",
    "\n",
    "                    if st not in current_preds_st:\n",
    "                        current_preds.append([st[0], st[1], \"None\"])\n",
    "\n",
    "        current_grounds.sort()\n",
    "        current_preds.sort()\n",
    "        final_grounds.append(current_grounds)\n",
    "        final_preds.append(current_preds)\n",
    "\n",
    "    return final_grounds, final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_grounds, final_preds = get_all_relations(nr_acs_l, grounds_tmp, preds_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_grounds), len(final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 'None'],\n",
       " [0, 2, 'None'],\n",
       " [1, 0, 'None'],\n",
       " [1, 2, 'None'],\n",
       " [2, 0, 'None'],\n",
       " [2, 1, 'reason']]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_grounds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 'None'],\n",
       " [0, 2, 'None'],\n",
       " [1, 0, 'None'],\n",
       " [1, 2, 'None'],\n",
       " [2, 0, 'reason'],\n",
       " [2, 1, 'reason']]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_grounds = [x for xs in final_grounds for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = [x for xs in final_preds for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10328, 10328)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_grounds), len(final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_grounds = [x[2] for x in final_grounds]\n",
    "final_preds = [x[2] for x in final_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None      0.979     0.983     0.981     10004\n",
      "    evidence      0.000     0.000     0.000        26\n",
      "      reason      0.363     0.342     0.352       298\n",
      "\n",
      "    accuracy                          0.962     10328\n",
      "   macro avg      0.447     0.442     0.444     10328\n",
      "weighted avg      0.959     0.962     0.960     10328\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(final_grounds, final_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opposite_acc(component_type):\n",
    "\n",
    "    if component_type == \"fact\":\n",
    "        return \"value\"\n",
    "    elif component_type == \"value\":\n",
    "        return \"policy\"\n",
    "    elif component_type == \"policy\":\n",
    "        return \"value\"\n",
    "    elif component_type == \"testimony\":\n",
    "        return \"fact\"\n",
    "    elif component_type == \"reference\":\n",
    "        return \"policy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonize_preds_acc(grounds, preds):\n",
    "\n",
    "    l1, l2 = len(preds), len(grounds)\n",
    "    if l1 < l2:\n",
    "        diff = l2 - l1\n",
    "        preds = preds + [opposite_acc(x) for x in grounds[l1:]]\n",
    "    else:\n",
    "        preds = preds[:l2]\n",
    "        \n",
    "    return preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(x,y) in enumerate(zip(grounds, preds)):\n",
    "    \n",
    "    if len(x) != len(y):\n",
    "            \n",
    "        preds[i] = harmonize_preds_acc(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_preds = [item for row in preds for item in row]\n",
    "task_grounds = [item for row in grounds for item in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check: \n",
    "len(task_preds) == len(task_grounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fact      0.596     0.750     0.664       132\n",
      "      policy      0.883     0.889     0.886       153\n",
      "   reference      1.000     1.000     1.000         1\n",
      "   testimony      0.922     0.869     0.895       244\n",
      "       value      0.872     0.835     0.853       496\n",
      "\n",
      "    accuracy                          0.840      1026\n",
      "   macro avg      0.855     0.868     0.860      1026\n",
      "weighted avg      0.850     0.840     0.844      1026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(task_grounds, task_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"\"\"{OUTPUT_DIR}/classification_report.pickle\"\"\", 'wb') as fh:\n",
    "    \n",
    "    pickle.dump(classification_report(task_grounds, task_preds, output_dict=True), fh)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "precision    recall  f1-score   support\n",
    "\n",
    "        fact      0.569     0.689     0.623       132\n",
    "      policy      0.893     0.876     0.884       153\n",
    "   reference      1.000     1.000     1.000         1\n",
    "   testimony      0.925     0.906     0.915       244\n",
    "       value      0.868     0.833     0.850       496\n",
    "\n",
    "    accuracy                          0.838      1026\n",
    "   macro avg      0.851     0.861     0.855      1026\n",
    "weighted avg      0.847     0.838     0.842      1026\n",
    "\n",
    "10 epochs\n",
    "\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "        fact      0.596     0.750     0.664       132\n",
    "      policy      0.883     0.889     0.886       153\n",
    "   reference      1.000     1.000     1.000         1\n",
    "   testimony      0.922     0.869     0.895       244\n",
    "       value      0.872     0.835     0.853       496\n",
    "\n",
    "    accuracy                          0.840      1026\n",
    "   macro avg      0.855     0.868     0.860      1026\n",
    "weighted avg      0.850     0.840     0.844      1026\n",
    "\n",
    "20 epochs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
